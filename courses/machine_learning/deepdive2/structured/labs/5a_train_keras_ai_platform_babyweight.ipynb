{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 5a:  Training Keras model on Cloud AI Platform.\n",
    "\n",
    "**Learning Objectives**\n",
    "\n",
    "1. Setup up the environment\n",
    "1. Create trainer module's task.py to hold hyperparameter argparsing code\n",
    "1. Create trainer module's model.py to hold Keras model code\n",
    "1. Run trainer module package locally\n",
    "1. Submit training job to Cloud AI Platform\n",
    "1. Submit hyperparameter tuning job to Cloud AI Platform\n",
    "\n",
    "\n",
    "## Introduction\n",
    "After having testing our training pipeline both locally and in the cloud on a susbset of the data, we can submit another (much larger) training job to the cloud. It is also a good idea to run a hyperparameter tuning job to make sure we have optimized the hyperparameters of our model. \n",
    "\n",
    "In this notebook, we'll be training our Keras model at scale using Cloud AI Platform.\n",
    "\n",
    "In this lab, we will set up the environment, create the trainer module's task.py to hold hyperparameter argparsing code, create the trainer module's model.py to hold Keras model code, run the trainer module package locally, submit a training job to Cloud AI Platform, and submit a hyperparameter tuning job to Cloud AI Platform.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in this student lab notebook -- try to complete this notebook first and then review the [solution notebook](../solutions/5a_train_keras_ai_platform_babyweight.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hJ7ByvoXzpVI"
   },
   "source": [
    "## Set up environment variables and load necessary libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task #1: Set environment variables.\n",
    "\n",
    "Set environment variables so that we can use them throughout the entire lab. We will be using our project name for our bucket, so you only need to change your project and region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current GCP Project Name is: qwiklabs-gcp-00-0db9b1bc58c6\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PROJECT=$(gcloud config list project --format \"value(core.project)\")\n",
    "echo \"Your current GCP Project Name is: \"${PROJECT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwiklabs-gcp-00-0db9b1bc58c6 qwiklabs-gcp-00-0db9b1bc58c6 us-central1\n"
     ]
    }
   ],
   "source": [
    "# TODO: Change these to try this notebook out\n",
    "PROJECT = 'qwiklabs-gcp-00-0db9b1bc58c6'  # Replace with your PROJECT\n",
    "BUCKET = PROJECT  # defaults to PROJECT\n",
    "REGION = \"us-central1\"  # Replace with your REGION\n",
    "\n",
    "print(PROJECT, BUCKET, REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.1\"\n",
    "os.environ[\"PYTHONVERSION\"] = \"3.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}; then\n",
    "    gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data exists\n",
    "\n",
    "Verify that you previously created CSV files we'll be using for training and evaluation. If not, go back to lab [1b_prepare_data_babyweight.ipynb](../solutions/1b_prepare_data_babyweight.ipynb) to create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/data/eval000000000000.csv\n",
      "gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/data/train000000000000.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/data/*000000000000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the [Keras wide-and-deep code](../solutions/4c_keras_wide_and_deep_babyweight.ipynb) working on a subset of the data, we can package the TensorFlow code up as a Python module and train it on Cloud AI Platform.\n",
    "\n",
    "## Train on Cloud AI Platform\n",
    "\n",
    "Training on Cloud AI Platform requires:\n",
    "* Making the code a Python package\n",
    "* Using gcloud to submit the training code to [Cloud AI Platform](https://console.cloud.google.com/ai-platform)\n",
    "\n",
    "**Ensure that the Cloud AI Platform API is enabled by going to this [link](https://console.developers.google.com/apis/library/ml.googleapis.com).**\n",
    "\n",
    "### Move code into a Python package\n",
    "\n",
    "A Python package is simply a collection of one or more `.py` files along with an `__init__.py` file to identify the containing directory as a package. The `__init__.py` sometimes contains initialization code but for our purposes an empty file suffices.\n",
    "\n",
    "The bash command `touch` creates an empty file in the specified location, the directory `babyweight` should already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p babyweight/trainer\n",
    "touch babyweight/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use the `%%writefile` magic to write the contents of the cell below to a file called `task.py` in the `babyweight/trainer` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task #2: Create trainer module's task.py to hold hyperparameter argparsing code.\n",
    "\n",
    "The cell below writes the file `babyweight/trainer/task.py` which sets up our training job. Here is where we determine which parameters of our model to pass as flags during training using the `parser` module. Look at how `batch_size` is passed to the model in the code below. Use this as an example to parse arguements for the following variables\n",
    "- `nnsize` which represents the hidden layer sizes to use for DNN feature columns\n",
    "- `nembeds` which represents the embedding size of a cross of n key real-valued parameters\n",
    "- `train_examples` which represents the number of examples (in thousands) to run the training job\n",
    "- `eval_steps` which represents the positive number of steps for which to evaluate model\n",
    "\n",
    "Be sure to include a default value for the parsed arguments above and specfy the `type` if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting babyweight/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile babyweight/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from trainer import model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"this model ignores this field, but it is required by gcloud\",\n",
    "        default=\"junk\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_data_path\",\n",
    "        help=\"GCS location of training data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--eval_data_path\",\n",
    "        help=\"GCS location of evaluation data\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help=\"GCS location to write checkpoints and export models\",\n",
    "        required=True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help=\"Number of examples to compute gradient over.\",\n",
    "        type=int,\n",
    "        default=512\n",
    "    )\n",
    "\n",
    "    # TODO: Add nnsize argument\n",
    "    parser.add_argument(\n",
    "        \"--nnsize\",\n",
    "        help = \"Hidden layer sizes (provide space-separated sizes)\",\n",
    "        nargs = \"+\",\n",
    "        type = int,\n",
    "        default=[32, 8]\n",
    "    )\n",
    "\n",
    "    # TODO: Add nembeds argument\n",
    "    parser.add_argument(\n",
    "        \"--nembeds\",\n",
    "        help=\"nembeds\",\n",
    "        type=int,\n",
    "        default=8\n",
    "    )\n",
    "\n",
    "    # TODO: Add num_epochs argument\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help=\"num_epochs\",\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "\n",
    "    # TODO: Add train_examples argument\n",
    "    parser.add_argument(\n",
    "        \"--train_examples\",\n",
    "        help=\"train_examples\",\n",
    "        type=int,\n",
    "        default=1\n",
    "    )\n",
    "\n",
    "    # TODO: Add eval_steps argument\n",
    "    parser.add_argument(\n",
    "        \"--eval_steps\",\n",
    "        help=\"eval_steps\",\n",
    "        type=int,\n",
    "        default=8\n",
    "    )\n",
    "\n",
    "    # Parse all arguments\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service\n",
    "    arguments.pop(\"job_dir\", None)\n",
    "    arguments.pop(\"job-dir\", None)\n",
    "\n",
    "    # Modify some arguments\n",
    "    arguments[\"train_examples\"] *= 1000\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    arguments[\"output_dir\"] = os.path.join(\n",
    "        arguments[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    )\n",
    "\n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way we can write to the file `model.py` the model that we developed in the previous notebooks. \n",
    "\n",
    "### Lab Task #3: Create trainer module's model.py to hold Keras model code.\n",
    "\n",
    "Complete the TODOs in the code cell below to create our `model.py`. We'll use the code we wrote for the Wide & Deep model. Look back at your [9_keras_wide_and_deep_babyweight](../solutions/9_keras_wide_and_deep_babyweight.ipynb) notebook and copy/paste the necessary code from that notebook into its place in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile babyweight/trainer/model.py\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import hypertune\n",
    "\n",
    "# Determine CSV, label, and key columns\n",
    "# TODO: Add CSV_COLUMNS and LABEL_COLUMN\n",
    "CSV_COLUMNS = [\"weight_pounds\", \"is_male\", \"mother_age\", \"plurality\", \"gestation_weeks\"]\n",
    "LABEL_COLUMN = 'weight_pounds'\n",
    "NUMERIC_COLUMNS = [\"mother_age\", \"gestation_weeks\"]\n",
    "STRING_COLUMNS = [\"is_male\", \"plurality\"]\n",
    "VOC_IS_MALE = ['true', 'false', 'Unknown']\n",
    "VOC_PLURALITY = ['Single(1)', 'Twins(2)', 'Triplets(3)', 'Quadruplets(4)', 'Quintuplets(5)', 'Multiple(2+)']\n",
    "\n",
    "# Set default values for each CSV column.\n",
    "# Treat is_male and plurality as strings.\n",
    "# TODO: Add DEFAULTS\n",
    "DEFAULTS = [[0.], ['null'], [0.], ['null'], [0.]]\n",
    "\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    # TODO: Add your code here\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "    feature = row_data\n",
    "    return feature, label\n",
    "\n",
    "\n",
    "def load_dataset(pattern, batch_size=1, mode='eval'):\n",
    "    # TODO: Add your code here\n",
    "    ds = tf.data.experimental.make_csv_dataset(pattern, batch_size, CSV_COLUMNS, DEFAULTS)\n",
    "    ds = ds.map(features_and_labels).cache()\n",
    "    \n",
    "    if mode == 'train':\n",
    "        ds = ds.shuffle(buffer_size=1000).repeat()\n",
    "    ds = ds.prefetch(buffer_size=1)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def create_input_layers():\n",
    "    # TODO: Add your code here\n",
    "    inputs = {\n",
    "        colname: tf.keras.Input(name=colname, shape=(), dtype='float32') for colname in NUMERIC_COLUMNS \n",
    "    }\n",
    "    inputs.update({\n",
    "        colname: tf.keras.Input(name=colname, shape=(), dtype='string') for colname in STRING_COLUMNS \n",
    "    })\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def categorical_fc(name, values):\n",
    "    # TODO: Add your code here\n",
    "    cat = tf.feature_column.categorical_column_with_vocabulary_list(name, values)\n",
    "    return tf.feature_column.indicator_column(cat)\n",
    "\n",
    "\n",
    "def create_feature_columns(nembeds):\n",
    "    # TODO: Add your code here\n",
    "    deep_fc = {\n",
    "        colname: tf.feature_column.numeric_column(colname) for colname in NUMERIC_COLUMNS\n",
    "    }\n",
    "    \n",
    "    wide_fc = {}\n",
    "    colname = 'is_male'\n",
    "    wide_fc[colname] = categorical_fc(colname, VOC_IS_MALE)\n",
    "    colname = 'plurality'\n",
    "    wide_fc[colname] = categorical_fc(colname, VOC_PLURALITY)\n",
    "    \n",
    "    age_bkt = tf.feature_column.bucketized_column(deep_fc['mother_age'], \n",
    "                                                  boundaries=np.arange(15, 45, 1).tolist()\n",
    "                                                 )\n",
    "    wide_fc['age_bkt'] = tf.feature_column.indicator_column(age_bkt)\n",
    "    gestation_bkt = tf.feature_column.bucketized_column(deep_fc['gestation_weeks'], boundaries=np.arange(17, 47, 1).tolist())\n",
    "    wide_fc['gestation_bkt'] = tf.feature_column.indicator_column(gestation_bkt)\n",
    "    \n",
    "    crossed = tf.feature_column.crossed_column([age_bkt, gestation_bkt], 1000)\n",
    "    deep_fc['crossed'] = tf.feature_column.embedding_column(crossed, dimension=nembeds)\n",
    "\n",
    "    return wide_fc, deep_fc\n",
    "\n",
    "\n",
    "def get_model_outputs(wide_inputs, deep_inputs, dnn_hidden_units):\n",
    "    # TODO: Add your code here\n",
    "    deep = deep_inputs\n",
    "    for i, units in enumerate(dnn_hidden_units):\n",
    "        deep = tf.keras.layers.Dense(units, name=f'deep_h{i+1}', activation='relu')(deep)\n",
    "        \n",
    "    wide = tf.keras.layers.Dense(10, name='wide_h', activation='relu')(wide_inputs)\n",
    "    \n",
    "    both = tf.keras.layers.concatenate(inputs=[deep, wide], name=\"both\")\n",
    "        \n",
    "    out = tf.keras.layers.Dense(1, name='output', activation='linear')(both)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    # TODO: Add your code here\n",
    "    return tf.sqrt(tf.reduce_mean((y_true - y_pred)**2))\n",
    "\n",
    "\n",
    "def build_wide_deep_model(dnn_hidden_units=[64, 32], nembeds=3):\n",
    "    # TODO: Add your code here\n",
    "    inputs = create_input_layers()\n",
    "    wide_fc, deep_fc = create_feature_columns(nembeds)\n",
    "    \n",
    "    wide_inputs = tf.keras.layers.DenseFeatures(\n",
    "        feature_columns=wide_fc.values(), name=\"wide_inputs\")(inputs)\n",
    "    deep_inputs = tf.keras.layers.DenseFeatures(\n",
    "        feature_columns=deep_fc.values(), name=\"deep_inputs\")(inputs)\n",
    "\n",
    "    output = get_model_outputs(wide_inputs, deep_inputs, dnn_hidden_units)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=[rmse, 'mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def build_summary():\n",
    "    model = build_wide_deep_model()\n",
    "    model.summary()\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    model = build_wide_deep_model(args[\"nnsize\"], args[\"nembeds\"])\n",
    "    print(\"Here is our Wide-and-Deep architecture so far:\\n\")\n",
    "    print(model.summary())\n",
    "\n",
    "    trainds = load_dataset(\n",
    "        args[\"train_data_path\"],\n",
    "        args[\"batch_size\"],\n",
    "        'train')\n",
    "\n",
    "    evalds = load_dataset(\n",
    "        args[\"eval_data_path\"], 1000, 'eval')\n",
    "    if args[\"eval_steps\"]:\n",
    "        evalds = evalds.take(count=args[\"eval_steps\"])\n",
    "\n",
    "    num_batches = args[\"batch_size\"] * args[\"num_epochs\"]\n",
    "    steps_per_epoch = args[\"train_examples\"] // num_batches\n",
    "\n",
    "    checkpoint_path = os.path.join(args[\"output_dir\"], \"checkpoints/babyweight\")\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, verbose=1, save_weights_only=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=args[\"num_epochs\"],\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "        callbacks=[cp_callback])\n",
    "    \n",
    "    hp_metric = history.history['val_rmse'][-1]\n",
    "\n",
    "    hptune = hypertune.HyperTune()\n",
    "    hptune.report_hyperparameter_tuning_metric(\n",
    "        hyperparameter_metric_tag='rmse', \n",
    "        metric_value=hp_metric,\n",
    "        global_step=args['num_epochs']\n",
    "    )\n",
    "\n",
    "    EXPORT_PATH = os.path.join(\n",
    "        args[\"output_dir\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    tf.saved_model.save(\n",
    "        obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
    "    print(\"Exported trained model to {}\".format(EXPORT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train locally\n",
    "\n",
    "After moving the code to a package, make sure it works as a standalone. Note, we incorporated the `--train_examples` flag so that we don't try to train on the entire dataset while we are developing our pipeline. Once we are sure that everything is working on a subset, we can change it so that we can train on all the data. Even for this subset, this takes about *3 minutes* in which you won't see any output ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Task #4: Run trainer module package locally.\n",
    "\n",
    "Fill in the missing code in the TODOs below so that we can run a very small training job over a single file with a small batch size, 1 epoch, 1 train example, and 1 eval step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our Wide-and-Deep architecture so far:\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "gestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "is_male (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mother_age (InputLayer)         [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "plurality (InputLayer)          [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "deep_inputs (DenseFeatures)     (None, 10)           8000        gestation_weeks[0][0]            \n",
      "                                                                 is_male[0][0]                    \n",
      "                                                                 mother_age[0][0]                 \n",
      "                                                                 plurality[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "deep_h1 (Dense)                 (None, 32)           352         deep_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "wide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "                                                                 is_male[0][0]                    \n",
      "                                                                 mother_age[0][0]                 \n",
      "                                                                 plurality[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "deep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "wide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "both (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "                                                                 wide_h[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 9,209\n",
      "Trainable params: 9,209\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train for 31 steps, validate for 32 steps\n",
      "\n",
      "Epoch 00001: saving model to babyweight_trained/checkpoints/babyweight\n",
      "31/31 - 3s - loss: 3.5956 - rmse: 1.8858 - mse: 3.5956 - val_loss: 2.9153 - val_rmse: 1.7069 - val_mse: 2.9153\n",
      "Exported trained model to babyweight_trained/20210602013726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-02 01:37:21.615038: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-06-02 01:37:21.615171: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-06-02 01:37:21.615189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "2021-06-02 01:37:22.447254: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2021-06-02 01:37:22.447299: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-06-02 01:37:22.447326: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lg-dlvm): /proc/driver/nvidia/version does not exist\n",
      "2021-06-02 01:37:22.447549: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-06-02 01:37:22.455160: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200150000 Hz\n",
      "2021-06-02 01:37:22.455932: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55da1083c240 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-06-02 01:37:22.455964: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "2021-06-02 01:37:26.415854: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-02 01:37:26.440022: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-02 01:37:27.488060: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=babyweight_trained\n",
    "rm -rf ${OUTDIR}\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/babyweight\n",
    "python3 -m trainer.task \\\n",
    "    --job-dir=./tmp \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --batch_size=32 \\\n",
    "    --num_epochs=1 \\\n",
    "    --train_examples=1 \\\n",
    "    --eval_steps=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Cloud AI Platform\n",
    "\n",
    "Now that we see everything is working locally, it's time to train on the cloud! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit to the Cloud we use [`gcloud ai-platform jobs submit training [jobname]`](https://cloud.google.com/sdk/gcloud/reference/ml-engine/jobs/submit/training) and simply specify some additional parameters for AI Platform Training Service:\n",
    "- jobname: A unique identifier for the Cloud job. We usually append system time to ensure uniqueness\n",
    "- job-dir: A GCS location to upload the Python package to\n",
    "- runtime-version: Version of TF to use.\n",
    "- python-version: Version of Python to use. Currently only Python 3.7 is supported for TF 2.1.\n",
    "- region: Cloud region to train in. See [here](https://cloud.google.com/ml-engine/docs/tensorflow/regions) for supported AI Platform Training Service regions\n",
    "\n",
    "Below the `-- \\` we add in the arguments for our `task.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: babyweight_210602_013729\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [babyweight_210602_013729] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe babyweight_210602_013729\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs babyweight_210602_013729\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model\n",
    "JOBID=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBID} \\\n",
    "    --region=${REGION} \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/babyweight/trainer \\\n",
    "    --job-dir=${OUTDIR} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --master-machine-type=n1-standard-8 \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    --python-version=${PYTHONVERSION} \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --num_epochs=10 \\\n",
    "    --train_examples=10000 \\\n",
    "    --eval_steps=100 \\\n",
    "    --batch_size=32 \\\n",
    "    --nembeds=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training job should complete within 10 to 15 minutes. You do not need to wait for this training job to finish before moving forward in the notebook, but will need a trained model to complete our next lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dockerized module\n",
    "\n",
    "Since we are using TensorFlow 2.3 and it is new, we will use a container image to run the code on AI Platform.\n",
    "\n",
    "Once TensorFlow 2.3 is natively supported on AI Platform, you will be able to simply do (without having to build a container):\n",
    "<pre>\n",
    "gcloud ai-platform jobs submit training ${JOBNAME} \\\n",
    "    --region=${REGION} \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/babyweight/trainer \\\n",
    "    --job-dir=${OUTDIR} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --scale-tier=STANDARD_1 \\\n",
    "    --runtime-version=${TFVERSION} \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --num_epochs=10 \\\n",
    "    --train_examples=20000 \\\n",
    "    --eval_steps=100 \\\n",
    "    --batch_size=32 \\\n",
    "    --nembeds=8\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dockerfile\n",
    "\n",
    "We need to create a container with everything we need to be able to run our model. This includes our trainer module package, python3, as well as the libraries we use such as the most up to date TensorFlow 2.0 version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting babyweight/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile babyweight/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu\n",
    "COPY trainer /babyweight/trainer\n",
    "RUN apt update && \\\n",
    "    apt install --yes python3-pip && \\\n",
    "    pip3 install --upgrade --quiet tensorflow==2.1 && \\\n",
    "    pip3 install --upgrade --quiet cloudml-hypertune\n",
    "\n",
    "ENV PYTHONPATH ${PYTHONPATH}:/babyweight\n",
    "ENTRYPOINT [\"python3\", \"babyweight/trainer/task.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and push container image to repo\n",
    "\n",
    "Now that we have created our Dockerfile, we need to build and push our container image to our project's container repo. To do this, we'll create a small shell script that we can call from the bash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting babyweight/push_docker.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile babyweight/push_docker.sh\n",
    "export PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "export IMAGE_REPO_NAME=babyweight_training_container\n",
    "export IMAGE_URI=gcr.io/${PROJECT_ID}/${IMAGE_REPO_NAME}\n",
    "\n",
    "echo \"Building  $IMAGE_URI\"\n",
    "docker build -f Dockerfile -t ${IMAGE_URI} ./\n",
    "echo \"Pushing $IMAGE_URI\"\n",
    "docker push ${IMAGE_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you get a permissions/stat error when running push_docker.sh from Notebooks, do it from CloudShell:\n",
    "\n",
    "Open CloudShell on the GCP Console\n",
    "* git clone https://github.com/GoogleCloudPlatform/training-data-analyst\n",
    "* cd training-data-analyst/courses/machine_learning/deepdive2/structured/solutions/babyweight\n",
    "* bash push_docker.sh\n",
    "\n",
    "This step takes 5-10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building  gcr.io/qwiklabs-gcp-00-0db9b1bc58c6/babyweight_training_container\n",
      "Sending build context to Docker daemon  23.55kB\n",
      "Step 1/5 : FROM gcr.io/deeplearning-platform-release/tf2-cpu\n",
      " ---> 4412d1fc4180\n",
      "Step 2/5 : COPY trainer /babyweight/trainer\n",
      " ---> Using cache\n",
      " ---> b7e383fa5b58\n",
      "Step 3/5 : RUN apt update &&     apt install --yes python3-pip &&     pip3 install --upgrade --quiet tensorflow==2.1 &&     pip3 install --upgrade --quiet cloudml-hypertune\n",
      " ---> Running in 7c98df809936\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\u001b[0mGet:1 http://packages.cloud.google.com/apt gcsfuse-bionic InRelease [5394 B]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:4 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease [6780 B]\n",
      "Get:5 http://packages.cloud.google.com/apt gcsfuse-bionic/main amd64 Packages [2649 B]\n",
      "Get:6 http://packages.cloud.google.com/apt cloud-sdk-bionic/main amd64 Packages [185 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1413 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:10 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2153 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]\n",
      "Get:12 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [423 kB]\n",
      "Get:13 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [24.7 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2586 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2184 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [31.6 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [452 kB]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]\n",
      "Fetched 22.9 MB in 4s (6294 kB/s)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "14 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "\u001b[91m\n",
      "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
      "\n",
      "\u001b[0mReading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "python3-pip is already the newest version (9.0.1-2.3~ubuntu1.18.04.4).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx-bsl 0.30.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.4.0 which is incompatible.\n",
      "tfx-bsl 0.30.0 requires pyarrow<3,>=1, but you have pyarrow 4.0.0 which is incompatible.\n",
      "tfx-bsl 0.30.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<3,>=1.15.2, but you have tensorflow 2.1.0 which is incompatible.\n",
      "tensorflow-transform 0.30.0 requires pyarrow<3,>=1, but you have pyarrow 4.0.0 which is incompatible.\n",
      "tensorflow-transform 0.30.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,<2.5,>=1.15.2, but you have tensorflow 2.1.0 which is incompatible.\n",
      "tensorflow-serving-api 2.4.0 requires tensorflow<3,>=2.4.0, but you have tensorflow 2.1.0 which is incompatible.\n",
      "tensorflow-probability 0.12.2 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
      "tensorflow-io 0.17.0 requires tensorflow<2.5.0,>=2.4.0, but you have tensorflow 2.1.0 which is incompatible.\n",
      "tensorflow-cloud 0.1.14 requires tensorboard>=2.3.0, but you have tensorboard 2.1.1 which is incompatible.\n",
      "phik 0.11.2 requires scipy>=1.5.2, but you have scipy 1.4.1 which is incompatible.\n",
      "keras 2.4.0 requires tensorflow>=2.2.0, but you have tensorflow 2.1.0 which is incompatible.\n",
      "WARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 7c98df809936\n",
      " ---> c12e873ca4ea\n",
      "Step 4/5 : ENV PYTHONPATH ${PYTHONPATH}:/babyweight\n",
      " ---> Running in eb8f69b36816\n",
      "Removing intermediate container eb8f69b36816\n",
      " ---> 7f73a810e852\n",
      "Step 5/5 : ENTRYPOINT [\"python3\", \"babyweight/trainer/task.py\"]\n",
      " ---> Running in 19cbdcbd4ea2\n",
      "Removing intermediate container 19cbdcbd4ea2\n",
      " ---> 79035261abb6\n",
      "Successfully built 79035261abb6\n",
      "Successfully tagged gcr.io/qwiklabs-gcp-00-0db9b1bc58c6/babyweight_training_container:latest\n",
      "Pushing gcr.io/qwiklabs-gcp-00-0db9b1bc58c6/babyweight_training_container\n",
      "Using default tag: latest\n",
      "The push refers to repository [gcr.io/qwiklabs-gcp-00-0db9b1bc58c6/babyweight_training_container]\n",
      "31482f095f9e: Preparing\n",
      "733a536216cd: Preparing\n",
      "a0ae98bd4003: Preparing\n",
      "c2e235b839d3: Preparing\n",
      "c11a20829af6: Preparing\n",
      "cd548db25290: Preparing\n",
      "a30699d982dd: Preparing\n",
      "a4b46603d114: Preparing\n",
      "0ca1c97a79f1: Preparing\n",
      "0ec2e02b8502: Preparing\n",
      "38308b24b7eb: Preparing\n",
      "087a3476a228: Preparing\n",
      "bba536aef700: Preparing\n",
      "7adab15561dc: Preparing\n",
      "e51439c5a810: Preparing\n",
      "05381c8b1ef8: Preparing\n",
      "5f70bf18a086: Preparing\n",
      "af6ac0db4d20: Preparing\n",
      "fc021b2df1ec: Preparing\n",
      "8f39db3c5655: Preparing\n",
      "8cafc6d2db45: Preparing\n",
      "a5d4bacb0351: Preparing\n",
      "5153e1acaabc: Preparing\n",
      "cd548db25290: Waiting\n",
      "a30699d982dd: Waiting\n",
      "a4b46603d114: Waiting\n",
      "0ca1c97a79f1: Waiting\n",
      "0ec2e02b8502: Waiting\n",
      "38308b24b7eb: Waiting\n",
      "087a3476a228: Waiting\n",
      "bba536aef700: Waiting\n",
      "7adab15561dc: Waiting\n",
      "e51439c5a810: Waiting\n",
      "05381c8b1ef8: Waiting\n",
      "5f70bf18a086: Waiting\n",
      "af6ac0db4d20: Waiting\n",
      "fc021b2df1ec: Waiting\n",
      "8f39db3c5655: Waiting\n",
      "8cafc6d2db45: Waiting\n",
      "a5d4bacb0351: Waiting\n",
      "5153e1acaabc: Waiting\n",
      "c2e235b839d3: Layer already exists\n",
      "a0ae98bd4003: Layer already exists\n",
      "c11a20829af6: Layer already exists\n",
      "cd548db25290: Layer already exists\n",
      "a30699d982dd: Layer already exists\n",
      "a4b46603d114: Layer already exists\n",
      "0ca1c97a79f1: Layer already exists\n",
      "0ec2e02b8502: Layer already exists\n",
      "38308b24b7eb: Layer already exists\n",
      "087a3476a228: Layer already exists\n",
      "bba536aef700: Layer already exists\n",
      "7adab15561dc: Layer already exists\n",
      "e51439c5a810: Layer already exists\n",
      "05381c8b1ef8: Layer already exists\n",
      "5f70bf18a086: Layer already exists\n",
      "af6ac0db4d20: Layer already exists\n",
      "fc021b2df1ec: Layer already exists\n",
      "8f39db3c5655: Layer already exists\n",
      "8cafc6d2db45: Layer already exists\n",
      "a5d4bacb0351: Layer already exists\n",
      "5153e1acaabc: Layer already exists\n",
      "733a536216cd: Pushed\n",
      "31482f095f9e: Pushed\n",
      "latest: digest: sha256:8f6a022eb9ffedc397c5dd4607f25214ee296f49495b9b50274e1760fbb6ccd5 size: 5132\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd babyweight\n",
    "bash push_docker.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kindly ignore the incompatibility errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test container locally\n",
    "\n",
    "Before we submit our training job to Cloud AI Platform, let's make sure our container that we just built and pushed to our project's container repo works perfectly. We can do that by calling our container in bash and passing the necessary user_args for our task.py's parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  gcr.io/qwiklabs-gcp-00-0db9b1bc58c6/babyweight_training_container\n",
      "Here is our Wide-and-Deep architecture so far:\n",
      "\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "gestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "is_male (InputLayer)            [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "mother_age (InputLayer)         [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "plurality (InputLayer)          [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "deep_inputs (DenseFeatures)     (None, 10)           8000        gestation_weeks[0][0]            \n",
      "                                                                 is_male[0][0]                    \n",
      "                                                                 mother_age[0][0]                 \n",
      "                                                                 plurality[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "deep_h1 (Dense)                 (None, 32)           352         deep_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "wide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "                                                                 is_male[0][0]                    \n",
      "                                                                 mother_age[0][0]                 \n",
      "                                                                 plurality[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "deep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "wide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "both (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "                                                                 wide_h[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 9,209\n",
      "Trainable params: 9,209\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Train for 10 steps, validate for 1 steps\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 8s - loss: 31.4985 - rmse: 5.5769 - mse: 31.4985 - val_loss: 24.2260 - val_rmse: 4.9220 - val_mse: 24.2260\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 6s - loss: 25.4964 - rmse: 5.0274 - mse: 25.4964 - val_loss: 23.6400 - val_rmse: 4.8621 - val_mse: 23.6400\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 5s - loss: 26.8645 - rmse: 5.1241 - mse: 26.8645 - val_loss: 23.3698 - val_rmse: 4.8342 - val_mse: 23.3698\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 6s - loss: 22.8300 - rmse: 4.7533 - mse: 22.8300 - val_loss: 24.2218 - val_rmse: 4.9216 - val_mse: 24.2218\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 6s - loss: 21.1460 - rmse: 4.5644 - mse: 21.1460 - val_loss: 22.8688 - val_rmse: 4.7821 - val_mse: 22.8688\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 6s - loss: 22.5154 - rmse: 4.6967 - mse: 22.5154 - val_loss: 22.6532 - val_rmse: 4.7595 - val_mse: 22.6532\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 5s - loss: 23.4264 - rmse: 4.7716 - mse: 23.4264 - val_loss: 21.8388 - val_rmse: 4.6732 - val_mse: 21.8388\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 5s - loss: 24.5371 - rmse: 4.9288 - mse: 24.5371 - val_loss: 19.4242 - val_rmse: 4.4073 - val_mse: 19.4242\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 6s - loss: 17.3002 - rmse: 4.0165 - mse: 17.3002 - val_loss: 19.9604 - val_rmse: 4.4677 - val_mse: 19.9604\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "10/10 - 6s - loss: 17.7650 - rmse: 4.1965 - mse: 17.7650 - val_loss: 19.4957 - val_rmse: 4.4154 - val_mse: 19.4957\n",
      "Exported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/20210602014329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-02 01:42:25.609124: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "2021-06-02 01:42:25.609343: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "2021-06-02 01:42:25.609367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "2021-06-02 01:42:28.605868: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-06-02 01:42:28.605924: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-06-02 01:42:28.605960: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (1664955a452c): /proc/driver/nvidia/version does not exist\n",
      "2021-06-02 01:42:28.606349: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-06-02 01:42:28.616429: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200150000 Hz\n",
      "2021-06-02 01:42:28.616899: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56358fedb5a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-06-02 01:42:28.617000: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "2021-06-02 01:42:31.936829: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-02 01:42:37.788589: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-02 01:42:43.465052: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-02 01:42:48.827685: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-02 01:42:54.614445: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-02 01:43:00.674164: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-02 01:43:06.319803: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-02 01:43:11.730880: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-02 01:43:17.163108: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-02 01:43:23.726355: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-02 01:43:28.986840: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2021-06-02 01:43:29.998280: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export PROJECT_ID=$(gcloud config list project --format \"value(core.project)\")\n",
    "export IMAGE_REPO_NAME=babyweight_training_container\n",
    "export IMAGE_URI=gcr.io/${PROJECT_ID}/${IMAGE_REPO_NAME}\n",
    "echo \"Running  $IMAGE_URI\"\n",
    "docker run ${IMAGE_URI} \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv \\\n",
    "    --output_dir=gs://${BUCKET}/babyweight/trained_model \\\n",
    "    --batch_size=10 \\\n",
    "    --num_epochs=10 \\\n",
    "    --train_examples=1 \\\n",
    "    --eval_steps=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task #5: Train on Cloud AI Platform.\n",
    "\n",
    "Once the code works in standalone mode, you can run it on Cloud AI Platform. Because this is on the entire dataset, it will take a while. The training run took about <b> two hours </b> for me. You can monitor the job from the GCP console in the Cloud AI Platform section. Complete the __#TODO__s to make sure you have the necessary user_args for our task.py's parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/data/eval000000000000.csv\n",
      "gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/data/eval000000000001.csv\n",
      "gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/data/train000000000000.csv\n",
      "gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/data/train000000000001.csv\n",
      "gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/data/train000000000002.csv\n",
      "gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/data/train000000000003.csv\n",
      "gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/data/train000000000004.csv\n",
      "gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/data/train000000000005.csv\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://{BUCKET}/babyweight/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model us-central1\n",
      "jobId: babyweight_210602_014638\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [babyweight_210602_014638] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe babyweight_210602_014638\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs babyweight_210602_014638\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model\n",
    "JOBID=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBNAME}\n",
    "# gsutil -m rm -rf ${OUTDIR}\n",
    "\n",
    "IMAGE=gcr.io/${PROJECT}/babyweight_training_container\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBID} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --region=${REGION} \\\n",
    "    --master-image-uri=${IMAGE} \\\n",
    "    --master-machine-type=n1-standard-4 \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --num_epochs=10 \\\n",
    "    --train_examples=2000000 \\\n",
    "    --eval_steps=32 \\\n",
    "    --batch_size=32 \\\n",
    "    --nembeds=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2021-06-02 01:46:39 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2021-06-02 01:46:39 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2021-06-02 01:46:40 +0000\tservice\t\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 01:46:40 +0000\tservice\t\tJob babyweight_210602_014638 is queued.\n",
      "INFO\t2021-06-02 01:46:41 +0000\tservice\t\tWaiting for training program to start.\n",
      "INFO\t2021-06-02 01:47:07 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:47:07 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:47:07 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:47:07 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:47:07 +0000\tmaster-replica-0\t\t\n",
      "ERROR\t2021-06-02 01:51:10 +0000\tmaster-replica-0\t\t2021-06-02 01:51:10.064896: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:51:10 +0000\tmaster-replica-0\t\t2021-06-02 01:51:10.065157: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:51:10 +0000\tmaster-replica-0\t\t2021-06-02 01:51:10.065189: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\t2021-06-02 01:51:11.782150: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\t2021-06-02 01:51:11.782204: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\t2021-06-02 01:51:11.782251: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-3763549999219918114): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\t2021-06-02 01:51:11.782536: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\t2021-06-02 01:51:11.794845: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\t2021-06-02 01:51:11.795302: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56149836d160 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\t2021-06-02 01:51:11.795390: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:51:11 +0000\tmaster-replica-0\t\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:51:23 +0000\tmaster-replica-0\t\t2021-06-02 01:51:23.624436: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tModel: \"model\"\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t==================================================================================================\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tdeep_inputs (DenseFeatures)     (None, 34)           32000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tdeep_h1 (Dense)                 (None, 32)           1120        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t==================================================================================================\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tTotal params: 33,977\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tTrainable params: 33,977\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tNone\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tTrain for 3125 steps, validate for 32 steps\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tEpoch 1/10\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:51:27 +0000\tmaster-replica-0\t\t3125/3125 - 15s - loss: 2.7006 - rmse: 1.5579 - mse: 2.7006 - val_loss: 2.2293 - val_rmse: 1.4924 - val_mse: 2.2293\n",
      "ERROR\t2021-06-02 01:51:36 +0000\tmaster-replica-0\t\t2021-06-02 01:51:36.024233: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:51:39 +0000\tmaster-replica-0\t\tEpoch 2/10\n",
      "INFO\t2021-06-02 01:51:39 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:51:39 +0000\tmaster-replica-0\t\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:51:39 +0000\tmaster-replica-0\t\t3125/3125 - 12s - loss: 1.9407 - rmse: 1.3751 - mse: 1.9407 - val_loss: 2.2297 - val_rmse: 1.4925 - val_mse: 2.2297\n",
      "ERROR\t2021-06-02 01:51:47 +0000\tmaster-replica-0\t\t2021-06-02 01:51:47.778784: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:51:51 +0000\tmaster-replica-0\t\tEpoch 3/10\n",
      "INFO\t2021-06-02 01:51:51 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:51:51 +0000\tmaster-replica-0\t\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:51:51 +0000\tmaster-replica-0\t\t3125/3125 - 12s - loss: 2.1391 - rmse: 1.4466 - mse: 2.1391 - val_loss: 2.2330 - val_rmse: 1.4939 - val_mse: 2.2330\n",
      "ERROR\t2021-06-02 01:51:59 +0000\tmaster-replica-0\t\t2021-06-02 01:51:59.183982: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:52:02 +0000\tmaster-replica-0\t\tEpoch 4/10\n",
      "INFO\t2021-06-02 01:52:02 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:52:02 +0000\tmaster-replica-0\t\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:52:02 +0000\tmaster-replica-0\t\t3125/3125 - 11s - loss: 1.9647 - rmse: 1.3892 - mse: 1.9647 - val_loss: 2.2866 - val_rmse: 1.5115 - val_mse: 2.2866\n",
      "ERROR\t2021-06-02 01:52:10 +0000\tmaster-replica-0\t\t2021-06-02 01:52:10.347063: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:52:14 +0000\tmaster-replica-0\t\tEpoch 5/10\n",
      "INFO\t2021-06-02 01:52:14 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:52:14 +0000\tmaster-replica-0\t\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:52:14 +0000\tmaster-replica-0\t\t3125/3125 - 12s - loss: 1.7325 - rmse: 1.3051 - mse: 1.7325 - val_loss: 2.4416 - val_rmse: 1.5621 - val_mse: 2.4416\n",
      "ERROR\t2021-06-02 01:52:22 +0000\tmaster-replica-0\t\t2021-06-02 01:52:22.361523: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:52:27 +0000\tmaster-replica-0\t\tEpoch 6/10\n",
      "INFO\t2021-06-02 01:52:27 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:52:27 +0000\tmaster-replica-0\t\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:52:27 +0000\tmaster-replica-0\t\t3125/3125 - 13s - loss: 1.5202 - rmse: 1.2211 - mse: 1.5202 - val_loss: 2.4365 - val_rmse: 1.5605 - val_mse: 2.4365\n",
      "ERROR\t2021-06-02 01:52:34 +0000\tmaster-replica-0\t\t2021-06-02 01:52:34.896969: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:52:39 +0000\tmaster-replica-0\t\tEpoch 7/10\n",
      "INFO\t2021-06-02 01:52:39 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:52:39 +0000\tmaster-replica-0\t\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:52:39 +0000\tmaster-replica-0\t\t3125/3125 - 12s - loss: 1.4335 - rmse: 1.1855 - mse: 1.4335 - val_loss: 2.5310 - val_rmse: 1.5905 - val_mse: 2.5310\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t\t2021-06-02 01:52:46.900643: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:52:52 +0000\tmaster-replica-0\t\tEpoch 8/10\n",
      "INFO\t2021-06-02 01:52:52 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:52:52 +0000\tmaster-replica-0\t\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:52:52 +0000\tmaster-replica-0\t\t3125/3125 - 13s - loss: 1.3275 - rmse: 1.1395 - mse: 1.3275 - val_loss: 2.7831 - val_rmse: 1.6680 - val_mse: 2.7831\n",
      "ERROR\t2021-06-02 01:53:00 +0000\tmaster-replica-0\t\t2021-06-02 01:53:00.389560: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:53:05 +0000\tmaster-replica-0\t\tEpoch 9/10\n",
      "INFO\t2021-06-02 01:53:05 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:53:05 +0000\tmaster-replica-0\t\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:53:05 +0000\tmaster-replica-0\t\t3125/3125 - 14s - loss: 1.2031 - rmse: 1.0855 - mse: 1.2030 - val_loss: 3.0487 - val_rmse: 1.7458 - val_mse: 3.0487\n",
      "ERROR\t2021-06-02 01:53:13 +0000\tmaster-replica-0\t\t2021-06-02 01:53:13.653582: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:53:19 +0000\tmaster-replica-0\t\tEpoch 10/10\n",
      "INFO\t2021-06-02 01:53:19 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:53:19 +0000\tmaster-replica-0\t\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:53:19 +0000\tmaster-replica-0\t\t3125/3125 - 13s - loss: 1.2028 - rmse: 1.0855 - mse: 1.2028 - val_loss: 3.2193 - val_rmse: 1.7941 - val_mse: 3.2193\n",
      "ERROR\t2021-06-02 01:53:19 +0000\tmaster-replica-0\t\t2021-06-02 01:53:19.049224: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 01:53:20 +0000\tmaster-replica-0\t\t2021-06-02 01:53:20.966470: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 01:53:21 +0000\tmaster-replica-0\t\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:53:21 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:53:21 +0000\tmaster-replica-0\t\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 01:53:27 +0000\tmaster-replica-0\t\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model/20210602015319\n",
      "INFO\t2021-06-02 01:55:44 +0000\tservice\t\tJob completed successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs stream-logs babyweight_210602_014638"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I ran it, I used train_examples=2000000. When training finished, I filtered in the Stackdriver log on the word \"dict\" and saw that the last line was:\n",
    "<pre>\n",
    "Saving dict for global step 5714290: average_loss = 1.06473, global_step = 5714290, loss = 34882.4, rmse = 1.03186\n",
    "</pre>\n",
    "The final RMSE was 1.03 pounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task #6: Hyperparameter tuning.\n",
    "\n",
    "All of these are command-line parameters to my program.  To do hyperparameter tuning, create `hyperparam.yaml` and pass it as `--config hyperparam.yaml`.\n",
    "This step will take <b>up to 2 hours</b> -- you can increase `maxParallelTrials` or reduce `maxTrials` to get it done faster.  Since `maxParallelTrials` is the number of initial seeds to start searching from, you don't want it to be too large; otherwise, all you have is a random search. Complete __#TODO__s in yaml file and gcloud training job bash command so that we can run hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "    scaleTier: STANDARD_1\n",
    "    hyperparameters:\n",
    "        hyperparameterMetricTag: rmse # TODO: Add metric we want to optimize\n",
    "        goal: MINIMIZE # TODO: MAXIMIZE or MINIMIZE?\n",
    "        maxTrials: 20\n",
    "        maxParallelTrials: 5  # 순차적으로 실행할 경우 다음 train 최적화에 도움이 되지만 속도 저하. 병렬 처리는 그와 반대\n",
    "        enableTrialEarlyStopping: True\n",
    "        params:\n",
    "        - parameterName: batch_size\n",
    "          type: INTEGER\n",
    "          minValue: 10\n",
    "          maxValue: 50\n",
    "          scaleType: UNIT_LINEAR_SCALE \n",
    "        - parameterName: nembeds\n",
    "          type: INTEGER\n",
    "          minValue: 4\n",
    "          maxValue: 64\n",
    "          scaleType: UNIT_LINEAR_SCALE \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam us-central1 babyweight_210602_014807\n",
      "jobId: babyweight_210602_014807\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [babyweight_210602_014807] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe babyweight_210602_014807\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs babyweight_210602_014807\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/hyperparam\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBNAME}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "\n",
    "IMAGE=gcr.io/${PROJECT}/babyweight_training_container\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBNAME} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --region=${REGION} \\\n",
    "    --master-image-uri=${IMAGE} \\\n",
    "    --master-machine-type=n1-standard-4 \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --num_epochs=10 \\\n",
    "    --train_examples=20000 \\\n",
    "    --eval_steps=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2021-06-02 01:48:09 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2021-06-02 01:48:10 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2021-06-02 01:48:10 +0000\tservice\t\tJob babyweight_210602_014807 is queued.\n",
      "INFO\t2021-06-02 01:48:17 +0000\tservice\t3\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 01:48:17 +0000\tservice\t4\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 01:48:17 +0000\tservice\t2\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 01:48:17 +0000\tservice\t5\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 01:48:17 +0000\tservice\t1\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 01:48:18 +0000\tservice\t3\tWaiting for training program to start.\n",
      "INFO\t2021-06-02 01:48:18 +0000\tservice\t1\tWaiting for training program to start.\n",
      "INFO\t2021-06-02 01:48:18 +0000\tservice\t2\tWaiting for training program to start.\n",
      "INFO\t2021-06-02 01:48:19 +0000\tservice\t4\tWaiting for training program to start.\n",
      "INFO\t2021-06-02 01:48:19 +0000\tservice\t5\tWaiting for training program to start.\n",
      "INFO\t2021-06-02 01:48:43 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 01:48:43 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 01:48:43 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 01:48:43 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 01:48:43 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 01:48:43 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 01:48:43 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 01:48:43 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 01:48:43 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 01:48:43 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 01:48:45 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 01:48:45 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 01:48:45 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 01:48:45 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 01:48:45 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 01:48:46 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 01:48:46 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 01:48:46 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 01:48:46 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 01:48:46 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 01:48:46 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 01:48:46 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 01:48:46 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 01:48:46 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 01:48:46 +0000\tmaster-replica-0\t1\t\n",
      "ERROR\t2021-06-02 01:52:44 +0000\tmaster-replica-0\t4\t2021-06-02 01:52:44.997264: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:44 +0000\tmaster-replica-0\t4\t2021-06-02 01:52:44.997505: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:44 +0000\tmaster-replica-0\t4\t2021-06-02 01:52:44.997554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t5\t2021-06-02 01:52:46.258717: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t5\t2021-06-02 01:52:46.259065: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t5\t2021-06-02 01:52:46.259101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\t2021-06-02 01:52:46.562894: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\t2021-06-02 01:52:46.563011: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\t2021-06-02 01:52:46.563081: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-17548160141325346936): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\t2021-06-02 01:52:46.563571: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\t2021-06-02 01:52:46.576977: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\t2021-06-02 01:52:46.577904: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5619afff2b10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\t2021-06-02 01:52:46.577953: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t4\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t2\t2021-06-02 01:52:46.838052: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t2\t2021-06-02 01:52:46.838307: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:46 +0000\tmaster-replica-0\t2\t2021-06-02 01:52:46.838353: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 01:52:47 +0000\tmaster-replica-0\t3\t2021-06-02 01:52:47.084429: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:47 +0000\tmaster-replica-0\t3\t2021-06-02 01:52:47.084662: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:47 +0000\tmaster-replica-0\t3\t2021-06-02 01:52:47.084734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\t2021-06-02 01:52:48.022673: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\t2021-06-02 01:52:48.022730: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\t2021-06-02 01:52:48.022763: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-11076356729456979663): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\t2021-06-02 01:52:48.023119: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\t2021-06-02 01:52:48.032709: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\t2021-06-02 01:52:48.033180: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557b61b4dea0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\t2021-06-02 01:52:48.033231: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t5\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\t2021-06-02 01:52:48.711869: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\t2021-06-02 01:52:48.711953: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\t2021-06-02 01:52:48.711999: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-3407653837999435090): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\t2021-06-02 01:52:48.712290: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\t2021-06-02 01:52:48.722595: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\t2021-06-02 01:52:48.723124: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5630dd9076c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\t2021-06-02 01:52:48.723180: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t3\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\t2021-06-02 01:52:48.736265: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\t2021-06-02 01:52:48.736396: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\t2021-06-02 01:52:48.736495: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-16227380624239418344): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\t2021-06-02 01:52:48.736990: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\t2021-06-02 01:52:48.751253: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\t2021-06-02 01:52:48.751766: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561a2f2a36d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\t2021-06-02 01:52:48.751836: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:48 +0000\tmaster-replica-0\t2\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:51 +0000\tmaster-replica-0\t1\t2021-06-02 01:52:51.301601: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:51 +0000\tmaster-replica-0\t1\t2021-06-02 01:52:51.301891: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:51 +0000\tmaster-replica-0\t1\t2021-06-02 01:52:51.301930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\t2021-06-02 01:52:53.371634: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\t2021-06-02 01:52:53.371699: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\t2021-06-02 01:52:53.371742: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-17249220118386170184): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\t2021-06-02 01:52:53.372316: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\t2021-06-02 01:52:53.387075: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\t2021-06-02 01:52:53.387911: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fbf3dbfc40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\t2021-06-02 01:52:53.387974: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:53 +0000\tmaster-replica-0\t1\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:54:41 +0000\tmaster-replica-0\t5\t2021-06-02 01:54:41.758160: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tModel: \"model\"\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t==================================================================================================\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tdeep_inputs (DenseFeatures)     (None, 52)           50000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tdeep_h1 (Dense)                 (None, 32)           1696        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t==================================================================================================\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tTotal params: 52,553\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tTrainable params: 52,553\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tNone\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tTrain for 48780 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tEpoch 1/10\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/5/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:54:46 +0000\tmaster-replica-0\t5\t48780/48780 - 118s - loss: 1.3911 - rmse: 1.1567 - mse: 1.3911 - val_loss: 4.3075 - val_rmse: 2.0748 - val_mse: 4.3075\n",
      "ERROR\t2021-06-02 01:55:24 +0000\tmaster-replica-0\t1\t2021-06-02 01:55:24.927821: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tModel: \"model\"\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t==================================================================================================\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tdeep_inputs (DenseFeatures)     (None, 36)           34000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tdeep_h1 (Dense)                 (None, 32)           1184        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t==================================================================================================\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tTotal params: 36,041\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tTrainable params: 36,041\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tNone\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tTrain for 66666 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tEpoch 1/10\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/1/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:55:29 +0000\tmaster-replica-0\t1\t66666/66666 - 156s - loss: 1.3871 - rmse: 1.1525 - mse: 1.3871 - val_loss: 4.4181 - val_rmse: 2.1015 - val_mse: 4.4181\n",
      "ERROR\t2021-06-02 01:56:28 +0000\tmaster-replica-0\t2\t2021-06-02 01:56:28.832663: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tModel: \"model\"\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t==================================================================================================\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tdeep_inputs (DenseFeatures)     (None, 50)           48000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tdeep_h1 (Dense)                 (None, 32)           1632        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t==================================================================================================\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tTotal params: 50,489\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tTrainable params: 50,489\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tNone\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tTrain for 95238 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tEpoch 1/10\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/2/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:56:33 +0000\tmaster-replica-0\t2\t95238/95238 - 224s - loss: 1.3847 - rmse: 1.1464 - mse: 1.3847 - val_loss: 5.2017 - val_rmse: 2.2800 - val_mse: 5.2017\n",
      "ERROR\t2021-06-02 01:56:35 +0000\tmaster-replica-0\t5\t2021-06-02 01:56:35.910783: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:56:39 +0000\tmaster-replica-0\t5\tEpoch 2/10\n",
      "INFO\t2021-06-02 01:56:39 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 01:56:39 +0000\tmaster-replica-0\t5\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/5/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:56:39 +0000\tmaster-replica-0\t5\t48780/48780 - 113s - loss: 0.9583 - rmse: 0.9705 - mse: 0.9583 - val_loss: 6.4434 - val_rmse: 2.5380 - val_mse: 6.4434\n",
      "ERROR\t2021-06-02 01:56:49 +0000\tmaster-replica-0\t3\t2021-06-02 01:56:49.674138: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tModel: \"model\"\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t==================================================================================================\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tdeep_inputs (DenseFeatures)     (None, 21)           19000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tdeep_h1 (Dense)                 (None, 32)           704         deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t==================================================================================================\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tTotal params: 20,561\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tTrainable params: 20,561\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tNone\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tTrain for 95238 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tEpoch 1/10\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/3/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:56:54 +0000\tmaster-replica-0\t3\t95238/95238 - 245s - loss: 1.3875 - rmse: 1.1473 - mse: 1.3875 - val_loss: 5.0406 - val_rmse: 2.2444 - val_mse: 5.0406\n",
      "ERROR\t2021-06-02 01:58:00 +0000\tmaster-replica-0\t1\t2021-06-02 01:58:00.784890: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:58:04 +0000\tmaster-replica-0\t1\tEpoch 2/10\n",
      "INFO\t2021-06-02 01:58:04 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 01:58:04 +0000\tmaster-replica-0\t1\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/1/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:58:04 +0000\tmaster-replica-0\t1\t66666/66666 - 155s - loss: 0.9598 - rmse: 0.9683 - mse: 0.9599 - val_loss: 5.6396 - val_rmse: 2.3743 - val_mse: 5.6396\n",
      "ERROR\t2021-06-02 01:58:30 +0000\tmaster-replica-0\t5\t2021-06-02 01:58:30.079714: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:58:33 +0000\tmaster-replica-0\t5\tEpoch 3/10\n",
      "INFO\t2021-06-02 01:58:33 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 01:58:33 +0000\tmaster-replica-0\t5\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/5/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:58:33 +0000\tmaster-replica-0\t5\t48780/48780 - 114s - loss: 0.9474 - rmse: 0.9653 - mse: 0.9474 - val_loss: 6.6539 - val_rmse: 2.5790 - val_mse: 6.6539\n",
      "ERROR\t2021-06-02 01:58:55 +0000\tmaster-replica-0\t4\t2021-06-02 01:58:55.979572: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tModel: \"model\"\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t==================================================================================================\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tdeep_inputs (DenseFeatures)     (None, 37)           35000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tdeep_h1 (Dense)                 (None, 32)           1216        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t==================================================================================================\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tTotal params: 37,073\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tTrainable params: 37,073\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tNone\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tTrain for 200000 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tEpoch 1/10\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/4/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:59:00 +0000\tmaster-replica-0\t4\t200000/200000 - 374s - loss: 1.3829 - rmse: 1.1284 - mse: 1.3829 - val_loss: 5.4030 - val_rmse: 2.3239 - val_mse: 5.4030\n",
      "ERROR\t2021-06-02 02:00:07 +0000\tmaster-replica-0\t2\t2021-06-02 02:00:07.864970: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:00:11 +0000\tmaster-replica-0\t2\tEpoch 2/10\n",
      "INFO\t2021-06-02 02:00:11 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 02:00:11 +0000\tmaster-replica-0\t2\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/2/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:00:11 +0000\tmaster-replica-0\t2\t95238/95238 - 218s - loss: 0.9588 - rmse: 0.9634 - mse: 0.9588 - val_loss: 6.8802 - val_rmse: 2.6225 - val_mse: 6.8802\n",
      "ERROR\t2021-06-02 02:00:22 +0000\tmaster-replica-0\t5\t2021-06-02 02:00:22.503499: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:00:26 +0000\tmaster-replica-0\t5\tEpoch 4/10\n",
      "INFO\t2021-06-02 02:00:26 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 02:00:26 +0000\tmaster-replica-0\t5\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/5/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:00:26 +0000\tmaster-replica-0\t5\t48780/48780 - 113s - loss: 0.9692 - rmse: 0.9765 - mse: 0.9692 - val_loss: 7.2181 - val_rmse: 2.6858 - val_mse: 7.2181\n",
      "ERROR\t2021-06-02 02:00:38 +0000\tmaster-replica-0\t1\t2021-06-02 02:00:38.128210: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:00:41 +0000\tmaster-replica-0\t1\tEpoch 3/10\n",
      "INFO\t2021-06-02 02:00:41 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 02:00:41 +0000\tmaster-replica-0\t1\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/1/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:00:41 +0000\tmaster-replica-0\t1\t66666/66666 - 157s - loss: 0.9462 - rmse: 0.9619 - mse: 0.9463 - val_loss: 6.7215 - val_rmse: 2.5918 - val_mse: 6.7215\n",
      "ERROR\t2021-06-02 02:01:00 +0000\tmaster-replica-0\t3\t2021-06-02 02:01:00.720253: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:01:04 +0000\tmaster-replica-0\t3\tEpoch 2/10\n",
      "INFO\t2021-06-02 02:01:04 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 02:01:04 +0000\tmaster-replica-0\t3\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/3/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:01:04 +0000\tmaster-replica-0\t3\t95238/95238 - 250s - loss: 0.9583 - rmse: 0.9633 - mse: 0.9583 - val_loss: 6.1029 - val_rmse: 2.4698 - val_mse: 6.1029\n",
      "ERROR\t2021-06-02 02:02:20 +0000\tmaster-replica-0\t5\t2021-06-02 02:02:20.385141: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:02:24 +0000\tmaster-replica-0\t5\tEpoch 5/10\n",
      "INFO\t2021-06-02 02:02:24 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 02:02:24 +0000\tmaster-replica-0\t5\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/5/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:02:24 +0000\tmaster-replica-0\t5\t48780/48780 - 118s - loss: 1.3755 - rmse: 1.1532 - mse: 1.3755 - val_loss: 2.5963 - val_rmse: 1.6108 - val_mse: 2.5963\n",
      "ERROR\t2021-06-02 02:03:14 +0000\tmaster-replica-0\t1\t2021-06-02 02:03:14.710548: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:03:18 +0000\tmaster-replica-0\t1\tEpoch 4/10\n",
      "INFO\t2021-06-02 02:03:18 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 02:03:18 +0000\tmaster-replica-0\t1\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/1/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:03:18 +0000\tmaster-replica-0\t1\t66666/66666 - 156s - loss: 0.9704 - rmse: 0.9742 - mse: 0.9704 - val_loss: 8.4056 - val_rmse: 2.8985 - val_mse: 8.4056\n",
      "ERROR\t2021-06-02 02:03:38 +0000\tmaster-replica-0\t2\t2021-06-02 02:03:38.532600: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:03:41 +0000\tmaster-replica-0\t2\tEpoch 3/10\n",
      "INFO\t2021-06-02 02:03:41 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 02:03:41 +0000\tmaster-replica-0\t2\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/2/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:03:41 +0000\tmaster-replica-0\t2\t95238/95238 - 210s - loss: 0.9483 - rmse: 0.9584 - mse: 0.9482 - val_loss: 8.1824 - val_rmse: 2.8600 - val_mse: 8.1824\n",
      "ERROR\t2021-06-02 02:04:15 +0000\tmaster-replica-0\t5\t2021-06-02 02:04:15.553179: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:04:19 +0000\tmaster-replica-0\t5\tEpoch 6/10\n",
      "INFO\t2021-06-02 02:04:19 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 02:04:19 +0000\tmaster-replica-0\t5\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/5/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:04:19 +0000\tmaster-replica-0\t5\t48780/48780 - 115s - loss: 0.9981 - rmse: 0.9901 - mse: 0.9981 - val_loss: 3.8394 - val_rmse: 1.9589 - val_mse: 3.8394\n",
      "ERROR\t2021-06-02 02:04:55 +0000\tmaster-replica-0\t3\t2021-06-02 02:04:55.896278: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:04:59 +0000\tmaster-replica-0\t3\tEpoch 3/10\n",
      "INFO\t2021-06-02 02:04:59 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 02:04:59 +0000\tmaster-replica-0\t3\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/3/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:04:59 +0000\tmaster-replica-0\t3\t95238/95238 - 235s - loss: 0.9471 - rmse: 0.9580 - mse: 0.9471 - val_loss: 7.4444 - val_rmse: 2.7276 - val_mse: 7.4444\n",
      "ERROR\t2021-06-02 02:05:03 +0000\tmaster-replica-0\t4\t2021-06-02 02:05:03.967853: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:05:07 +0000\tmaster-replica-0\t4\tEpoch 2/10\n",
      "INFO\t2021-06-02 02:05:07 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 02:05:07 +0000\tmaster-replica-0\t4\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/4/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:05:07 +0000\tmaster-replica-0\t4\t200000/200000 - 367s - loss: 0.9554 - rmse: 0.9462 - mse: 0.9554 - val_loss: 7.4156 - val_rmse: 2.7218 - val_mse: 7.4156\n",
      "ERROR\t2021-06-02 02:05:48 +0000\tmaster-replica-0\t1\t2021-06-02 02:05:48.930030: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:05:52 +0000\tmaster-replica-0\t1\tEpoch 5/10\n",
      "INFO\t2021-06-02 02:05:52 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 02:05:52 +0000\tmaster-replica-0\t1\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/1/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:05:52 +0000\tmaster-replica-0\t1\t66666/66666 - 155s - loss: 1.3781 - rmse: 1.1510 - mse: 1.3781 - val_loss: 3.2249 - val_rmse: 1.7953 - val_mse: 3.2249\n",
      "ERROR\t2021-06-02 02:06:06 +0000\tmaster-replica-0\t5\t2021-06-02 02:06:06.148171: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:06:09 +0000\tmaster-replica-0\t5\tEpoch 7/10\n",
      "INFO\t2021-06-02 02:06:09 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 02:06:09 +0000\tmaster-replica-0\t5\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/5/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:06:09 +0000\tmaster-replica-0\t5\t48780/48780 - 111s - loss: 0.9408 - rmse: 0.9617 - mse: 0.9408 - val_loss: 4.3645 - val_rmse: 2.0885 - val_mse: 4.3645\n",
      "ERROR\t2021-06-02 02:07:11 +0000\tmaster-replica-0\t2\t2021-06-02 02:07:11.320159: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:07:15 +0000\tmaster-replica-0\t2\tEpoch 4/10\n",
      "INFO\t2021-06-02 02:07:15 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 02:07:15 +0000\tmaster-replica-0\t2\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/2/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:07:15 +0000\tmaster-replica-0\t2\t95238/95238 - 214s - loss: 0.9691 - rmse: 0.9693 - mse: 0.9691 - val_loss: 9.6491 - val_rmse: 3.1055 - val_mse: 9.6491\n",
      "ERROR\t2021-06-02 02:07:56 +0000\tmaster-replica-0\t5\t2021-06-02 02:07:56.407524: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:07:59 +0000\tmaster-replica-0\t5\tEpoch 8/10\n",
      "INFO\t2021-06-02 02:07:59 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 02:07:59 +0000\tmaster-replica-0\t5\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/5/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:07:59 +0000\tmaster-replica-0\t5\t48780/48780 - 110s - loss: 0.9529 - rmse: 0.9683 - mse: 0.9529 - val_loss: 5.1396 - val_rmse: 2.2664 - val_mse: 5.1396\n",
      "ERROR\t2021-06-02 02:08:19 +0000\tmaster-replica-0\t1\t2021-06-02 02:08:19.957991: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:08:23 +0000\tmaster-replica-0\t1\tEpoch 6/10\n",
      "INFO\t2021-06-02 02:08:23 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 02:08:23 +0000\tmaster-replica-0\t1\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/1/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:08:23 +0000\tmaster-replica-0\t1\t66666/66666 - 151s - loss: 0.9987 - rmse: 0.9874 - mse: 0.9988 - val_loss: 4.6017 - val_rmse: 2.1445 - val_mse: 4.6017\n",
      "ERROR\t2021-06-02 02:08:40 +0000\tmaster-replica-0\t3\t2021-06-02 02:08:40.887064: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:08:44 +0000\tmaster-replica-0\t3\tEpoch 4/10\n",
      "INFO\t2021-06-02 02:08:44 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 02:08:44 +0000\tmaster-replica-0\t3\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/3/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:08:44 +0000\tmaster-replica-0\t3\t95238/95238 - 225s - loss: 0.9703 - rmse: 0.9699 - mse: 0.9703 - val_loss: 9.8461 - val_rmse: 3.1370 - val_mse: 9.8461\n",
      "ERROR\t2021-06-02 02:09:45 +0000\tmaster-replica-0\t5\t2021-06-02 02:09:45.066369: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:09:48 +0000\tmaster-replica-0\t5\tEpoch 9/10\n",
      "INFO\t2021-06-02 02:09:48 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 02:09:48 +0000\tmaster-replica-0\t5\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/5/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:09:48 +0000\tmaster-replica-0\t5\t48780/48780 - 109s - loss: 1.2281 - rmse: 1.0874 - mse: 1.2281 - val_loss: 2.1272 - val_rmse: 1.4578 - val_mse: 2.1272\n",
      "ERROR\t2021-06-02 02:10:40 +0000\tmaster-replica-0\t2\t2021-06-02 02:10:40.282816: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:10:44 +0000\tmaster-replica-0\t2\tEpoch 5/10\n",
      "INFO\t2021-06-02 02:10:44 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 02:10:44 +0000\tmaster-replica-0\t2\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/2/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:10:44 +0000\tmaster-replica-0\t2\t95238/95238 - 208s - loss: 1.3803 - rmse: 1.1462 - mse: 1.3803 - val_loss: 2.7004 - val_rmse: 1.6422 - val_mse: 2.7004\n",
      "ERROR\t2021-06-02 02:10:49 +0000\tmaster-replica-0\t1\t2021-06-02 02:10:49.155894: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:10:52 +0000\tmaster-replica-0\t1\tEpoch 7/10\n",
      "INFO\t2021-06-02 02:10:52 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 02:10:52 +0000\tmaster-replica-0\t1\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/1/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:10:52 +0000\tmaster-replica-0\t1\t66666/66666 - 149s - loss: 0.9411 - rmse: 0.9590 - mse: 0.9411 - val_loss: 4.9694 - val_rmse: 2.2284 - val_mse: 4.9694\n",
      "INFO\t2021-06-02 02:11:16 +0000\tmaster-replica-0\t4\tEpoch 3/10\n",
      "INFO\t2021-06-02 02:11:16 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 02:11:16 +0000\tmaster-replica-0\t4\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/4/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:11:16 +0000\tmaster-replica-0\t4\t200000/200000 - 368s - loss: 0.9452 - rmse: 0.9420 - mse: 0.9452 - val_loss: 8.8579 - val_rmse: 2.9749 - val_mse: 8.8579\n",
      "ERROR\t2021-06-02 02:11:35 +0000\tmaster-replica-0\t5\t2021-06-02 02:11:35.512328: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:11:38 +0000\tmaster-replica-0\t5\tEpoch 10/10\n",
      "INFO\t2021-06-02 02:11:38 +0000\tmaster-replica-0\t5\t\n",
      "INFO\t2021-06-02 02:11:38 +0000\tmaster-replica-0\t5\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/5/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:11:38 +0000\tmaster-replica-0\t5\t48780/48780 - 111s - loss: 1.1624 - rmse: 1.0648 - mse: 1.1624 - val_loss: 3.4462 - val_rmse: 1.8560 - val_mse: 3.4462\n",
      "ERROR\t2021-06-02 02:11:38 +0000\tmaster-replica-0\t5\t2021-06-02 02:11:38.965799: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 02:11:54 +0000\tmaster-replica-0\t5\t2021-06-02 02:11:54.520378: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 02:11:55 +0000\tmaster-replica-0\t5\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:11:55 +0000\tmaster-replica-0\t5\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:11:55 +0000\tmaster-replica-0\t5\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 02:12:01 +0000\tmaster-replica-0\t5\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/5/20210602021153\n",
      "ERROR\t2021-06-02 02:12:33 +0000\tmaster-replica-0\t3\t2021-06-02 02:12:33.052770: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:12:36 +0000\tmaster-replica-0\t3\tEpoch 5/10\n",
      "INFO\t2021-06-02 02:12:36 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 02:12:36 +0000\tmaster-replica-0\t3\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/3/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:12:36 +0000\tmaster-replica-0\t3\t95238/95238 - 232s - loss: 1.3769 - rmse: 1.1454 - mse: 1.3769 - val_loss: 4.4068 - val_rmse: 2.0986 - val_mse: 4.4068\n",
      "ERROR\t2021-06-02 02:13:27 +0000\tmaster-replica-0\t1\t2021-06-02 02:13:27.643173: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:13:31 +0000\tmaster-replica-0\t1\tEpoch 8/10\n",
      "INFO\t2021-06-02 02:13:31 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 02:13:31 +0000\tmaster-replica-0\t1\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/1/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:13:31 +0000\tmaster-replica-0\t1\t66666/66666 - 158s - loss: 0.9506 - rmse: 0.9641 - mse: 0.9506 - val_loss: 5.9002 - val_rmse: 2.4284 - val_mse: 5.9002\n",
      "ERROR\t2021-06-02 02:14:12 +0000\tmaster-replica-0\t2\t2021-06-02 02:14:12.238688: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:14:16 +0000\tmaster-replica-0\t2\tEpoch 6/10\n",
      "INFO\t2021-06-02 02:14:16 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 02:14:16 +0000\tmaster-replica-0\t2\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/2/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:14:16 +0000\tmaster-replica-0\t2\t95238/95238 - 212s - loss: 0.9986 - rmse: 0.9828 - mse: 0.9986 - val_loss: 4.1383 - val_rmse: 2.0333 - val_mse: 4.1383\n",
      "INFO\t2021-06-02 02:14:37 +0000\tservice\t5\tJob completed successfully.\n",
      "ERROR\t2021-06-02 02:16:03 +0000\tmaster-replica-0\t1\t2021-06-02 02:16:03.020234: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:16:06 +0000\tmaster-replica-0\t1\tEpoch 9/10\n",
      "INFO\t2021-06-02 02:16:06 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 02:16:06 +0000\tmaster-replica-0\t1\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/1/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:16:06 +0000\tmaster-replica-0\t1\t66666/66666 - 156s - loss: 1.2258 - rmse: 1.0830 - mse: 1.2258 - val_loss: 2.1174 - val_rmse: 1.4543 - val_mse: 2.1174\n",
      "ERROR\t2021-06-02 02:16:21 +0000\tmaster-replica-0\t3\t2021-06-02 02:16:21.756300: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:16:25 +0000\tmaster-replica-0\t3\tEpoch 6/10\n",
      "INFO\t2021-06-02 02:16:25 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 02:16:25 +0000\tmaster-replica-0\t3\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/3/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:16:25 +0000\tmaster-replica-0\t3\t95238/95238 - 229s - loss: 0.9949 - rmse: 0.9810 - mse: 0.9949 - val_loss: 6.2910 - val_rmse: 2.5074 - val_mse: 6.2910\n",
      "INFO\t2021-06-02 02:16:28 +0000\tservice\t6\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 02:16:29 +0000\tservice\t6\tWaiting for training program to start.\n",
      "ERROR\t2021-06-02 02:17:18 +0000\tmaster-replica-0\t4\t2021-06-02 02:17:18.099695: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:17:21 +0000\tmaster-replica-0\t4\tEpoch 4/10\n",
      "INFO\t2021-06-02 02:17:21 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 02:17:21 +0000\tmaster-replica-0\t4\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/4/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:17:21 +0000\tmaster-replica-0\t4\t200000/200000 - 366s - loss: 0.9683 - rmse: 0.9540 - mse: 0.9683 - val_loss: 10.2336 - val_rmse: 3.1985 - val_mse: 10.2336\n",
      "ERROR\t2021-06-02 02:17:46 +0000\tmaster-replica-0\t2\t2021-06-02 02:17:46.121650: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:17:49 +0000\tmaster-replica-0\t2\tEpoch 7/10\n",
      "INFO\t2021-06-02 02:17:49 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 02:17:49 +0000\tmaster-replica-0\t2\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/2/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:17:49 +0000\tmaster-replica-0\t2\t95238/95238 - 214s - loss: 0.9398 - rmse: 0.9541 - mse: 0.9398 - val_loss: 5.0761 - val_rmse: 2.2521 - val_mse: 5.0761\n",
      "ERROR\t2021-06-02 02:18:37 +0000\tmaster-replica-0\t1\t2021-06-02 02:18:37.143004: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:18:41 +0000\tmaster-replica-0\t1\tEpoch 10/10\n",
      "INFO\t2021-06-02 02:18:41 +0000\tmaster-replica-0\t1\t\n",
      "INFO\t2021-06-02 02:18:41 +0000\tmaster-replica-0\t1\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/1/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:18:41 +0000\tmaster-replica-0\t1\t66666/66666 - 154s - loss: 1.1588 - rmse: 1.0599 - mse: 1.1588 - val_loss: 3.8876 - val_rmse: 1.9713 - val_mse: 3.8876\n",
      "ERROR\t2021-06-02 02:18:41 +0000\tmaster-replica-0\t1\t2021-06-02 02:18:41.020373: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 02:18:56 +0000\tmaster-replica-0\t1\t2021-06-02 02:18:56.564388: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 02:18:57 +0000\tmaster-replica-0\t1\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:18:57 +0000\tmaster-replica-0\t1\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:18:57 +0000\tmaster-replica-0\t1\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 02:19:03 +0000\tmaster-replica-0\t1\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/1/20210602021855\n",
      "ERROR\t2021-06-02 02:20:14 +0000\tmaster-replica-0\t3\t2021-06-02 02:20:14.489811: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:20:18 +0000\tmaster-replica-0\t3\tEpoch 7/10\n",
      "INFO\t2021-06-02 02:20:18 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 02:20:18 +0000\tmaster-replica-0\t3\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/3/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:20:18 +0000\tmaster-replica-0\t3\t95238/95238 - 233s - loss: 0.9389 - rmse: 0.9535 - mse: 0.9389 - val_loss: 6.6711 - val_rmse: 2.5820 - val_mse: 6.6711\n",
      "ERROR\t2021-06-02 02:21:15 +0000\tmaster-replica-0\t2\t2021-06-02 02:21:15.122397: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:21:18 +0000\tmaster-replica-0\t2\tEpoch 8/10\n",
      "INFO\t2021-06-02 02:21:18 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 02:21:18 +0000\tmaster-replica-0\t2\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/2/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:21:18 +0000\tmaster-replica-0\t2\t95238/95238 - 209s - loss: 0.9531 - rmse: 0.9611 - mse: 0.9531 - val_loss: 6.2463 - val_rmse: 2.4980 - val_mse: 6.2463\n",
      "INFO\t2021-06-02 02:22:10 +0000\tservice\t1\tJob completed successfully.\n",
      "ERROR\t2021-06-02 02:22:38 +0000\tmaster-replica-0\t6\t2021-06-02 02:22:38.519754: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tModel: \"model\"\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t==================================================================================================\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tdeep_inputs (DenseFeatures)     (None, 66)           64000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tdeep_h1 (Dense)                 (None, 32)           2144        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t==================================================================================================\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tTotal params: 67,001\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tTrainable params: 67,001\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tNone\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tTrain for 40000 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tEpoch 1/10\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/6/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:22:43 +0000\tmaster-replica-0\t6\t40000/40000 - 103s - loss: 1.3807 - rmse: 1.1560 - mse: 1.3807 - val_loss: 3.9849 - val_rmse: 1.9954 - val_mse: 3.9849\n",
      "ERROR\t2021-06-02 02:23:20 +0000\tmaster-replica-0\t4\t2021-06-02 02:23:20.633103: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:23:25 +0000\tmaster-replica-0\t4\tEpoch 5/10\n",
      "INFO\t2021-06-02 02:23:25 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 02:23:25 +0000\tmaster-replica-0\t4\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/4/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:23:25 +0000\tmaster-replica-0\t4\t200000/200000 - 364s - loss: 1.3764 - rmse: 1.1271 - mse: 1.3764 - val_loss: 3.3220 - val_rmse: 1.8222 - val_mse: 3.3220\n",
      "ERROR\t2021-06-02 02:24:14 +0000\tmaster-replica-0\t3\t2021-06-02 02:24:14.222807: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:24:17 +0000\tmaster-replica-0\t3\tEpoch 8/10\n",
      "INFO\t2021-06-02 02:24:17 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 02:24:17 +0000\tmaster-replica-0\t3\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/3/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:24:17 +0000\tmaster-replica-0\t3\t95238/95238 - 240s - loss: 0.9509 - rmse: 0.9600 - mse: 0.9509 - val_loss: 7.1805 - val_rmse: 2.6788 - val_mse: 7.1805\n",
      "ERROR\t2021-06-02 02:24:19 +0000\tmaster-replica-0\t6\t2021-06-02 02:24:19.704497: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:24:23 +0000\tmaster-replica-0\t6\tEpoch 2/10\n",
      "INFO\t2021-06-02 02:24:23 +0000\tmaster-replica-0\t6\t\n",
      "INFO\t2021-06-02 02:24:23 +0000\tmaster-replica-0\t6\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/6/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:24:23 +0000\tmaster-replica-0\t6\t40000/40000 - 100s - loss: 0.9589 - rmse: 0.9721 - mse: 0.9589 - val_loss: 5.6368 - val_rmse: 2.3738 - val_mse: 5.6368\n",
      "ERROR\t2021-06-02 02:24:41 +0000\tmaster-replica-0\t2\t2021-06-02 02:24:41.222552: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:24:44 +0000\tmaster-replica-0\t2\tEpoch 9/10\n",
      "INFO\t2021-06-02 02:24:44 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 02:24:44 +0000\tmaster-replica-0\t2\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/2/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:24:44 +0000\tmaster-replica-0\t2\t95238/95238 - 206s - loss: 1.2282 - rmse: 1.0790 - mse: 1.2282 - val_loss: 2.3159 - val_rmse: 1.5205 - val_mse: 2.3159\n",
      "INFO\t2021-06-02 02:24:47 +0000\tservice\t7\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 02:24:49 +0000\tservice\t7\tWaiting for training program to start.\n",
      "ERROR\t2021-06-02 02:25:59 +0000\tmaster-replica-0\t6\t2021-06-02 02:25:59.358188: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:26:03 +0000\tmaster-replica-0\t6\tEpoch 3/10\n",
      "INFO\t2021-06-02 02:26:03 +0000\tmaster-replica-0\t6\t\n",
      "INFO\t2021-06-02 02:26:03 +0000\tmaster-replica-0\t6\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/6/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:26:03 +0000\tmaster-replica-0\t6\t40000/40000 - 99s - loss: 0.9478 - rmse: 0.9668 - mse: 0.9478 - val_loss: 6.4941 - val_rmse: 2.5476 - val_mse: 6.4941\n",
      "ERROR\t2021-06-02 02:27:37 +0000\tmaster-replica-0\t6\t2021-06-02 02:27:37.377157: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:27:41 +0000\tmaster-replica-0\t6\tEpoch 4/10\n",
      "INFO\t2021-06-02 02:27:41 +0000\tmaster-replica-0\t6\t\n",
      "INFO\t2021-06-02 02:27:41 +0000\tmaster-replica-0\t6\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/6/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:27:41 +0000\tmaster-replica-0\t6\t40000/40000 - 98s - loss: 0.9706 - rmse: 0.9786 - mse: 0.9706 - val_loss: 8.0894 - val_rmse: 2.8432 - val_mse: 8.0894\n",
      "ERROR\t2021-06-02 02:28:06 +0000\tmaster-replica-0\t2\t2021-06-02 02:28:06.410376: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:28:10 +0000\tmaster-replica-0\t2\tEpoch 10/10\n",
      "INFO\t2021-06-02 02:28:10 +0000\tmaster-replica-0\t2\t\n",
      "INFO\t2021-06-02 02:28:10 +0000\tmaster-replica-0\t2\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/2/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:28:10 +0000\tmaster-replica-0\t2\t95238/95238 - 205s - loss: 1.1597 - rmse: 1.0557 - mse: 1.1597 - val_loss: 2.8290 - val_rmse: 1.6817 - val_mse: 2.8290\n",
      "ERROR\t2021-06-02 02:28:10 +0000\tmaster-replica-0\t2\t2021-06-02 02:28:10.012341: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 02:28:12 +0000\tmaster-replica-0\t3\t2021-06-02 02:28:12.070855: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:28:15 +0000\tmaster-replica-0\t3\tEpoch 9/10\n",
      "INFO\t2021-06-02 02:28:15 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 02:28:15 +0000\tmaster-replica-0\t3\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/3/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:28:15 +0000\tmaster-replica-0\t3\t95238/95238 - 238s - loss: 1.2267 - rmse: 1.0787 - mse: 1.2267 - val_loss: 2.1207 - val_rmse: 1.4554 - val_mse: 2.1207\n",
      "ERROR\t2021-06-02 02:28:24 +0000\tmaster-replica-0\t2\t2021-06-02 02:28:24.029906: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 02:28:24 +0000\tmaster-replica-0\t2\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:28:24 +0000\tmaster-replica-0\t2\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:28:24 +0000\tmaster-replica-0\t2\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 02:28:30 +0000\tmaster-replica-0\t2\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/2/20210602022822\n",
      "ERROR\t2021-06-02 02:29:15 +0000\tmaster-replica-0\t6\t2021-06-02 02:29:15.154836: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:29:18 +0000\tmaster-replica-0\t6\tEpoch 5/10\n",
      "INFO\t2021-06-02 02:29:18 +0000\tmaster-replica-0\t6\t\n",
      "INFO\t2021-06-02 02:29:18 +0000\tmaster-replica-0\t6\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/6/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:29:18 +0000\tmaster-replica-0\t6\t40000/40000 - 98s - loss: 1.3776 - rmse: 1.1555 - mse: 1.3776 - val_loss: 2.4230 - val_rmse: 1.5560 - val_mse: 2.4230\n",
      "ERROR\t2021-06-02 02:29:25 +0000\tmaster-replica-0\t4\t2021-06-02 02:29:25.565763: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:29:29 +0000\tmaster-replica-0\t4\tEpoch 6/10\n",
      "INFO\t2021-06-02 02:29:29 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 02:29:29 +0000\tmaster-replica-0\t4\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/4/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:29:29 +0000\tmaster-replica-0\t4\t200000/200000 - 364s - loss: 0.9970 - rmse: 0.9662 - mse: 0.9970 - val_loss: 4.8291 - val_rmse: 2.1966 - val_mse: 4.8291\n",
      "ERROR\t2021-06-02 02:30:51 +0000\tmaster-replica-0\t6\t2021-06-02 02:30:51.041934: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:30:54 +0000\tmaster-replica-0\t6\tEpoch 6/10\n",
      "INFO\t2021-06-02 02:30:54 +0000\tmaster-replica-0\t6\t\n",
      "INFO\t2021-06-02 02:30:54 +0000\tmaster-replica-0\t6\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/6/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:30:54 +0000\tmaster-replica-0\t6\t40000/40000 - 96s - loss: 1.0009 - rmse: 0.9929 - mse: 1.0009 - val_loss: 3.4206 - val_rmse: 1.8485 - val_mse: 3.4206\n",
      "INFO\t2021-06-02 02:31:13 +0000\tservice\t2\tJob completed successfully.\n",
      "ERROR\t2021-06-02 02:31:16 +0000\tmaster-replica-0\t7\t2021-06-02 02:31:16.820902: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tModel: \"model\"\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t==================================================================================================\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tdeep_inputs (DenseFeatures)     (None, 52)           50000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tdeep_h1 (Dense)                 (None, 32)           1696        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t==================================================================================================\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tTotal params: 52,553\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tTrainable params: 52,553\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tNone\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tTrain for 50000 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tEpoch 1/10\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/7/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:31:21 +0000\tmaster-replica-0\t7\t50000/50000 - 120s - loss: 1.3816 - rmse: 1.1549 - mse: 1.3815 - val_loss: 4.3937 - val_rmse: 2.0952 - val_mse: 4.3937\n",
      "ERROR\t2021-06-02 02:32:15 +0000\tmaster-replica-0\t3\t2021-06-02 02:32:15.013263: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:32:18 +0000\tmaster-replica-0\t3\tEpoch 10/10\n",
      "INFO\t2021-06-02 02:32:18 +0000\tmaster-replica-0\t3\t\n",
      "INFO\t2021-06-02 02:32:18 +0000\tmaster-replica-0\t3\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/3/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:32:18 +0000\tmaster-replica-0\t3\t95238/95238 - 243s - loss: 1.1614 - rmse: 1.0563 - mse: 1.1614 - val_loss: 4.4900 - val_rmse: 2.1184 - val_mse: 4.4900\n",
      "ERROR\t2021-06-02 02:32:18 +0000\tmaster-replica-0\t3\t2021-06-02 02:32:18.645847: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 02:32:27 +0000\tmaster-replica-0\t6\t2021-06-02 02:32:27.410933: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:32:31 +0000\tmaster-replica-0\t6\tEpoch 7/10\n",
      "INFO\t2021-06-02 02:32:31 +0000\tmaster-replica-0\t6\t\n",
      "INFO\t2021-06-02 02:32:31 +0000\tmaster-replica-0\t6\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/6/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:32:31 +0000\tmaster-replica-0\t6\t40000/40000 - 96s - loss: 0.9409 - rmse: 0.9632 - mse: 0.9409 - val_loss: 4.1358 - val_rmse: 2.0332 - val_mse: 4.1358\n",
      "ERROR\t2021-06-02 02:32:35 +0000\tmaster-replica-0\t3\t2021-06-02 02:32:35.328157: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 02:32:35 +0000\tmaster-replica-0\t3\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:32:35 +0000\tmaster-replica-0\t3\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:32:35 +0000\tmaster-replica-0\t3\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 02:32:42 +0000\tmaster-replica-0\t3\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/3/20210602023234\n",
      "ERROR\t2021-06-02 02:33:12 +0000\tmaster-replica-0\t7\t2021-06-02 02:33:12.840522: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:33:16 +0000\tmaster-replica-0\t7\tEpoch 2/10\n",
      "INFO\t2021-06-02 02:33:16 +0000\tmaster-replica-0\t7\t\n",
      "INFO\t2021-06-02 02:33:16 +0000\tmaster-replica-0\t7\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/7/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:33:16 +0000\tmaster-replica-0\t7\t50000/50000 - 115s - loss: 0.9585 - rmse: 0.9703 - mse: 0.9585 - val_loss: 6.6572 - val_rmse: 2.5797 - val_mse: 6.6572\n",
      "INFO\t2021-06-02 02:33:42 +0000\tservice\t8\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 02:33:43 +0000\tservice\t8\tWaiting for training program to start.\n",
      "ERROR\t2021-06-02 02:34:02 +0000\tmaster-replica-0\t6\t2021-06-02 02:34:02.355634: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:34:06 +0000\tmaster-replica-0\t6\tEpoch 8/10\n",
      "INFO\t2021-06-02 02:34:06 +0000\tmaster-replica-0\t6\t\n",
      "INFO\t2021-06-02 02:34:06 +0000\tmaster-replica-0\t6\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/6/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:34:06 +0000\tmaster-replica-0\t6\t40000/40000 - 95s - loss: 0.9541 - rmse: 0.9702 - mse: 0.9541 - val_loss: 5.0908 - val_rmse: 2.2556 - val_mse: 5.0908\n",
      "ERROR\t2021-06-02 02:35:07 +0000\tmaster-replica-0\t7\t2021-06-02 02:35:07.010774: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:35:10 +0000\tmaster-replica-0\t7\tEpoch 3/10\n",
      "INFO\t2021-06-02 02:35:10 +0000\tmaster-replica-0\t7\t\n",
      "INFO\t2021-06-02 02:35:10 +0000\tmaster-replica-0\t7\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/7/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:35:10 +0000\tmaster-replica-0\t7\t50000/50000 - 114s - loss: 0.9468 - rmse: 0.9648 - mse: 0.9468 - val_loss: 7.9410 - val_rmse: 2.8175 - val_mse: 7.9410\n",
      "ERROR\t2021-06-02 02:35:28 +0000\tmaster-replica-0\t4\t2021-06-02 02:35:28.995731: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:35:33 +0000\tmaster-replica-0\t4\tEpoch 7/10\n",
      "INFO\t2021-06-02 02:35:33 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 02:35:33 +0000\tmaster-replica-0\t4\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/4/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:35:33 +0000\tmaster-replica-0\t4\t200000/200000 - 363s - loss: 0.9420 - rmse: 0.9401 - mse: 0.9419 - val_loss: 6.3263 - val_rmse: 2.5145 - val_mse: 6.3263\n",
      "ERROR\t2021-06-02 02:35:39 +0000\tmaster-replica-0\t6\t2021-06-02 02:35:39.489709: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:35:43 +0000\tmaster-replica-0\t6\tEpoch 9/10\n",
      "INFO\t2021-06-02 02:35:43 +0000\tmaster-replica-0\t6\t\n",
      "INFO\t2021-06-02 02:35:43 +0000\tmaster-replica-0\t6\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/6/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:35:43 +0000\tmaster-replica-0\t6\t40000/40000 - 97s - loss: 1.2250 - rmse: 1.0877 - mse: 1.2250 - val_loss: 2.1164 - val_rmse: 1.4537 - val_mse: 2.1164\n",
      "INFO\t2021-06-02 02:36:00 +0000\tservice\t3\tJob completed successfully.\n",
      "ERROR\t2021-06-02 02:37:00 +0000\tmaster-replica-0\t7\t2021-06-02 02:37:00.174639: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:37:03 +0000\tmaster-replica-0\t7\tEpoch 4/10\n",
      "INFO\t2021-06-02 02:37:03 +0000\tmaster-replica-0\t7\t\n",
      "INFO\t2021-06-02 02:37:03 +0000\tmaster-replica-0\t7\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/7/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:37:03 +0000\tmaster-replica-0\t7\t50000/50000 - 113s - loss: 0.9685 - rmse: 0.9758 - mse: 0.9685 - val_loss: 8.3359 - val_rmse: 2.8865 - val_mse: 8.3358\n",
      "ERROR\t2021-06-02 02:37:14 +0000\tmaster-replica-0\t6\t2021-06-02 02:37:14.665753: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:37:18 +0000\tmaster-replica-0\t6\tEpoch 10/10\n",
      "INFO\t2021-06-02 02:37:18 +0000\tmaster-replica-0\t6\t\n",
      "INFO\t2021-06-02 02:37:18 +0000\tmaster-replica-0\t6\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/6/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:37:18 +0000\tmaster-replica-0\t6\t40000/40000 - 95s - loss: 1.1605 - rmse: 1.0651 - mse: 1.1605 - val_loss: 3.0038 - val_rmse: 1.7329 - val_mse: 3.0038\n",
      "ERROR\t2021-06-02 02:37:18 +0000\tmaster-replica-0\t6\t2021-06-02 02:37:18.117818: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 02:37:30 +0000\tmaster-replica-0\t6\t2021-06-02 02:37:30.746528: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 02:37:31 +0000\tmaster-replica-0\t6\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:37:31 +0000\tmaster-replica-0\t6\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:37:31 +0000\tmaster-replica-0\t6\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 02:37:37 +0000\tmaster-replica-0\t6\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/6/20210602023729\n",
      "INFO\t2021-06-02 02:38:18 +0000\tservice\t9\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 02:38:20 +0000\tservice\t9\tWaiting for training program to start.\n",
      "ERROR\t2021-06-02 02:38:52 +0000\tmaster-replica-0\t7\t2021-06-02 02:38:52.911731: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:38:56 +0000\tmaster-replica-0\t7\tEpoch 5/10\n",
      "INFO\t2021-06-02 02:38:56 +0000\tmaster-replica-0\t7\t\n",
      "INFO\t2021-06-02 02:38:56 +0000\tmaster-replica-0\t7\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/7/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:38:56 +0000\tmaster-replica-0\t7\t50000/50000 - 113s - loss: 1.3772 - rmse: 1.1537 - mse: 1.3772 - val_loss: 2.3265 - val_rmse: 1.5247 - val_mse: 2.3265\n",
      "INFO\t2021-06-02 02:40:19 +0000\tservice\t6\tJob completed successfully.\n",
      "ERROR\t2021-06-02 02:40:46 +0000\tmaster-replica-0\t7\t2021-06-02 02:40:46.028406: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:40:49 +0000\tmaster-replica-0\t7\tEpoch 6/10\n",
      "INFO\t2021-06-02 02:40:49 +0000\tmaster-replica-0\t7\t\n",
      "INFO\t2021-06-02 02:40:49 +0000\tmaster-replica-0\t7\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/7/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:40:49 +0000\tmaster-replica-0\t7\t50000/50000 - 113s - loss: 1.0008 - rmse: 0.9913 - mse: 1.0008 - val_loss: 3.5341 - val_rmse: 1.8793 - val_mse: 3.5341\n",
      "ERROR\t2021-06-02 02:41:35 +0000\tmaster-replica-0\t4\t2021-06-02 02:41:35.072926: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:41:38 +0000\tmaster-replica-0\t4\tEpoch 8/10\n",
      "INFO\t2021-06-02 02:41:38 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 02:41:38 +0000\tmaster-replica-0\t4\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/4/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:41:38 +0000\tmaster-replica-0\t4\t200000/200000 - 366s - loss: 0.9522 - rmse: 0.9458 - mse: 0.9522 - val_loss: 7.3873 - val_rmse: 2.7171 - val_mse: 7.3873\n",
      "INFO\t2021-06-02 02:41:42 +0000\tservice\t10\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 02:41:44 +0000\tservice\t10\tWaiting for training program to start.\n",
      "ERROR\t2021-06-02 02:42:39 +0000\tmaster-replica-0\t7\t2021-06-02 02:42:39.608741: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:42:43 +0000\tmaster-replica-0\t7\tEpoch 7/10\n",
      "INFO\t2021-06-02 02:42:43 +0000\tmaster-replica-0\t7\t\n",
      "INFO\t2021-06-02 02:42:43 +0000\tmaster-replica-0\t7\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/7/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:42:43 +0000\tmaster-replica-0\t7\t50000/50000 - 113s - loss: 0.9406 - rmse: 0.9615 - mse: 0.9406 - val_loss: 4.5099 - val_rmse: 2.1231 - val_mse: 4.5099\n",
      "ERROR\t2021-06-02 02:42:49 +0000\tmaster-replica-0\t9\t2021-06-02 02:42:49.059801: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 02:42:49 +0000\tmaster-replica-0\t9\t2021-06-02 02:42:49.060133: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 02:42:49 +0000\tmaster-replica-0\t9\t2021-06-02 02:42:49.060180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 02:42:50 +0000\tmaster-replica-0\t9\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:42:50 +0000\tmaster-replica-0\t9\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:42:50 +0000\tmaster-replica-0\t9\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 02:42:50 +0000\tmaster-replica-0\t9\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:42:50 +0000\tmaster-replica-0\t9\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:42:50 +0000\tmaster-replica-0\t9\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 02:42:51 +0000\tmaster-replica-0\t9\t2021-06-02 02:42:51.007305: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 02:42:51 +0000\tmaster-replica-0\t9\t2021-06-02 02:42:51.007361: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 02:42:51 +0000\tmaster-replica-0\t9\t2021-06-02 02:42:51.007415: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-6834556607167226113): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 02:42:51 +0000\tmaster-replica-0\t9\t2021-06-02 02:42:51.007722: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 02:42:51 +0000\tmaster-replica-0\t9\t2021-06-02 02:42:51.020194: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 02:42:51 +0000\tmaster-replica-0\t9\t2021-06-02 02:42:51.020719: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a863288170 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 02:42:51 +0000\tmaster-replica-0\t9\t2021-06-02 02:42:51.020768: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 02:42:51 +0000\tmaster-replica-0\t9\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:42:51 +0000\tmaster-replica-0\t9\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:42:51 +0000\tmaster-replica-0\t9\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 02:44:32 +0000\tmaster-replica-0\t7\t2021-06-02 02:44:32.546256: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:44:36 +0000\tmaster-replica-0\t7\tEpoch 8/10\n",
      "INFO\t2021-06-02 02:44:36 +0000\tmaster-replica-0\t7\t\n",
      "INFO\t2021-06-02 02:44:36 +0000\tmaster-replica-0\t7\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/7/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:44:36 +0000\tmaster-replica-0\t7\t50000/50000 - 113s - loss: 0.9535 - rmse: 0.9683 - mse: 0.9535 - val_loss: 5.4994 - val_rmse: 2.3441 - val_mse: 5.4994\n",
      "ERROR\t2021-06-02 02:46:05 +0000\tmaster-replica-0\t9\t2021-06-02 02:46:05.212158: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t10\t2021-06-02 02:46:09.419608: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t10\t2021-06-02 02:46:09.419834: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t10\t2021-06-02 02:46:09.419866: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tModel: \"model\"\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t==================================================================================================\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tdeep_inputs (DenseFeatures)     (None, 49)           47000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tdeep_h1 (Dense)                 (None, 32)           1600        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t==================================================================================================\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tTotal params: 49,457\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tTrainable params: 49,457\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tNone\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tTrain for 83333 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tEpoch 1/10\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/9/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:46:09 +0000\tmaster-replica-0\t9\t83333/83333 - 198s - loss: 1.3874 - rmse: 1.1494 - mse: 1.3874 - val_loss: 4.2980 - val_rmse: 2.0723 - val_mse: 4.2980\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\t2021-06-02 02:46:11.302537: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\t2021-06-02 02:46:11.302587: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\t2021-06-02 02:46:11.302622: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-8144157705937462487): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\t2021-06-02 02:46:11.303020: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\t2021-06-02 02:46:11.316345: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\t2021-06-02 02:46:11.316790: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557d0a8b4790 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\t2021-06-02 02:46:11.316838: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:46:11 +0000\tmaster-replica-0\t10\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 02:46:24 +0000\tmaster-replica-0\t7\t2021-06-02 02:46:24.752714: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:46:28 +0000\tmaster-replica-0\t7\tEpoch 9/10\n",
      "INFO\t2021-06-02 02:46:28 +0000\tmaster-replica-0\t7\t\n",
      "INFO\t2021-06-02 02:46:28 +0000\tmaster-replica-0\t7\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/7/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:46:28 +0000\tmaster-replica-0\t7\t50000/50000 - 112s - loss: 1.2256 - rmse: 1.0861 - mse: 1.2256 - val_loss: 2.1211 - val_rmse: 1.4556 - val_mse: 2.1211\n",
      "ERROR\t2021-06-02 02:46:39 +0000\tmaster-replica-0\t8\t2021-06-02 02:46:39.674686: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tModel: \"model\"\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t==================================================================================================\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tdeep_inputs (DenseFeatures)     (None, 52)           50000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tdeep_h1 (Dense)                 (None, 32)           1696        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t==================================================================================================\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tTotal params: 52,553\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tTrainable params: 52,553\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tNone\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tTrain for 200000 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tEpoch 1/10\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/8/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:46:44 +0000\tmaster-replica-0\t8\t200000/200000 - 505s - loss: 1.3892 - rmse: 1.1301 - mse: 1.3892 - val_loss: 6.2011 - val_rmse: 2.4898 - val_mse: 6.2011\n",
      "ERROR\t2021-06-02 02:47:38 +0000\tmaster-replica-0\t4\t2021-06-02 02:47:38.621756: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:47:42 +0000\tmaster-replica-0\t4\tEpoch 9/10\n",
      "INFO\t2021-06-02 02:47:42 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 02:47:42 +0000\tmaster-replica-0\t4\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/4/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:47:42 +0000\tmaster-replica-0\t4\t200000/200000 - 363s - loss: 1.2237 - rmse: 1.0599 - mse: 1.2237 - val_loss: 2.1922 - val_rmse: 1.4792 - val_mse: 2.1922\n",
      "ERROR\t2021-06-02 02:48:16 +0000\tmaster-replica-0\t7\t2021-06-02 02:48:16.395373: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 02:48:17 +0000\tmaster-replica-0\t10\t2021-06-02 02:48:17.716933: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:48:19 +0000\tmaster-replica-0\t7\tEpoch 10/10\n",
      "INFO\t2021-06-02 02:48:19 +0000\tmaster-replica-0\t7\t\n",
      "INFO\t2021-06-02 02:48:19 +0000\tmaster-replica-0\t7\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/7/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:48:19 +0000\tmaster-replica-0\t7\t50000/50000 - 112s - loss: 1.1609 - rmse: 1.0637 - mse: 1.1609 - val_loss: 2.8656 - val_rmse: 1.6925 - val_mse: 2.8656\n",
      "ERROR\t2021-06-02 02:48:19 +0000\tmaster-replica-0\t7\t2021-06-02 02:48:19.932724: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tModel: \"model\"\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t==================================================================================================\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tdeep_inputs (DenseFeatures)     (None, 6)            4000        gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tdeep_h1 (Dense)                 (None, 32)           224         deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t==================================================================================================\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tTotal params: 5,081\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tTrainable params: 5,081\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tNone\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tTrain for 57142 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tEpoch 1/10\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/10/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:48:22 +0000\tmaster-replica-0\t10\t57142/57142 - 131s - loss: 1.3885 - rmse: 1.1547 - mse: 1.3885 - val_loss: 4.4186 - val_rmse: 2.1011 - val_mse: 4.4186\n",
      "ERROR\t2021-06-02 02:48:34 +0000\tmaster-replica-0\t7\t2021-06-02 02:48:34.793288: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 02:48:35 +0000\tmaster-replica-0\t7\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:48:35 +0000\tmaster-replica-0\t7\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:48:35 +0000\tmaster-replica-0\t7\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 02:48:41 +0000\tmaster-replica-0\t7\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/7/20210602024833\n",
      "ERROR\t2021-06-02 02:50:17 +0000\tmaster-replica-0\t10\t2021-06-02 02:50:17.456833: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 02:50:19 +0000\tmaster-replica-0\t9\t2021-06-02 02:50:19.397907: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:50:21 +0000\tmaster-replica-0\t10\tEpoch 2/10\n",
      "INFO\t2021-06-02 02:50:21 +0000\tmaster-replica-0\t10\t\n",
      "INFO\t2021-06-02 02:50:21 +0000\tmaster-replica-0\t10\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/10/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:50:21 +0000\tmaster-replica-0\t10\t57142/57142 - 119s - loss: 0.9563 - rmse: 0.9680 - mse: 0.9563 - val_loss: 5.5989 - val_rmse: 2.3657 - val_mse: 5.5989\n",
      "INFO\t2021-06-02 02:50:22 +0000\tmaster-replica-0\t9\tEpoch 2/10\n",
      "INFO\t2021-06-02 02:50:22 +0000\tmaster-replica-0\t9\t\n",
      "INFO\t2021-06-02 02:50:22 +0000\tmaster-replica-0\t9\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/9/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:50:22 +0000\tmaster-replica-0\t9\t83333/83333 - 253s - loss: 0.9579 - rmse: 0.9648 - mse: 0.9579 - val_loss: 5.3457 - val_rmse: 2.3116 - val_mse: 5.3457\n",
      "INFO\t2021-06-02 02:51:39 +0000\tservice\t7\tJob completed successfully.\n",
      "ERROR\t2021-06-02 02:52:14 +0000\tmaster-replica-0\t10\t2021-06-02 02:52:14.437388: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:52:18 +0000\tmaster-replica-0\t10\tEpoch 3/10\n",
      "INFO\t2021-06-02 02:52:18 +0000\tmaster-replica-0\t10\t\n",
      "INFO\t2021-06-02 02:52:18 +0000\tmaster-replica-0\t10\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/10/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:52:18 +0000\tmaster-replica-0\t10\t57142/57142 - 117s - loss: 0.9463 - rmse: 0.9634 - mse: 0.9463 - val_loss: 5.8632 - val_rmse: 2.4207 - val_mse: 5.8632\n",
      "ERROR\t2021-06-02 02:53:42 +0000\tmaster-replica-0\t4\t2021-06-02 02:53:42.874479: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:53:46 +0000\tmaster-replica-0\t4\tEpoch 10/10\n",
      "INFO\t2021-06-02 02:53:46 +0000\tmaster-replica-0\t4\t\n",
      "INFO\t2021-06-02 02:53:46 +0000\tmaster-replica-0\t4\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/4/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:53:46 +0000\tmaster-replica-0\t4\t200000/200000 - 365s - loss: 1.1626 - rmse: 1.0401 - mse: 1.1626 - val_loss: 4.2896 - val_rmse: 2.0706 - val_mse: 4.2896\n",
      "ERROR\t2021-06-02 02:53:46 +0000\tmaster-replica-0\t4\t2021-06-02 02:53:46.900443: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 02:54:02 +0000\tmaster-replica-0\t4\t2021-06-02 02:54:02.127108: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 02:54:02 +0000\tmaster-replica-0\t4\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:54:02 +0000\tmaster-replica-0\t4\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:54:02 +0000\tmaster-replica-0\t4\tIf using Keras pass *_constraint arguments to layers.\n",
      "ERROR\t2021-06-02 02:54:05 +0000\tmaster-replica-0\t9\t2021-06-02 02:54:05.985886: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:54:09 +0000\tmaster-replica-0\t4\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/4/20210602025401\n",
      "INFO\t2021-06-02 02:54:12 +0000\tmaster-replica-0\t9\tEpoch 3/10\n",
      "INFO\t2021-06-02 02:54:12 +0000\tmaster-replica-0\t9\t\n",
      "INFO\t2021-06-02 02:54:12 +0000\tmaster-replica-0\t9\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/9/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:54:12 +0000\tmaster-replica-0\t9\t83333/83333 - 230s - loss: 0.9456 - rmse: 0.9591 - mse: 0.9456 - val_loss: 6.4081 - val_rmse: 2.5306 - val_mse: 6.4081\n",
      "ERROR\t2021-06-02 02:54:16 +0000\tmaster-replica-0\t10\t2021-06-02 02:54:16.416624: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:54:22 +0000\tmaster-replica-0\t10\tEpoch 4/10\n",
      "INFO\t2021-06-02 02:54:22 +0000\tmaster-replica-0\t10\t\n",
      "INFO\t2021-06-02 02:54:22 +0000\tmaster-replica-0\t10\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/10/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:54:22 +0000\tmaster-replica-0\t10\t57142/57142 - 124s - loss: 0.9696 - rmse: 0.9754 - mse: 0.9696 - val_loss: 6.8030 - val_rmse: 2.6073 - val_mse: 6.8030\n",
      "INFO\t2021-06-02 02:54:25 +0000\tservice\t11\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 02:54:26 +0000\tservice\t11\tWaiting for training program to start.\n",
      "ERROR\t2021-06-02 02:55:00 +0000\tmaster-replica-0\t8\t2021-06-02 02:55:00.858347: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:55:04 +0000\tmaster-replica-0\t8\tEpoch 2/10\n",
      "INFO\t2021-06-02 02:55:04 +0000\tmaster-replica-0\t8\t\n",
      "INFO\t2021-06-02 02:55:04 +0000\tmaster-replica-0\t8\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/8/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:55:04 +0000\tmaster-replica-0\t8\t200000/200000 - 500s - loss: 0.9600 - rmse: 0.9486 - mse: 0.9600 - val_loss: 7.5282 - val_rmse: 2.7433 - val_mse: 7.5282\n",
      "ERROR\t2021-06-02 02:56:25 +0000\tmaster-replica-0\t10\t2021-06-02 02:56:25.432001: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:150] Filling up shuffle buffer (this may take a while): 1 of 10000\n",
      "ERROR\t2021-06-02 02:56:25 +0000\tmaster-replica-0\t10\t2021-06-02 02:56:25.541009: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:199] Shuffle buffer filled.\n",
      "ERROR\t2021-06-02 02:56:26 +0000\tmaster-replica-0\t10\t2021-06-02 02:56:26.955396: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:56:30 +0000\tmaster-replica-0\t10\tEpoch 5/10\n",
      "INFO\t2021-06-02 02:56:30 +0000\tmaster-replica-0\t10\t\n",
      "INFO\t2021-06-02 02:56:30 +0000\tmaster-replica-0\t10\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/10/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:56:30 +0000\tmaster-replica-0\t10\t57142/57142 - 128s - loss: 1.3777 - rmse: 1.1529 - mse: 1.3777 - val_loss: 2.6363 - val_rmse: 1.6233 - val_mse: 2.6363\n",
      "ERROR\t2021-06-02 02:57:21 +0000\tmaster-replica-0\t9\t2021-06-02 02:57:21.871558: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:57:23 +0000\tservice\t4\tJob completed successfully.\n",
      "INFO\t2021-06-02 02:57:25 +0000\tmaster-replica-0\t9\tEpoch 4/10\n",
      "INFO\t2021-06-02 02:57:25 +0000\tmaster-replica-0\t9\t\n",
      "INFO\t2021-06-02 02:57:25 +0000\tmaster-replica-0\t9\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/9/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:57:25 +0000\tmaster-replica-0\t9\t83333/83333 - 193s - loss: 0.9689 - rmse: 0.9709 - mse: 0.9689 - val_loss: 8.2927 - val_rmse: 2.8787 - val_mse: 8.2927\n",
      "ERROR\t2021-06-02 02:58:26 +0000\tmaster-replica-0\t10\t2021-06-02 02:58:26.924359: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:58:30 +0000\tmaster-replica-0\t10\tEpoch 6/10\n",
      "INFO\t2021-06-02 02:58:30 +0000\tmaster-replica-0\t10\t\n",
      "INFO\t2021-06-02 02:58:30 +0000\tmaster-replica-0\t10\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/10/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:58:30 +0000\tmaster-replica-0\t10\t57142/57142 - 120s - loss: 0.9998 - rmse: 0.9895 - mse: 0.9998 - val_loss: 3.7441 - val_rmse: 1.9343 - val_mse: 3.7441\n",
      "ERROR\t2021-06-02 02:58:54 +0000\tmaster-replica-0\t11\t2021-06-02 02:58:54.902760: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 02:58:54 +0000\tmaster-replica-0\t11\t2021-06-02 02:58:54.903030: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 02:58:54 +0000\tmaster-replica-0\t11\t2021-06-02 02:58:54.903094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\t2021-06-02 02:58:56.524883: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\t2021-06-02 02:58:56.524943: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\t2021-06-02 02:58:56.525144: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-17492642326249282570): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\t2021-06-02 02:58:56.525466: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\t2021-06-02 02:58:56.535992: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\t2021-06-02 02:58:56.536623: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556b16bebfb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\t2021-06-02 02:58:56.536689: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:58:56 +0000\tmaster-replica-0\t11\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "INFO\t2021-06-02 02:59:37 +0000\tservice\t12\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 02:59:39 +0000\tservice\t12\tWaiting for training program to start.\n",
      "ERROR\t2021-06-02 03:00:28 +0000\tmaster-replica-0\t10\t2021-06-02 03:00:28.790317: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:00:32 +0000\tmaster-replica-0\t10\tEpoch 7/10\n",
      "INFO\t2021-06-02 03:00:32 +0000\tmaster-replica-0\t10\t\n",
      "INFO\t2021-06-02 03:00:32 +0000\tmaster-replica-0\t10\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/10/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:00:32 +0000\tmaster-replica-0\t10\t57142/57142 - 122s - loss: 0.9421 - rmse: 0.9610 - mse: 0.9421 - val_loss: 4.2448 - val_rmse: 2.0594 - val_mse: 4.2448\n",
      "ERROR\t2021-06-02 03:00:34 +0000\tmaster-replica-0\t9\t2021-06-02 03:00:34.103504: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:00:37 +0000\tmaster-replica-0\t9\tEpoch 5/10\n",
      "INFO\t2021-06-02 03:00:37 +0000\tmaster-replica-0\t9\t\n",
      "INFO\t2021-06-02 03:00:37 +0000\tmaster-replica-0\t9\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/9/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:00:37 +0000\tmaster-replica-0\t9\t83333/83333 - 192s - loss: 1.3763 - rmse: 1.1472 - mse: 1.3763 - val_loss: 2.7099 - val_rmse: 1.6458 - val_mse: 2.7099\n",
      "ERROR\t2021-06-02 03:00:38 +0000\tmaster-replica-0\t11\t2021-06-02 03:00:38.426866: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tModel: \"model\"\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t==================================================================================================\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tdeep_inputs (DenseFeatures)     (None, 54)           52000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tdeep_h1 (Dense)                 (None, 32)           1760        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t==================================================================================================\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tTotal params: 54,617\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tTrainable params: 54,617\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tNone\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tTrain for 40000 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tEpoch 1/10\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/11/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:00:43 +0000\tmaster-replica-0\t11\t40000/40000 - 106s - loss: 1.3788 - rmse: 1.1554 - mse: 1.3788 - val_loss: 3.8773 - val_rmse: 1.9684 - val_mse: 3.8773\n",
      "ERROR\t2021-06-02 03:02:20 +0000\tmaster-replica-0\t11\t2021-06-02 03:02:20.463051: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:02:23 +0000\tmaster-replica-0\t11\tEpoch 2/10\n",
      "INFO\t2021-06-02 03:02:23 +0000\tmaster-replica-0\t11\t\n",
      "INFO\t2021-06-02 03:02:23 +0000\tmaster-replica-0\t11\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/11/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:02:23 +0000\tmaster-replica-0\t11\t40000/40000 - 101s - loss: 0.9595 - rmse: 0.9725 - mse: 0.9595 - val_loss: 4.9522 - val_rmse: 2.2248 - val_mse: 4.9522\n",
      "ERROR\t2021-06-02 03:02:28 +0000\tmaster-replica-0\t10\t2021-06-02 03:02:28.383970: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:02:31 +0000\tmaster-replica-0\t10\tEpoch 8/10\n",
      "INFO\t2021-06-02 03:02:31 +0000\tmaster-replica-0\t10\t\n",
      "INFO\t2021-06-02 03:02:31 +0000\tmaster-replica-0\t10\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/10/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:02:31 +0000\tmaster-replica-0\t10\t57142/57142 - 119s - loss: 0.9518 - rmse: 0.9665 - mse: 0.9518 - val_loss: 4.3640 - val_rmse: 2.0883 - val_mse: 4.3640\n",
      "ERROR\t2021-06-02 03:03:20 +0000\tmaster-replica-0\t8\t2021-06-02 03:03:20.802392: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:03:24 +0000\tmaster-replica-0\t8\tEpoch 3/10\n",
      "INFO\t2021-06-02 03:03:24 +0000\tmaster-replica-0\t8\t\n",
      "INFO\t2021-06-02 03:03:24 +0000\tmaster-replica-0\t8\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/8/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:03:24 +0000\tmaster-replica-0\t8\t200000/200000 - 500s - loss: 0.9447 - rmse: 0.9418 - mse: 0.9447 - val_loss: 9.2417 - val_rmse: 3.0384 - val_mse: 9.2417\n",
      "ERROR\t2021-06-02 03:03:48 +0000\tmaster-replica-0\t9\t2021-06-02 03:03:48.891464: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:03:52 +0000\tmaster-replica-0\t9\tEpoch 6/10\n",
      "INFO\t2021-06-02 03:03:52 +0000\tmaster-replica-0\t9\t\n",
      "INFO\t2021-06-02 03:03:52 +0000\tmaster-replica-0\t9\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/9/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:03:52 +0000\tmaster-replica-0\t9\t83333/83333 - 195s - loss: 0.9989 - rmse: 0.9848 - mse: 0.9989 - val_loss: 3.5483 - val_rmse: 1.8831 - val_mse: 3.5483\n",
      "ERROR\t2021-06-02 03:04:02 +0000\tmaster-replica-0\t11\t2021-06-02 03:04:02.652499: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:04:05 +0000\tmaster-replica-0\t11\tEpoch 3/10\n",
      "INFO\t2021-06-02 03:04:05 +0000\tmaster-replica-0\t11\t\n",
      "INFO\t2021-06-02 03:04:05 +0000\tmaster-replica-0\t11\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/11/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:04:05 +0000\tmaster-replica-0\t11\t40000/40000 - 102s - loss: 0.9479 - rmse: 0.9668 - mse: 0.9479 - val_loss: 5.8239 - val_rmse: 2.4125 - val_mse: 5.8239\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\t2021-06-02 03:04:06.292980: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\t2021-06-02 03:04:06.293029: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\t2021-06-02 03:04:06.293121: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-12688126069112504135): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\t2021-06-02 03:04:06.293402: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\t2021-06-02 03:04:06.303745: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\t2021-06-02 03:04:06.304196: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5567a7f3c8a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\t2021-06-02 03:04:06.304243: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:04:06 +0000\tmaster-replica-0\t12\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:04:26 +0000\tmaster-replica-0\t10\t2021-06-02 03:04:26.468515: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:04:29 +0000\tmaster-replica-0\t10\tEpoch 9/10\n",
      "INFO\t2021-06-02 03:04:29 +0000\tmaster-replica-0\t10\t\n",
      "INFO\t2021-06-02 03:04:29 +0000\tmaster-replica-0\t10\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/10/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:04:29 +0000\tmaster-replica-0\t10\t57142/57142 - 118s - loss: 1.2240 - rmse: 1.0842 - mse: 1.2240 - val_loss: 2.1093 - val_rmse: 1.4516 - val_mse: 2.1093\n",
      "ERROR\t2021-06-02 03:05:44 +0000\tmaster-replica-0\t11\t2021-06-02 03:05:44.262240: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:05:47 +0000\tmaster-replica-0\t11\tEpoch 4/10\n",
      "INFO\t2021-06-02 03:05:47 +0000\tmaster-replica-0\t11\t\n",
      "INFO\t2021-06-02 03:05:47 +0000\tmaster-replica-0\t11\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/11/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:05:47 +0000\tmaster-replica-0\t11\t40000/40000 - 102s - loss: 0.9695 - rmse: 0.9780 - mse: 0.9695 - val_loss: 7.2914 - val_rmse: 2.6989 - val_mse: 7.2914\n",
      "ERROR\t2021-06-02 03:06:23 +0000\tmaster-replica-0\t10\t2021-06-02 03:06:23.483608: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:06:27 +0000\tmaster-replica-0\t10\tEpoch 10/10\n",
      "INFO\t2021-06-02 03:06:27 +0000\tmaster-replica-0\t10\t\n",
      "INFO\t2021-06-02 03:06:27 +0000\tmaster-replica-0\t10\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/10/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:06:27 +0000\tmaster-replica-0\t10\t57142/57142 - 117s - loss: 1.1611 - rmse: 1.0625 - mse: 1.1611 - val_loss: 3.4870 - val_rmse: 1.8671 - val_mse: 3.4870\n",
      "ERROR\t2021-06-02 03:06:27 +0000\tmaster-replica-0\t10\t2021-06-02 03:06:27.042617: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 03:06:46 +0000\tmaster-replica-0\t10\t2021-06-02 03:06:46.478024: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 03:06:46 +0000\tmaster-replica-0\t10\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:06:46 +0000\tmaster-replica-0\t10\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:06:46 +0000\tmaster-replica-0\t10\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 03:06:53 +0000\tmaster-replica-0\t10\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/10/20210602030645\n",
      "ERROR\t2021-06-02 03:07:02 +0000\tmaster-replica-0\t9\t2021-06-02 03:07:02.026486: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:07:05 +0000\tmaster-replica-0\t9\tEpoch 7/10\n",
      "INFO\t2021-06-02 03:07:05 +0000\tmaster-replica-0\t9\t\n",
      "INFO\t2021-06-02 03:07:05 +0000\tmaster-replica-0\t9\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/9/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:07:05 +0000\tmaster-replica-0\t9\t83333/83333 - 193s - loss: 0.9400 - rmse: 0.9561 - mse: 0.9400 - val_loss: 4.1281 - val_rmse: 2.0313 - val_mse: 4.1281\n",
      "ERROR\t2021-06-02 03:07:25 +0000\tmaster-replica-0\t11\t2021-06-02 03:07:25.644153: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:07:29 +0000\tmaster-replica-0\t11\tEpoch 5/10\n",
      "INFO\t2021-06-02 03:07:29 +0000\tmaster-replica-0\t11\t\n",
      "INFO\t2021-06-02 03:07:29 +0000\tmaster-replica-0\t11\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/11/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:07:29 +0000\tmaster-replica-0\t11\t40000/40000 - 101s - loss: 1.3760 - rmse: 1.1550 - mse: 1.3760 - val_loss: 2.5639 - val_rmse: 1.6008 - val_mse: 2.5639\n",
      "ERROR\t2021-06-02 03:09:06 +0000\tmaster-replica-0\t11\t2021-06-02 03:09:06.822571: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:09:10 +0000\tmaster-replica-0\t11\tEpoch 6/10\n",
      "INFO\t2021-06-02 03:09:10 +0000\tmaster-replica-0\t11\t\n",
      "INFO\t2021-06-02 03:09:10 +0000\tmaster-replica-0\t11\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/11/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:09:10 +0000\tmaster-replica-0\t11\t40000/40000 - 101s - loss: 0.9982 - rmse: 0.9917 - mse: 0.9982 - val_loss: 3.8670 - val_rmse: 1.9659 - val_mse: 3.8670\n",
      "INFO\t2021-06-02 03:09:54 +0000\tservice\t10\tJob completed successfully.\n",
      "ERROR\t2021-06-02 03:10:13 +0000\tmaster-replica-0\t9\t2021-06-02 03:10:13.890831: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:10:17 +0000\tmaster-replica-0\t9\tEpoch 8/10\n",
      "INFO\t2021-06-02 03:10:17 +0000\tmaster-replica-0\t9\t\n",
      "INFO\t2021-06-02 03:10:17 +0000\tmaster-replica-0\t9\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/9/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:10:17 +0000\tmaster-replica-0\t9\t83333/83333 - 192s - loss: 0.9535 - rmse: 0.9631 - mse: 0.9535 - val_loss: 4.9270 - val_rmse: 2.2191 - val_mse: 4.9270\n",
      "ERROR\t2021-06-02 03:10:47 +0000\tmaster-replica-0\t11\t2021-06-02 03:10:47.904426: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:10:51 +0000\tmaster-replica-0\t11\tEpoch 7/10\n",
      "INFO\t2021-06-02 03:10:51 +0000\tmaster-replica-0\t11\t\n",
      "INFO\t2021-06-02 03:10:51 +0000\tmaster-replica-0\t11\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/11/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:10:51 +0000\tmaster-replica-0\t11\t40000/40000 - 101s - loss: 0.9407 - rmse: 0.9630 - mse: 0.9407 - val_loss: 4.9014 - val_rmse: 2.2132 - val_mse: 4.9014\n",
      "ERROR\t2021-06-02 03:10:58 +0000\tmaster-replica-0\t12\t2021-06-02 03:10:58.056804: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tModel: \"model\"\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t==================================================================================================\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tdeep_inputs (DenseFeatures)     (None, 66)           64000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tdeep_h1 (Dense)                 (None, 32)           2144        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t==================================================================================================\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tTotal params: 67,001\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tTrainable params: 67,001\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tNone\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tTrain for 200000 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tEpoch 1/10\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/12/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:11:02 +0000\tmaster-replica-0\t12\t200000/200000 - 416s - loss: 1.3889 - rmse: 1.1297 - mse: 1.3889 - val_loss: 5.0468 - val_rmse: 2.2460 - val_mse: 5.0468\n",
      "INFO\t2021-06-02 03:11:27 +0000\tservice\t13\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 03:11:29 +0000\tservice\t13\tWaiting for training program to start.\n",
      "ERROR\t2021-06-02 03:11:36 +0000\tmaster-replica-0\t8\t2021-06-02 03:11:36.895286: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:11:40 +0000\tmaster-replica-0\t8\tEpoch 4/10\n",
      "INFO\t2021-06-02 03:11:40 +0000\tmaster-replica-0\t8\t\n",
      "INFO\t2021-06-02 03:11:40 +0000\tmaster-replica-0\t8\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/8/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:11:40 +0000\tmaster-replica-0\t8\t200000/200000 - 496s - loss: 0.9685 - rmse: 0.9539 - mse: 0.9685 - val_loss: 10.6228 - val_rmse: 3.2583 - val_mse: 10.6228\n",
      "ERROR\t2021-06-02 03:12:28 +0000\tmaster-replica-0\t11\t2021-06-02 03:12:28.564052: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:12:32 +0000\tmaster-replica-0\t11\tEpoch 8/10\n",
      "INFO\t2021-06-02 03:12:32 +0000\tmaster-replica-0\t11\t\n",
      "INFO\t2021-06-02 03:12:32 +0000\tmaster-replica-0\t11\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/11/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:12:32 +0000\tmaster-replica-0\t11\t40000/40000 - 101s - loss: 0.9520 - rmse: 0.9691 - mse: 0.9520 - val_loss: 5.2463 - val_rmse: 2.2899 - val_mse: 5.2463\n",
      "ERROR\t2021-06-02 03:13:27 +0000\tmaster-replica-0\t9\t2021-06-02 03:13:27.723055: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:13:31 +0000\tmaster-replica-0\t9\tEpoch 9/10\n",
      "INFO\t2021-06-02 03:13:31 +0000\tmaster-replica-0\t9\t\n",
      "INFO\t2021-06-02 03:13:31 +0000\tmaster-replica-0\t9\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/9/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:13:31 +0000\tmaster-replica-0\t9\t83333/83333 - 194s - loss: 1.2260 - rmse: 1.0805 - mse: 1.2260 - val_loss: 2.1301 - val_rmse: 1.4588 - val_mse: 2.1301\n",
      "ERROR\t2021-06-02 03:14:09 +0000\tmaster-replica-0\t11\t2021-06-02 03:14:09.978818: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:14:17 +0000\tmaster-replica-0\t11\tEpoch 9/10\n",
      "INFO\t2021-06-02 03:14:17 +0000\tmaster-replica-0\t11\t\n",
      "INFO\t2021-06-02 03:14:17 +0000\tmaster-replica-0\t11\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/11/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:14:17 +0000\tmaster-replica-0\t11\t40000/40000 - 106s - loss: 1.2264 - rmse: 1.0881 - mse: 1.2264 - val_loss: 2.1123 - val_rmse: 1.4524 - val_mse: 2.1123\n",
      "ERROR\t2021-06-02 03:15:54 +0000\tmaster-replica-0\t11\t2021-06-02 03:15:54.486283: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:15:58 +0000\tmaster-replica-0\t11\tEpoch 10/10\n",
      "INFO\t2021-06-02 03:15:58 +0000\tmaster-replica-0\t11\t\n",
      "INFO\t2021-06-02 03:15:58 +0000\tmaster-replica-0\t11\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/11/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:15:58 +0000\tmaster-replica-0\t11\t40000/40000 - 100s - loss: 1.1647 - rmse: 1.0671 - mse: 1.1647 - val_loss: 2.7465 - val_rmse: 1.6569 - val_mse: 2.7465\n",
      "ERROR\t2021-06-02 03:15:58 +0000\tmaster-replica-0\t11\t2021-06-02 03:15:58.092370: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 03:16:12 +0000\tmaster-replica-0\t11\t2021-06-02 03:16:12.549196: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 03:16:13 +0000\tmaster-replica-0\t11\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:16:13 +0000\tmaster-replica-0\t11\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:16:13 +0000\tmaster-replica-0\t11\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 03:16:19 +0000\tmaster-replica-0\t11\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/11/20210602031611\n",
      "ERROR\t2021-06-02 03:16:45 +0000\tmaster-replica-0\t9\t2021-06-02 03:16:45.114041: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:16:48 +0000\tmaster-replica-0\t9\tEpoch 10/10\n",
      "INFO\t2021-06-02 03:16:48 +0000\tmaster-replica-0\t9\t\n",
      "INFO\t2021-06-02 03:16:48 +0000\tmaster-replica-0\t9\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/9/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:16:48 +0000\tmaster-replica-0\t9\t83333/83333 - 198s - loss: 1.1588 - rmse: 1.0570 - mse: 1.1588 - val_loss: 3.5690 - val_rmse: 1.8887 - val_mse: 3.5690\n",
      "ERROR\t2021-06-02 03:16:48 +0000\tmaster-replica-0\t9\t2021-06-02 03:16:48.815760: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 03:17:04 +0000\tmaster-replica-0\t9\t2021-06-02 03:17:04.621937: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 03:17:05 +0000\tmaster-replica-0\t9\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:17:05 +0000\tmaster-replica-0\t9\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:17:05 +0000\tmaster-replica-0\t9\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 03:17:10 +0000\tmaster-replica-0\t9\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/9/20210602031703\n",
      "ERROR\t2021-06-02 03:17:39 +0000\tmaster-replica-0\t13\t2021-06-02 03:17:39.905332: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tModel: \"model\"\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t==================================================================================================\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tdeep_inputs (DenseFeatures)     (None, 50)           48000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tdeep_h1 (Dense)                 (None, 32)           1632        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t==================================================================================================\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tTotal params: 50,489\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tTrainable params: 50,489\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tNone\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tTrain for 40000 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tEpoch 1/10\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/13/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:17:44 +0000\tmaster-replica-0\t13\t40000/40000 - 101s - loss: 1.3898 - rmse: 1.1580 - mse: 1.3898 - val_loss: 4.4053 - val_rmse: 2.0981 - val_mse: 4.4053\n",
      "ERROR\t2021-06-02 03:17:49 +0000\tmaster-replica-0\t12\t2021-06-02 03:17:49.900634: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:17:53 +0000\tmaster-replica-0\t12\tEpoch 2/10\n",
      "INFO\t2021-06-02 03:17:53 +0000\tmaster-replica-0\t12\t\n",
      "INFO\t2021-06-02 03:17:53 +0000\tmaster-replica-0\t12\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/12/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:17:53 +0000\tmaster-replica-0\t12\t200000/200000 - 410s - loss: 0.9565 - rmse: 0.9470 - mse: 0.9565 - val_loss: 6.4838 - val_rmse: 2.5456 - val_mse: 6.4838\n",
      "INFO\t2021-06-02 03:19:15 +0000\tservice\t11\tJob completed successfully.\n",
      "ERROR\t2021-06-02 03:19:17 +0000\tmaster-replica-0\t13\t2021-06-02 03:19:17.700743: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:19:21 +0000\tmaster-replica-0\t13\tEpoch 2/10\n",
      "INFO\t2021-06-02 03:19:21 +0000\tmaster-replica-0\t13\t\n",
      "INFO\t2021-06-02 03:19:21 +0000\tmaster-replica-0\t13\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/13/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:19:21 +0000\tmaster-replica-0\t13\t40000/40000 - 97s - loss: 0.9613 - rmse: 0.9733 - mse: 0.9613 - val_loss: 6.5068 - val_rmse: 2.5504 - val_mse: 6.5068\n",
      "ERROR\t2021-06-02 03:19:53 +0000\tmaster-replica-0\t8\t2021-06-02 03:19:53.009739: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:19:56 +0000\tmaster-replica-0\t8\tEpoch 5/10\n",
      "INFO\t2021-06-02 03:19:56 +0000\tmaster-replica-0\t8\t\n",
      "INFO\t2021-06-02 03:19:56 +0000\tmaster-replica-0\t8\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/8/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:19:56 +0000\tmaster-replica-0\t8\t200000/200000 - 496s - loss: 1.3776 - rmse: 1.1274 - mse: 1.3776 - val_loss: 4.0930 - val_rmse: 2.0228 - val_mse: 4.0930\n",
      "INFO\t2021-06-02 03:20:41 +0000\tservice\t9\tJob completed successfully.\n",
      "ERROR\t2021-06-02 03:20:54 +0000\tmaster-replica-0\t13\t2021-06-02 03:20:54.177328: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:20:57 +0000\tmaster-replica-0\t13\tEpoch 3/10\n",
      "INFO\t2021-06-02 03:20:57 +0000\tmaster-replica-0\t13\t\n",
      "INFO\t2021-06-02 03:20:57 +0000\tmaster-replica-0\t13\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/13/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:20:57 +0000\tmaster-replica-0\t13\t40000/40000 - 97s - loss: 0.9456 - rmse: 0.9657 - mse: 0.9456 - val_loss: 6.7964 - val_rmse: 2.6063 - val_mse: 6.7964\n",
      "INFO\t2021-06-02 03:21:20 +0000\tservice\t14\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 03:21:21 +0000\tservice\t15\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 03:21:21 +0000\tservice\t14\tWaiting for training program to start.\n",
      "INFO\t2021-06-02 03:21:23 +0000\tservice\t15\tWaiting for training program to start.\n",
      "ERROR\t2021-06-02 03:22:31 +0000\tmaster-replica-0\t13\t2021-06-02 03:22:31.086079: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:22:34 +0000\tmaster-replica-0\t13\tEpoch 4/10\n",
      "INFO\t2021-06-02 03:22:34 +0000\tmaster-replica-0\t13\t\n",
      "INFO\t2021-06-02 03:22:34 +0000\tmaster-replica-0\t13\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/13/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:22:34 +0000\tmaster-replica-0\t13\t40000/40000 - 97s - loss: 0.9691 - rmse: 0.9778 - mse: 0.9691 - val_loss: 7.3780 - val_rmse: 2.7151 - val_mse: 7.3780\n",
      "ERROR\t2021-06-02 03:24:07 +0000\tmaster-replica-0\t13\t2021-06-02 03:24:07.404525: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:24:10 +0000\tmaster-replica-0\t13\tEpoch 5/10\n",
      "INFO\t2021-06-02 03:24:10 +0000\tmaster-replica-0\t13\t\n",
      "INFO\t2021-06-02 03:24:10 +0000\tmaster-replica-0\t13\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/13/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:24:10 +0000\tmaster-replica-0\t13\t40000/40000 - 96s - loss: 1.3761 - rmse: 1.1552 - mse: 1.3761 - val_loss: 2.4069 - val_rmse: 1.5509 - val_mse: 2.4069\n",
      "ERROR\t2021-06-02 03:24:39 +0000\tmaster-replica-0\t12\t2021-06-02 03:24:39.277735: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:24:42 +0000\tmaster-replica-0\t12\tEpoch 3/10\n",
      "INFO\t2021-06-02 03:24:42 +0000\tmaster-replica-0\t12\t\n",
      "INFO\t2021-06-02 03:24:42 +0000\tmaster-replica-0\t12\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/12/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:24:42 +0000\tmaster-replica-0\t12\t200000/200000 - 410s - loss: 0.9455 - rmse: 0.9422 - mse: 0.9455 - val_loss: 7.7631 - val_rmse: 2.7845 - val_mse: 7.7631\n",
      "ERROR\t2021-06-02 03:25:43 +0000\tmaster-replica-0\t13\t2021-06-02 03:25:43.158575: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:25:46 +0000\tmaster-replica-0\t13\tEpoch 6/10\n",
      "INFO\t2021-06-02 03:25:46 +0000\tmaster-replica-0\t13\t\n",
      "INFO\t2021-06-02 03:25:46 +0000\tmaster-replica-0\t13\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/13/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:25:46 +0000\tmaster-replica-0\t13\t40000/40000 - 96s - loss: 0.9980 - rmse: 0.9914 - mse: 0.9980 - val_loss: 3.7470 - val_rmse: 1.9352 - val_mse: 3.7470\n",
      "ERROR\t2021-06-02 03:25:51 +0000\tmaster-replica-0\t14\t2021-06-02 03:25:51.328164: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 03:25:51 +0000\tmaster-replica-0\t14\t2021-06-02 03:25:51.328407: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 03:25:51 +0000\tmaster-replica-0\t14\t2021-06-02 03:25:51.328441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\t2021-06-02 03:25:52.901266: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\t2021-06-02 03:25:52.901322: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\t2021-06-02 03:25:52.901356: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-1321564659254352986): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\t2021-06-02 03:25:52.901618: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\t2021-06-02 03:25:52.911091: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\t2021-06-02 03:25:52.911600: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5563406b0a20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\t2021-06-02 03:25:52.911653: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:25:52 +0000\tmaster-replica-0\t14\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:25:59 +0000\tmaster-replica-0\t15\t2021-06-02 03:25:59.957212: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 03:25:59 +0000\tmaster-replica-0\t15\t2021-06-02 03:25:59.957460: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 03:25:59 +0000\tmaster-replica-0\t15\t2021-06-02 03:25:59.957490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\t2021-06-02 03:26:01.969459: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\t2021-06-02 03:26:01.969562: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\t2021-06-02 03:26:01.969614: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-5879987257530277403): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\t2021-06-02 03:26:01.970074: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\t2021-06-02 03:26:01.982881: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\t2021-06-02 03:26:01.983761: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555b193a5230 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\t2021-06-02 03:26:01.983837: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:26:01 +0000\tmaster-replica-0\t15\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:27:20 +0000\tmaster-replica-0\t13\t2021-06-02 03:27:20.009773: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:27:23 +0000\tmaster-replica-0\t13\tEpoch 7/10\n",
      "INFO\t2021-06-02 03:27:23 +0000\tmaster-replica-0\t13\t\n",
      "INFO\t2021-06-02 03:27:23 +0000\tmaster-replica-0\t13\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/13/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:27:23 +0000\tmaster-replica-0\t13\t40000/40000 - 97s - loss: 0.9401 - rmse: 0.9627 - mse: 0.9401 - val_loss: 4.8738 - val_rmse: 2.2073 - val_mse: 4.8738\n",
      "ERROR\t2021-06-02 03:27:31 +0000\tmaster-replica-0\t14\t2021-06-02 03:27:31.894023: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tModel: \"model\"\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t==================================================================================================\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tdeep_inputs (DenseFeatures)     (None, 59)           57000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tdeep_h1 (Dense)                 (None, 32)           1920        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t==================================================================================================\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tTotal params: 59,777\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tTrainable params: 59,777\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tNone\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tTrain for 40000 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tEpoch 1/10\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/14/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:27:36 +0000\tmaster-replica-0\t14\t40000/40000 - 103s - loss: 1.3886 - rmse: 1.1582 - mse: 1.3886 - val_loss: 4.3624 - val_rmse: 2.0878 - val_mse: 4.3624\n",
      "ERROR\t2021-06-02 03:27:50 +0000\tmaster-replica-0\t15\t2021-06-02 03:27:50.047506: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tModel: \"model\"\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t==================================================================================================\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tdeep_inputs (DenseFeatures)     (None, 55)           53000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tdeep_h1 (Dense)                 (None, 32)           1792        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t==================================================================================================\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tTotal params: 55,649\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tTrainable params: 55,649\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tNone\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tTrain for 40000 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tEpoch 1/10\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/15/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:27:54 +0000\tmaster-replica-0\t15\t40000/40000 - 112s - loss: 1.3908 - rmse: 1.1577 - mse: 1.3908 - val_loss: 3.7211 - val_rmse: 1.9284 - val_mse: 3.7211\n",
      "ERROR\t2021-06-02 03:28:01 +0000\tmaster-replica-0\t8\t2021-06-02 03:28:01.461642: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:28:04 +0000\tmaster-replica-0\t8\tEpoch 6/10\n",
      "INFO\t2021-06-02 03:28:04 +0000\tmaster-replica-0\t8\t\n",
      "INFO\t2021-06-02 03:28:04 +0000\tmaster-replica-0\t8\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/8/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:28:04 +0000\tmaster-replica-0\t8\t200000/200000 - 488s - loss: 0.9959 - rmse: 0.9658 - mse: 0.9959 - val_loss: 4.9396 - val_rmse: 2.2218 - val_mse: 4.9396\n",
      "ERROR\t2021-06-02 03:28:57 +0000\tmaster-replica-0\t13\t2021-06-02 03:28:57.147041: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:29:00 +0000\tmaster-replica-0\t13\tEpoch 8/10\n",
      "INFO\t2021-06-02 03:29:00 +0000\tmaster-replica-0\t13\t\n",
      "INFO\t2021-06-02 03:29:00 +0000\tmaster-replica-0\t13\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/13/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:29:00 +0000\tmaster-replica-0\t13\t40000/40000 - 97s - loss: 0.9529 - rmse: 0.9695 - mse: 0.9529 - val_loss: 5.2352 - val_rmse: 2.2873 - val_mse: 5.2352\n",
      "ERROR\t2021-06-02 03:29:09 +0000\tmaster-replica-0\t14\t2021-06-02 03:29:09.433989: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:29:12 +0000\tmaster-replica-0\t14\tEpoch 2/10\n",
      "INFO\t2021-06-02 03:29:12 +0000\tmaster-replica-0\t14\t\n",
      "INFO\t2021-06-02 03:29:12 +0000\tmaster-replica-0\t14\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/14/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:29:12 +0000\tmaster-replica-0\t14\t40000/40000 - 96s - loss: 0.9601 - rmse: 0.9728 - mse: 0.9601 - val_loss: 5.7377 - val_rmse: 2.3950 - val_mse: 5.7377\n",
      "ERROR\t2021-06-02 03:29:35 +0000\tmaster-replica-0\t15\t2021-06-02 03:29:35.009047: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:29:38 +0000\tmaster-replica-0\t15\tEpoch 2/10\n",
      "INFO\t2021-06-02 03:29:38 +0000\tmaster-replica-0\t15\t\n",
      "INFO\t2021-06-02 03:29:38 +0000\tmaster-replica-0\t15\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/15/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:29:38 +0000\tmaster-replica-0\t15\t40000/40000 - 104s - loss: 0.9591 - rmse: 0.9722 - mse: 0.9591 - val_loss: 4.8358 - val_rmse: 2.1988 - val_mse: 4.8358\n",
      "ERROR\t2021-06-02 03:30:34 +0000\tmaster-replica-0\t13\t2021-06-02 03:30:34.852233: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:30:38 +0000\tmaster-replica-0\t13\tEpoch 9/10\n",
      "INFO\t2021-06-02 03:30:38 +0000\tmaster-replica-0\t13\t\n",
      "INFO\t2021-06-02 03:30:38 +0000\tmaster-replica-0\t13\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/13/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:30:38 +0000\tmaster-replica-0\t13\t40000/40000 - 98s - loss: 1.2258 - rmse: 1.0881 - mse: 1.2258 - val_loss: 2.1107 - val_rmse: 1.4522 - val_mse: 2.1107\n",
      "ERROR\t2021-06-02 03:30:45 +0000\tmaster-replica-0\t14\t2021-06-02 03:30:45.144139: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:30:48 +0000\tmaster-replica-0\t14\tEpoch 3/10\n",
      "INFO\t2021-06-02 03:30:48 +0000\tmaster-replica-0\t14\t\n",
      "INFO\t2021-06-02 03:30:48 +0000\tmaster-replica-0\t14\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/14/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:30:48 +0000\tmaster-replica-0\t14\t40000/40000 - 96s - loss: 0.9463 - rmse: 0.9662 - mse: 0.9463 - val_loss: 6.1728 - val_rmse: 2.4839 - val_mse: 6.1728\n",
      "ERROR\t2021-06-02 03:31:20 +0000\tmaster-replica-0\t15\t2021-06-02 03:31:20.991132: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:31:24 +0000\tmaster-replica-0\t15\tEpoch 3/10\n",
      "INFO\t2021-06-02 03:31:24 +0000\tmaster-replica-0\t15\t\n",
      "INFO\t2021-06-02 03:31:24 +0000\tmaster-replica-0\t15\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/15/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:31:24 +0000\tmaster-replica-0\t15\t40000/40000 - 106s - loss: 0.9475 - rmse: 0.9666 - mse: 0.9475 - val_loss: 5.7494 - val_rmse: 2.3974 - val_mse: 5.7494\n",
      "ERROR\t2021-06-02 03:31:25 +0000\tmaster-replica-0\t12\t2021-06-02 03:31:25.507876: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:31:29 +0000\tmaster-replica-0\t12\tEpoch 4/10\n",
      "INFO\t2021-06-02 03:31:29 +0000\tmaster-replica-0\t12\t\n",
      "INFO\t2021-06-02 03:31:29 +0000\tmaster-replica-0\t12\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/12/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:31:29 +0000\tmaster-replica-0\t12\t200000/200000 - 406s - loss: 0.9685 - rmse: 0.9540 - mse: 0.9685 - val_loss: 10.1820 - val_rmse: 3.1902 - val_mse: 10.1820\n",
      "ERROR\t2021-06-02 03:32:10 +0000\tmaster-replica-0\t13\t2021-06-02 03:32:10.818685: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:32:14 +0000\tmaster-replica-0\t13\tEpoch 10/10\n",
      "INFO\t2021-06-02 03:32:14 +0000\tmaster-replica-0\t13\t\n",
      "INFO\t2021-06-02 03:32:14 +0000\tmaster-replica-0\t13\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/13/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:32:14 +0000\tmaster-replica-0\t13\t40000/40000 - 96s - loss: 1.1646 - rmse: 1.0672 - mse: 1.1646 - val_loss: 3.0903 - val_rmse: 1.7575 - val_mse: 3.0903\n",
      "ERROR\t2021-06-02 03:32:14 +0000\tmaster-replica-0\t13\t2021-06-02 03:32:14.443451: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 03:32:21 +0000\tmaster-replica-0\t14\t2021-06-02 03:32:21.974583: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:32:25 +0000\tmaster-replica-0\t14\tEpoch 4/10\n",
      "INFO\t2021-06-02 03:32:25 +0000\tmaster-replica-0\t14\t\n",
      "INFO\t2021-06-02 03:32:25 +0000\tmaster-replica-0\t14\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/14/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:32:25 +0000\tmaster-replica-0\t14\t40000/40000 - 97s - loss: 0.9694 - rmse: 0.9779 - mse: 0.9694 - val_loss: 6.9171 - val_rmse: 2.6292 - val_mse: 6.9171\n",
      "ERROR\t2021-06-02 03:32:28 +0000\tmaster-replica-0\t13\t2021-06-02 03:32:28.118697: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 03:32:28 +0000\tmaster-replica-0\t13\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:32:28 +0000\tmaster-replica-0\t13\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:32:28 +0000\tmaster-replica-0\t13\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 03:32:34 +0000\tmaster-replica-0\t13\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/13/20210602033227\n",
      "ERROR\t2021-06-02 03:33:08 +0000\tmaster-replica-0\t15\t2021-06-02 03:33:08.594423: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:33:11 +0000\tmaster-replica-0\t15\tEpoch 4/10\n",
      "INFO\t2021-06-02 03:33:11 +0000\tmaster-replica-0\t15\t\n",
      "INFO\t2021-06-02 03:33:11 +0000\tmaster-replica-0\t15\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/15/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:33:11 +0000\tmaster-replica-0\t15\t40000/40000 - 107s - loss: 0.9711 - rmse: 0.9788 - mse: 0.9711 - val_loss: 7.8457 - val_rmse: 2.8002 - val_mse: 7.8457\n",
      "ERROR\t2021-06-02 03:33:57 +0000\tmaster-replica-0\t14\t2021-06-02 03:33:57.062054: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:34:00 +0000\tmaster-replica-0\t14\tEpoch 5/10\n",
      "INFO\t2021-06-02 03:34:00 +0000\tmaster-replica-0\t14\t\n",
      "INFO\t2021-06-02 03:34:00 +0000\tmaster-replica-0\t14\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/14/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:34:00 +0000\tmaster-replica-0\t14\t40000/40000 - 95s - loss: 1.3775 - rmse: 1.1559 - mse: 1.3775 - val_loss: 2.2930 - val_rmse: 1.5138 - val_mse: 2.2930\n",
      "ERROR\t2021-06-02 03:34:54 +0000\tmaster-replica-0\t15\t2021-06-02 03:34:54.542431: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:34:58 +0000\tmaster-replica-0\t15\tEpoch 5/10\n",
      "INFO\t2021-06-02 03:34:58 +0000\tmaster-replica-0\t15\t\n",
      "INFO\t2021-06-02 03:34:58 +0000\tmaster-replica-0\t15\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/15/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:34:58 +0000\tmaster-replica-0\t15\t40000/40000 - 106s - loss: 1.3773 - rmse: 1.1557 - mse: 1.3774 - val_loss: 2.2759 - val_rmse: 1.5080 - val_mse: 2.2759\n",
      "INFO\t2021-06-02 03:35:27 +0000\tservice\t13\tJob completed successfully.\n",
      "ERROR\t2021-06-02 03:35:32 +0000\tmaster-replica-0\t14\t2021-06-02 03:35:32.403606: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:35:35 +0000\tmaster-replica-0\t14\tEpoch 6/10\n",
      "INFO\t2021-06-02 03:35:35 +0000\tmaster-replica-0\t14\t\n",
      "INFO\t2021-06-02 03:35:35 +0000\tmaster-replica-0\t14\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/14/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:35:35 +0000\tmaster-replica-0\t14\t40000/40000 - 95s - loss: 0.9985 - rmse: 0.9918 - mse: 0.9985 - val_loss: 3.4566 - val_rmse: 1.8586 - val_mse: 3.4566\n",
      "ERROR\t2021-06-02 03:36:15 +0000\tmaster-replica-0\t8\t2021-06-02 03:36:15.397308: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:36:18 +0000\tmaster-replica-0\t8\tEpoch 7/10\n",
      "INFO\t2021-06-02 03:36:18 +0000\tmaster-replica-0\t8\t\n",
      "INFO\t2021-06-02 03:36:18 +0000\tmaster-replica-0\t8\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/8/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:36:18 +0000\tmaster-replica-0\t8\t200000/200000 - 494s - loss: 0.9409 - rmse: 0.9393 - mse: 0.9409 - val_loss: 5.6220 - val_rmse: 2.3704 - val_mse: 5.6220\n",
      "ERROR\t2021-06-02 03:36:39 +0000\tmaster-replica-0\t15\t2021-06-02 03:36:39.588424: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:36:43 +0000\tmaster-replica-0\t15\tEpoch 6/10\n",
      "INFO\t2021-06-02 03:36:43 +0000\tmaster-replica-0\t15\t\n",
      "INFO\t2021-06-02 03:36:43 +0000\tmaster-replica-0\t15\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/15/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:36:43 +0000\tmaster-replica-0\t15\t40000/40000 - 105s - loss: 0.9996 - rmse: 0.9922 - mse: 0.9996 - val_loss: 3.4993 - val_rmse: 1.8701 - val_mse: 3.4993\n",
      "INFO\t2021-06-02 03:37:06 +0000\tservice\t16\tWaiting for job to be provisioned.\n",
      "ERROR\t2021-06-02 03:37:07 +0000\tmaster-replica-0\t14\t2021-06-02 03:37:07.175417: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:37:08 +0000\tservice\t16\tWaiting for training program to start.\n",
      "INFO\t2021-06-02 03:37:10 +0000\tmaster-replica-0\t14\tEpoch 7/10\n",
      "INFO\t2021-06-02 03:37:10 +0000\tmaster-replica-0\t14\t\n",
      "INFO\t2021-06-02 03:37:10 +0000\tmaster-replica-0\t14\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/14/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:37:10 +0000\tmaster-replica-0\t14\t40000/40000 - 95s - loss: 0.9420 - rmse: 0.9636 - mse: 0.9420 - val_loss: 4.2266 - val_rmse: 2.0554 - val_mse: 4.2266\n",
      "ERROR\t2021-06-02 03:38:10 +0000\tmaster-replica-0\t12\t2021-06-02 03:38:10.782692: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:38:14 +0000\tmaster-replica-0\t12\tEpoch 5/10\n",
      "INFO\t2021-06-02 03:38:14 +0000\tmaster-replica-0\t12\t\n",
      "INFO\t2021-06-02 03:38:14 +0000\tmaster-replica-0\t12\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/12/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:38:14 +0000\tmaster-replica-0\t12\t200000/200000 - 406s - loss: 1.3773 - rmse: 1.1274 - mse: 1.3773 - val_loss: 4.5804 - val_rmse: 2.1396 - val_mse: 4.5804\n",
      "ERROR\t2021-06-02 03:38:22 +0000\tmaster-replica-0\t15\t2021-06-02 03:38:22.984702: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:38:26 +0000\tmaster-replica-0\t15\tEpoch 7/10\n",
      "INFO\t2021-06-02 03:38:26 +0000\tmaster-replica-0\t15\t\n",
      "INFO\t2021-06-02 03:38:26 +0000\tmaster-replica-0\t15\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/15/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:38:26 +0000\tmaster-replica-0\t15\t40000/40000 - 103s - loss: 0.9414 - rmse: 0.9633 - mse: 0.9414 - val_loss: 4.3112 - val_rmse: 2.0757 - val_mse: 4.3112\n",
      "ERROR\t2021-06-02 03:38:41 +0000\tmaster-replica-0\t14\t2021-06-02 03:38:41.085069: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:38:44 +0000\tmaster-replica-0\t14\tEpoch 8/10\n",
      "INFO\t2021-06-02 03:38:44 +0000\tmaster-replica-0\t14\t\n",
      "INFO\t2021-06-02 03:38:44 +0000\tmaster-replica-0\t14\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/14/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:38:44 +0000\tmaster-replica-0\t14\t40000/40000 - 94s - loss: 0.9534 - rmse: 0.9697 - mse: 0.9534 - val_loss: 4.9874 - val_rmse: 2.2326 - val_mse: 4.9874\n",
      "ERROR\t2021-06-02 03:40:02 +0000\tmaster-replica-0\t15\t2021-06-02 03:40:02.172421: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:40:05 +0000\tmaster-replica-0\t15\tEpoch 8/10\n",
      "INFO\t2021-06-02 03:40:05 +0000\tmaster-replica-0\t15\t\n",
      "INFO\t2021-06-02 03:40:05 +0000\tmaster-replica-0\t15\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/15/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:40:05 +0000\tmaster-replica-0\t15\t40000/40000 - 99s - loss: 0.9505 - rmse: 0.9682 - mse: 0.9505 - val_loss: 4.8266 - val_rmse: 2.1960 - val_mse: 4.8266\n",
      "ERROR\t2021-06-02 03:40:15 +0000\tmaster-replica-0\t14\t2021-06-02 03:40:15.071340: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:40:18 +0000\tmaster-replica-0\t14\tEpoch 9/10\n",
      "INFO\t2021-06-02 03:40:18 +0000\tmaster-replica-0\t14\t\n",
      "INFO\t2021-06-02 03:40:18 +0000\tmaster-replica-0\t14\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/14/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:40:18 +0000\tmaster-replica-0\t14\t40000/40000 - 94s - loss: 1.2284 - rmse: 1.0892 - mse: 1.2284 - val_loss: 2.1228 - val_rmse: 1.4561 - val_mse: 2.1228\n",
      "ERROR\t2021-06-02 03:41:43 +0000\tmaster-replica-0\t15\t2021-06-02 03:41:43.867679: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:41:47 +0000\tmaster-replica-0\t15\tEpoch 9/10\n",
      "INFO\t2021-06-02 03:41:47 +0000\tmaster-replica-0\t15\t\n",
      "INFO\t2021-06-02 03:41:47 +0000\tmaster-replica-0\t15\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/15/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:41:47 +0000\tmaster-replica-0\t15\t40000/40000 - 102s - loss: 1.2269 - rmse: 1.0883 - mse: 1.2269 - val_loss: 2.1228 - val_rmse: 1.4559 - val_mse: 2.1228\n",
      "ERROR\t2021-06-02 03:41:48 +0000\tmaster-replica-0\t14\t2021-06-02 03:41:48.984228: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:41:52 +0000\tmaster-replica-0\t14\tEpoch 10/10\n",
      "INFO\t2021-06-02 03:41:52 +0000\tmaster-replica-0\t14\t\n",
      "INFO\t2021-06-02 03:41:52 +0000\tmaster-replica-0\t14\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/14/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:41:52 +0000\tmaster-replica-0\t14\t40000/40000 - 94s - loss: 1.1606 - rmse: 1.0654 - mse: 1.1606 - val_loss: 3.0161 - val_rmse: 1.7364 - val_mse: 3.0161\n",
      "ERROR\t2021-06-02 03:41:52 +0000\tmaster-replica-0\t14\t2021-06-02 03:41:52.497702: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 03:42:05 +0000\tmaster-replica-0\t14\t2021-06-02 03:42:05.656364: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 03:42:06 +0000\tmaster-replica-0\t14\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:42:06 +0000\tmaster-replica-0\t14\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:42:06 +0000\tmaster-replica-0\t14\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 03:42:12 +0000\tmaster-replica-0\t14\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/14/20210602034204\n",
      "ERROR\t2021-06-02 03:43:17 +0000\tmaster-replica-0\t16\t2021-06-02 03:43:17.389116: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tModel: \"model\"\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t==================================================================================================\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tdeep_inputs (DenseFeatures)     (None, 52)           50000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tdeep_h1 (Dense)                 (None, 32)           1696        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t==================================================================================================\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tTotal params: 52,553\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tTrainable params: 52,553\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tNone\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tTrain for 40000 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tEpoch 1/10\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/16/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:43:22 +0000\tmaster-replica-0\t16\t40000/40000 - 105s - loss: 1.3947 - rmse: 1.1597 - mse: 1.3947 - val_loss: 4.1762 - val_rmse: 2.0424 - val_mse: 4.1762\n",
      "ERROR\t2021-06-02 03:43:24 +0000\tmaster-replica-0\t15\t2021-06-02 03:43:24.447238: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:43:28 +0000\tmaster-replica-0\t15\tEpoch 10/10\n",
      "INFO\t2021-06-02 03:43:28 +0000\tmaster-replica-0\t15\t\n",
      "INFO\t2021-06-02 03:43:28 +0000\tmaster-replica-0\t15\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/15/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:43:28 +0000\tmaster-replica-0\t15\t40000/40000 - 100s - loss: 1.1610 - rmse: 1.0655 - mse: 1.1610 - val_loss: 2.2589 - val_rmse: 1.5024 - val_mse: 2.2589\n",
      "ERROR\t2021-06-02 03:43:28 +0000\tmaster-replica-0\t15\t2021-06-02 03:43:28.010967: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 03:43:42 +0000\tmaster-replica-0\t15\t2021-06-02 03:43:42.537544: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 03:43:43 +0000\tmaster-replica-0\t15\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:43:43 +0000\tmaster-replica-0\t15\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:43:43 +0000\tmaster-replica-0\t15\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 03:43:49 +0000\tmaster-replica-0\t15\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/15/20210602034341\n",
      "ERROR\t2021-06-02 03:44:28 +0000\tmaster-replica-0\t8\t2021-06-02 03:44:28.231748: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:44:31 +0000\tmaster-replica-0\t8\tEpoch 8/10\n",
      "INFO\t2021-06-02 03:44:31 +0000\tmaster-replica-0\t8\t\n",
      "INFO\t2021-06-02 03:44:31 +0000\tmaster-replica-0\t8\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/8/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:44:31 +0000\tmaster-replica-0\t8\t200000/200000 - 493s - loss: 0.9526 - rmse: 0.9458 - mse: 0.9526 - val_loss: 6.8855 - val_rmse: 2.6234 - val_mse: 6.8855\n",
      "ERROR\t2021-06-02 03:44:57 +0000\tmaster-replica-0\t16\t2021-06-02 03:44:57.407174: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:45:00 +0000\tservice\t14\tJob completed successfully.\n",
      "INFO\t2021-06-02 03:45:01 +0000\tmaster-replica-0\t16\tEpoch 2/10\n",
      "INFO\t2021-06-02 03:45:01 +0000\tmaster-replica-0\t16\t\n",
      "INFO\t2021-06-02 03:45:01 +0000\tmaster-replica-0\t16\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/16/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:45:01 +0000\tmaster-replica-0\t16\t40000/40000 - 99s - loss: 0.9592 - rmse: 0.9722 - mse: 0.9592 - val_loss: 6.0713 - val_rmse: 2.4636 - val_mse: 6.0713\n",
      "ERROR\t2021-06-02 03:45:02 +0000\tmaster-replica-0\t12\t2021-06-02 03:45:02.358863: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:45:06 +0000\tmaster-replica-0\t12\tEpoch 6/10\n",
      "INFO\t2021-06-02 03:45:06 +0000\tmaster-replica-0\t12\t\n",
      "INFO\t2021-06-02 03:45:06 +0000\tmaster-replica-0\t12\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/12/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:45:06 +0000\tmaster-replica-0\t12\t200000/200000 - 412s - loss: 0.9959 - rmse: 0.9657 - mse: 0.9959 - val_loss: 6.3363 - val_rmse: 2.5162 - val_mse: 6.3363\n",
      "ERROR\t2021-06-02 03:46:35 +0000\tmaster-replica-0\t16\t2021-06-02 03:46:35.556233: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:46:39 +0000\tmaster-replica-0\t16\tEpoch 3/10\n",
      "INFO\t2021-06-02 03:46:39 +0000\tmaster-replica-0\t16\t\n",
      "INFO\t2021-06-02 03:46:39 +0000\tmaster-replica-0\t16\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/16/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:46:39 +0000\tmaster-replica-0\t16\t40000/40000 - 98s - loss: 0.9458 - rmse: 0.9658 - mse: 0.9458 - val_loss: 7.6294 - val_rmse: 2.7616 - val_mse: 7.6294\n",
      "INFO\t2021-06-02 03:46:42 +0000\tservice\t15\tJob completed successfully.\n",
      "INFO\t2021-06-02 03:46:51 +0000\tservice\t17\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 03:46:53 +0000\tservice\t17\tWaiting for training program to start.\n",
      "INFO\t2021-06-02 03:47:25 +0000\tservice\t18\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 03:47:27 +0000\tservice\t18\tWaiting for training program to start.\n",
      "ERROR\t2021-06-02 03:48:12 +0000\tmaster-replica-0\t16\t2021-06-02 03:48:12.118842: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:48:16 +0000\tmaster-replica-0\t16\tEpoch 4/10\n",
      "INFO\t2021-06-02 03:48:16 +0000\tmaster-replica-0\t16\t\n",
      "INFO\t2021-06-02 03:48:16 +0000\tmaster-replica-0\t16\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/16/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:48:16 +0000\tmaster-replica-0\t16\t40000/40000 - 97s - loss: 0.9684 - rmse: 0.9774 - mse: 0.9684 - val_loss: 8.6329 - val_rmse: 2.9372 - val_mse: 8.6329\n",
      "ERROR\t2021-06-02 03:49:48 +0000\tmaster-replica-0\t16\t2021-06-02 03:49:48.948664: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:49:52 +0000\tmaster-replica-0\t16\tEpoch 5/10\n",
      "INFO\t2021-06-02 03:49:52 +0000\tmaster-replica-0\t16\t\n",
      "INFO\t2021-06-02 03:49:52 +0000\tmaster-replica-0\t16\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/16/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:49:52 +0000\tmaster-replica-0\t16\t40000/40000 - 96s - loss: 1.3792 - rmse: 1.1563 - mse: 1.3792 - val_loss: 2.2491 - val_rmse: 1.4991 - val_mse: 2.2491\n",
      "ERROR\t2021-06-02 03:51:25 +0000\tmaster-replica-0\t17\t2021-06-02 03:51:25.031679: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 03:51:25 +0000\tmaster-replica-0\t17\t2021-06-02 03:51:25.031954: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 03:51:25 +0000\tmaster-replica-0\t17\t2021-06-02 03:51:25.031984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 03:51:26 +0000\tmaster-replica-0\t16\t2021-06-02 03:51:26.514913: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\t2021-06-02 03:51:27.084567: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\t2021-06-02 03:51:27.084627: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\t2021-06-02 03:51:27.084683: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-8446937006266565): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\t2021-06-02 03:51:27.084981: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\t2021-06-02 03:51:27.096852: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\t2021-06-02 03:51:27.097332: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5611e4f53080 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\t2021-06-02 03:51:27.097382: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:51:27 +0000\tmaster-replica-0\t17\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "INFO\t2021-06-02 03:51:30 +0000\tmaster-replica-0\t16\tEpoch 6/10\n",
      "INFO\t2021-06-02 03:51:30 +0000\tmaster-replica-0\t16\t\n",
      "INFO\t2021-06-02 03:51:30 +0000\tmaster-replica-0\t16\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/16/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:51:30 +0000\tmaster-replica-0\t16\t40000/40000 - 98s - loss: 0.9998 - rmse: 0.9923 - mse: 0.9998 - val_loss: 3.1383 - val_rmse: 1.7710 - val_mse: 3.1383\n",
      "ERROR\t2021-06-02 03:51:43 +0000\tmaster-replica-0\t12\t2021-06-02 03:51:43.079970: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:51:46 +0000\tmaster-replica-0\t12\tEpoch 7/10\n",
      "INFO\t2021-06-02 03:51:46 +0000\tmaster-replica-0\t12\t\n",
      "INFO\t2021-06-02 03:51:46 +0000\tmaster-replica-0\t12\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/12/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:51:46 +0000\tmaster-replica-0\t12\t200000/200000 - 400s - loss: 0.9404 - rmse: 0.9392 - mse: 0.9404 - val_loss: 7.8342 - val_rmse: 2.7981 - val_mse: 7.8342\n",
      "ERROR\t2021-06-02 03:52:00 +0000\tmaster-replica-0\t18\t2021-06-02 03:52:00.508795: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 03:52:00 +0000\tmaster-replica-0\t18\t2021-06-02 03:52:00.509097: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 03:52:00 +0000\tmaster-replica-0\t18\t2021-06-02 03:52:00.509142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\t2021-06-02 03:52:02.329535: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\t2021-06-02 03:52:02.329612: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\t2021-06-02 03:52:02.329657: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-810423576859595920): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\t2021-06-02 03:52:02.329999: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\t2021-06-02 03:52:02.340836: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\t2021-06-02 03:52:02.341732: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ad63232b90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\t2021-06-02 03:52:02.341790: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:52:02 +0000\tmaster-replica-0\t18\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 03:52:37 +0000\tmaster-replica-0\t8\t2021-06-02 03:52:37.463845: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:52:41 +0000\tmaster-replica-0\t8\tEpoch 9/10\n",
      "INFO\t2021-06-02 03:52:41 +0000\tmaster-replica-0\t8\t\n",
      "INFO\t2021-06-02 03:52:41 +0000\tmaster-replica-0\t8\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/8/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:52:41 +0000\tmaster-replica-0\t8\t200000/200000 - 489s - loss: 1.2264 - rmse: 1.0615 - mse: 1.2264 - val_loss: 2.2746 - val_rmse: 1.5070 - val_mse: 2.2746\n",
      "ERROR\t2021-06-02 03:53:04 +0000\tmaster-replica-0\t16\t2021-06-02 03:53:04.111381: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:53:07 +0000\tmaster-replica-0\t16\tEpoch 7/10\n",
      "INFO\t2021-06-02 03:53:07 +0000\tmaster-replica-0\t16\t\n",
      "INFO\t2021-06-02 03:53:07 +0000\tmaster-replica-0\t16\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/16/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:53:07 +0000\tmaster-replica-0\t16\t40000/40000 - 97s - loss: 0.9394 - rmse: 0.9623 - mse: 0.9394 - val_loss: 3.6993 - val_rmse: 1.9226 - val_mse: 3.6993\n",
      "ERROR\t2021-06-02 03:53:31 +0000\tmaster-replica-0\t17\t2021-06-02 03:53:31.163383: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tModel: \"model\"\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t==================================================================================================\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tdeep_inputs (DenseFeatures)     (None, 60)           58000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tdeep_h1 (Dense)                 (None, 32)           1952        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t==================================================================================================\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tTotal params: 60,809\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tTrainable params: 60,809\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tNone\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tTrain for 40000 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tEpoch 1/10\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/17/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:53:35 +0000\tmaster-replica-0\t17\t40000/40000 - 128s - loss: 1.3819 - rmse: 1.1560 - mse: 1.3819 - val_loss: 3.8683 - val_rmse: 1.9663 - val_mse: 3.8683\n",
      "ERROR\t2021-06-02 03:54:00 +0000\tmaster-replica-0\t18\t2021-06-02 03:54:00.554799: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tModel: \"model\"\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t==================================================================================================\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tdeep_inputs (DenseFeatures)     (None, 22)           20000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tdeep_h1 (Dense)                 (None, 32)           736         deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t==================================================================================================\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tTotal params: 21,593\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tTrainable params: 21,593\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tNone\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tTrain for 40000 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tEpoch 1/10\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/18/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:54:06 +0000\tmaster-replica-0\t18\t40000/40000 - 123s - loss: 1.3846 - rmse: 1.1575 - mse: 1.3846 - val_loss: 3.8352 - val_rmse: 1.9576 - val_mse: 3.8352\n",
      "ERROR\t2021-06-02 03:54:41 +0000\tmaster-replica-0\t16\t2021-06-02 03:54:41.502929: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:54:45 +0000\tmaster-replica-0\t16\tEpoch 8/10\n",
      "INFO\t2021-06-02 03:54:45 +0000\tmaster-replica-0\t16\t\n",
      "INFO\t2021-06-02 03:54:45 +0000\tmaster-replica-0\t16\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/16/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:54:45 +0000\tmaster-replica-0\t16\t40000/40000 - 97s - loss: 0.9531 - rmse: 0.9696 - mse: 0.9531 - val_loss: 4.0517 - val_rmse: 2.0122 - val_mse: 4.0517\n",
      "ERROR\t2021-06-02 03:55:43 +0000\tmaster-replica-0\t17\t2021-06-02 03:55:43.414572: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:55:47 +0000\tmaster-replica-0\t17\tEpoch 2/10\n",
      "INFO\t2021-06-02 03:55:47 +0000\tmaster-replica-0\t17\t\n",
      "INFO\t2021-06-02 03:55:47 +0000\tmaster-replica-0\t17\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/17/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:55:47 +0000\tmaster-replica-0\t17\t40000/40000 - 131s - loss: 0.9583 - rmse: 0.9718 - mse: 0.9583 - val_loss: 5.4196 - val_rmse: 2.3275 - val_mse: 5.4196\n",
      "ERROR\t2021-06-02 03:55:57 +0000\tmaster-replica-0\t18\t2021-06-02 03:55:57.084158: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:56:00 +0000\tmaster-replica-0\t18\tEpoch 2/10\n",
      "INFO\t2021-06-02 03:56:00 +0000\tmaster-replica-0\t18\t\n",
      "INFO\t2021-06-02 03:56:00 +0000\tmaster-replica-0\t18\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/18/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:56:00 +0000\tmaster-replica-0\t18\t40000/40000 - 115s - loss: 0.9589 - rmse: 0.9721 - mse: 0.9589 - val_loss: 5.6947 - val_rmse: 2.3858 - val_mse: 5.6947\n",
      "ERROR\t2021-06-02 03:56:18 +0000\tmaster-replica-0\t16\t2021-06-02 03:56:18.695624: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:56:22 +0000\tmaster-replica-0\t16\tEpoch 9/10\n",
      "INFO\t2021-06-02 03:56:22 +0000\tmaster-replica-0\t16\t\n",
      "INFO\t2021-06-02 03:56:22 +0000\tmaster-replica-0\t16\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/16/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:56:22 +0000\tmaster-replica-0\t16\t40000/40000 - 97s - loss: 1.2259 - rmse: 1.0881 - mse: 1.2259 - val_loss: 2.1415 - val_rmse: 1.4622 - val_mse: 2.1415\n",
      "ERROR\t2021-06-02 03:57:56 +0000\tmaster-replica-0\t17\t2021-06-02 03:57:56.569042: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 03:57:56 +0000\tmaster-replica-0\t18\t2021-06-02 03:57:56.951580: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 03:57:57 +0000\tmaster-replica-0\t16\t2021-06-02 03:57:57.983925: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:58:00 +0000\tmaster-replica-0\t17\tEpoch 3/10\n",
      "INFO\t2021-06-02 03:58:00 +0000\tmaster-replica-0\t17\t\n",
      "INFO\t2021-06-02 03:58:00 +0000\tmaster-replica-0\t17\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/17/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:58:00 +0000\tmaster-replica-0\t17\t40000/40000 - 133s - loss: 0.9470 - rmse: 0.9664 - mse: 0.9470 - val_loss: 6.1075 - val_rmse: 2.4704 - val_mse: 6.1075\n",
      "INFO\t2021-06-02 03:58:00 +0000\tmaster-replica-0\t18\tEpoch 3/10\n",
      "INFO\t2021-06-02 03:58:00 +0000\tmaster-replica-0\t18\t\n",
      "INFO\t2021-06-02 03:58:00 +0000\tmaster-replica-0\t18\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/18/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:58:00 +0000\tmaster-replica-0\t18\t40000/40000 - 120s - loss: 0.9474 - rmse: 0.9666 - mse: 0.9474 - val_loss: 6.4779 - val_rmse: 2.5443 - val_mse: 6.4779\n",
      "INFO\t2021-06-02 03:58:01 +0000\tmaster-replica-0\t16\tEpoch 10/10\n",
      "INFO\t2021-06-02 03:58:01 +0000\tmaster-replica-0\t16\t\n",
      "INFO\t2021-06-02 03:58:01 +0000\tmaster-replica-0\t16\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/16/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:58:01 +0000\tmaster-replica-0\t16\t40000/40000 - 99s - loss: 1.1600 - rmse: 1.0651 - mse: 1.1600 - val_loss: 2.3054 - val_rmse: 1.5178 - val_mse: 2.3054\n",
      "ERROR\t2021-06-02 03:58:01 +0000\tmaster-replica-0\t16\t2021-06-02 03:58:01.472726: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 03:58:14 +0000\tmaster-replica-0\t16\t2021-06-02 03:58:14.811529: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 03:58:15 +0000\tmaster-replica-0\t16\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 03:58:15 +0000\tmaster-replica-0\t16\tInstructions for updating:\n",
      "ERROR\t2021-06-02 03:58:15 +0000\tmaster-replica-0\t16\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 03:58:21 +0000\tmaster-replica-0\t16\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/16/20210602035813\n",
      "ERROR\t2021-06-02 03:58:26 +0000\tmaster-replica-0\t12\t2021-06-02 03:58:26.391791: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:58:31 +0000\tmaster-replica-0\t12\tEpoch 8/10\n",
      "INFO\t2021-06-02 03:58:31 +0000\tmaster-replica-0\t12\t\n",
      "INFO\t2021-06-02 03:58:31 +0000\tmaster-replica-0\t12\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/12/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:58:31 +0000\tmaster-replica-0\t12\t200000/200000 - 405s - loss: 0.9519 - rmse: 0.9455 - mse: 0.9518 - val_loss: 9.7487 - val_rmse: 3.1215 - val_mse: 9.7487\n",
      "ERROR\t2021-06-02 03:59:51 +0000\tmaster-replica-0\t18\t2021-06-02 03:59:51.413326: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 03:59:55 +0000\tmaster-replica-0\t18\tEpoch 4/10\n",
      "INFO\t2021-06-02 03:59:55 +0000\tmaster-replica-0\t18\t\n",
      "INFO\t2021-06-02 03:59:55 +0000\tmaster-replica-0\t18\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/18/checkpoints/babyweight\n",
      "INFO\t2021-06-02 03:59:55 +0000\tmaster-replica-0\t18\t40000/40000 - 114s - loss: 0.9688 - rmse: 0.9777 - mse: 0.9688 - val_loss: 6.6869 - val_rmse: 2.5851 - val_mse: 6.6869\n",
      "ERROR\t2021-06-02 04:00:08 +0000\tmaster-replica-0\t17\t2021-06-02 04:00:08.846424: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:00:12 +0000\tmaster-replica-0\t17\tEpoch 4/10\n",
      "INFO\t2021-06-02 04:00:12 +0000\tmaster-replica-0\t17\t\n",
      "INFO\t2021-06-02 04:00:12 +0000\tmaster-replica-0\t17\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/17/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:00:12 +0000\tmaster-replica-0\t17\t40000/40000 - 132s - loss: 0.9685 - rmse: 0.9775 - mse: 0.9685 - val_loss: 6.8806 - val_rmse: 2.6223 - val_mse: 6.8806\n",
      "ERROR\t2021-06-02 04:00:43 +0000\tmaster-replica-0\t8\t2021-06-02 04:00:43.978087: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:00:47 +0000\tmaster-replica-0\t8\tEpoch 10/10\n",
      "INFO\t2021-06-02 04:00:47 +0000\tmaster-replica-0\t8\t\n",
      "INFO\t2021-06-02 04:00:47 +0000\tmaster-replica-0\t8\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/8/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:00:47 +0000\tmaster-replica-0\t8\t200000/200000 - 486s - loss: 1.1586 - rmse: 1.0383 - mse: 1.1586 - val_loss: 3.7746 - val_rmse: 1.9422 - val_mse: 3.7746\n",
      "ERROR\t2021-06-02 04:00:47 +0000\tmaster-replica-0\t8\t2021-06-02 04:00:47.483314: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 04:01:08 +0000\tmaster-replica-0\t8\t2021-06-02 04:01:08.089477: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 04:01:08 +0000\tmaster-replica-0\t8\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 04:01:08 +0000\tmaster-replica-0\t8\tInstructions for updating:\n",
      "ERROR\t2021-06-02 04:01:08 +0000\tmaster-replica-0\t8\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 04:01:14 +0000\tmaster-replica-0\t8\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/8/20210602040107\n",
      "INFO\t2021-06-02 04:01:26 +0000\tservice\t16\tJob completed successfully.\n",
      "ERROR\t2021-06-02 04:01:42 +0000\tmaster-replica-0\t18\t2021-06-02 04:01:42.398863: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:01:46 +0000\tmaster-replica-0\t18\tEpoch 5/10\n",
      "INFO\t2021-06-02 04:01:46 +0000\tmaster-replica-0\t18\t\n",
      "INFO\t2021-06-02 04:01:46 +0000\tmaster-replica-0\t18\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/18/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:01:46 +0000\tmaster-replica-0\t18\t40000/40000 - 111s - loss: 1.3764 - rmse: 1.1553 - mse: 1.3764 - val_loss: 2.5897 - val_rmse: 1.6087 - val_mse: 2.5897\n",
      "ERROR\t2021-06-02 04:02:17 +0000\tmaster-replica-0\t17\t2021-06-02 04:02:17.663013: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:02:21 +0000\tmaster-replica-0\t17\tEpoch 5/10\n",
      "INFO\t2021-06-02 04:02:21 +0000\tmaster-replica-0\t17\t\n",
      "INFO\t2021-06-02 04:02:21 +0000\tmaster-replica-0\t17\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/17/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:02:21 +0000\tmaster-replica-0\t17\t40000/40000 - 128s - loss: 1.3749 - rmse: 1.1547 - mse: 1.3749 - val_loss: 2.3208 - val_rmse: 1.5229 - val_mse: 2.3208\n",
      "ERROR\t2021-06-02 04:03:40 +0000\tmaster-replica-0\t18\t2021-06-02 04:03:40.240962: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:03:44 +0000\tmaster-replica-0\t18\tEpoch 6/10\n",
      "INFO\t2021-06-02 04:03:44 +0000\tmaster-replica-0\t18\t\n",
      "INFO\t2021-06-02 04:03:44 +0000\tmaster-replica-0\t18\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/18/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:03:44 +0000\tmaster-replica-0\t18\t40000/40000 - 118s - loss: 0.9985 - rmse: 0.9916 - mse: 0.9985 - val_loss: 3.4884 - val_rmse: 1.8671 - val_mse: 3.4884\n",
      "INFO\t2021-06-02 04:03:54 +0000\tservice\t19\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 04:03:56 +0000\tservice\t19\tWaiting for training program to start.\n",
      "INFO\t2021-06-02 04:04:03 +0000\tservice\t8\tJob completed successfully.\n",
      "ERROR\t2021-06-02 04:05:11 +0000\tmaster-replica-0\t12\t2021-06-02 04:05:11.997960: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:05:15 +0000\tmaster-replica-0\t12\tEpoch 9/10\n",
      "INFO\t2021-06-02 04:05:15 +0000\tmaster-replica-0\t12\t\n",
      "INFO\t2021-06-02 04:05:15 +0000\tmaster-replica-0\t12\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/12/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:05:15 +0000\tmaster-replica-0\t12\t200000/200000 - 404s - loss: 1.2296 - rmse: 1.0627 - mse: 1.2296 - val_loss: 2.2201 - val_rmse: 1.4893 - val_mse: 2.2201\n",
      "ERROR\t2021-06-02 04:05:35 +0000\tmaster-replica-0\t17\t2021-06-02 04:05:35.020115: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:150] Filling up shuffle buffer (this may take a while): 1 of 10000\n",
      "ERROR\t2021-06-02 04:05:35 +0000\tmaster-replica-0\t17\t2021-06-02 04:05:35.020252: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:150] Filling up shuffle buffer (this may take a while): 2 of 10000\n",
      "ERROR\t2021-06-02 04:05:35 +0000\tmaster-replica-0\t17\t2021-06-02 04:05:35.020294: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:150] Filling up shuffle buffer (this may take a while): 3 of 10000\n",
      "ERROR\t2021-06-02 04:05:35 +0000\tmaster-replica-0\t17\t2021-06-02 04:05:35.020320: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:150] Filling up shuffle buffer (this may take a while): 4 of 10000\n",
      "ERROR\t2021-06-02 04:05:35 +0000\tmaster-replica-0\t17\t2021-06-02 04:05:35.022079: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:150] Filling up shuffle buffer (this may take a while): 5 of 10000\n",
      "ERROR\t2021-06-02 04:05:35 +0000\tmaster-replica-0\t17\t2021-06-02 04:05:35.022151: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:150] Filling up shuffle buffer (this may take a while): 6 of 10000\n",
      "ERROR\t2021-06-02 04:05:35 +0000\tmaster-replica-0\t17\t2021-06-02 04:05:35.138674: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:199] Shuffle buffer filled.\n",
      "ERROR\t2021-06-02 04:05:36 +0000\tmaster-replica-0\t17\t2021-06-02 04:05:36.997195: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:05:41 +0000\tmaster-replica-0\t17\tEpoch 6/10\n",
      "INFO\t2021-06-02 04:05:41 +0000\tmaster-replica-0\t17\t\n",
      "INFO\t2021-06-02 04:05:41 +0000\tmaster-replica-0\t17\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/17/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:05:41 +0000\tmaster-replica-0\t17\t40000/40000 - 200s - loss: 1.0004 - rmse: 0.9926 - mse: 1.0004 - val_loss: 3.4536 - val_rmse: 1.8577 - val_mse: 3.4536\n",
      "ERROR\t2021-06-02 04:05:53 +0000\tmaster-replica-0\t18\t2021-06-02 04:05:53.460542: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:05:57 +0000\tmaster-replica-0\t18\tEpoch 7/10\n",
      "INFO\t2021-06-02 04:05:57 +0000\tmaster-replica-0\t18\t\n",
      "INFO\t2021-06-02 04:05:57 +0000\tmaster-replica-0\t18\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/18/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:05:57 +0000\tmaster-replica-0\t18\t40000/40000 - 133s - loss: 0.9408 - rmse: 0.9631 - mse: 0.9408 - val_loss: 4.1473 - val_rmse: 2.0358 - val_mse: 4.1473\n",
      "INFO\t2021-06-02 04:06:41 +0000\tservice\t20\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 04:06:43 +0000\tservice\t20\tWaiting for training program to start.\n",
      "ERROR\t2021-06-02 04:07:58 +0000\tmaster-replica-0\t17\t2021-06-02 04:07:58.161351: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 04:07:58 +0000\tmaster-replica-0\t18\t2021-06-02 04:07:58.607868: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:08:02 +0000\tmaster-replica-0\t17\tEpoch 7/10\n",
      "INFO\t2021-06-02 04:08:02 +0000\tmaster-replica-0\t17\t\n",
      "INFO\t2021-06-02 04:08:02 +0000\tmaster-replica-0\t17\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/17/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:08:02 +0000\tmaster-replica-0\t17\t40000/40000 - 141s - loss: 0.9419 - rmse: 0.9636 - mse: 0.9419 - val_loss: 4.2566 - val_rmse: 2.0625 - val_mse: 4.2566\n",
      "INFO\t2021-06-02 04:08:02 +0000\tmaster-replica-0\t18\tEpoch 8/10\n",
      "INFO\t2021-06-02 04:08:02 +0000\tmaster-replica-0\t18\t\n",
      "INFO\t2021-06-02 04:08:02 +0000\tmaster-replica-0\t18\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/18/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:08:02 +0000\tmaster-replica-0\t18\t40000/40000 - 125s - loss: 0.9529 - rmse: 0.9696 - mse: 0.9529 - val_loss: 4.5578 - val_rmse: 2.1340 - val_mse: 4.5578\n",
      "ERROR\t2021-06-02 04:08:22 +0000\tmaster-replica-0\t19\t2021-06-02 04:08:22.486879: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 04:08:22 +0000\tmaster-replica-0\t19\t2021-06-02 04:08:22.487168: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 04:08:22 +0000\tmaster-replica-0\t19\t2021-06-02 04:08:22.487219: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\tInstructions for updating:\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\tInstructions for updating:\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\t2021-06-02 04:08:24.075163: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\t2021-06-02 04:08:24.075223: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\t2021-06-02 04:08:24.075264: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-13046618972230641684): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\t2021-06-02 04:08:24.075585: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\t2021-06-02 04:08:24.087909: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\t2021-06-02 04:08:24.088459: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562b0ce14030 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\t2021-06-02 04:08:24.088513: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\tInstructions for updating:\n",
      "ERROR\t2021-06-02 04:08:24 +0000\tmaster-replica-0\t19\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 04:09:59 +0000\tmaster-replica-0\t18\t2021-06-02 04:09:59.389370: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:10:03 +0000\tmaster-replica-0\t18\tEpoch 9/10\n",
      "INFO\t2021-06-02 04:10:03 +0000\tmaster-replica-0\t18\t\n",
      "INFO\t2021-06-02 04:10:03 +0000\tmaster-replica-0\t18\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/18/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:10:03 +0000\tmaster-replica-0\t18\t40000/40000 - 121s - loss: 1.2263 - rmse: 1.0883 - mse: 1.2263 - val_loss: 2.1212 - val_rmse: 1.4553 - val_mse: 2.1212\n",
      "ERROR\t2021-06-02 04:10:03 +0000\tmaster-replica-0\t19\t2021-06-02 04:10:03.114141: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tModel: \"model\"\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t==================================================================================================\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tdeep_inputs (DenseFeatures)     (None, 53)           51000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tdeep_h1 (Dense)                 (None, 32)           1728        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t==================================================================================================\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tTotal params: 53,585\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tTrainable params: 53,585\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tNone\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tTrain for 40000 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tEpoch 1/10\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/19/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:10:07 +0000\tmaster-replica-0\t19\t40000/40000 - 103s - loss: 1.3902 - rmse: 1.1592 - mse: 1.3902 - val_loss: 3.8482 - val_rmse: 1.9607 - val_mse: 3.8482\n",
      "ERROR\t2021-06-02 04:10:17 +0000\tmaster-replica-0\t17\t2021-06-02 04:10:17.333571: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:10:21 +0000\tmaster-replica-0\t17\tEpoch 8/10\n",
      "INFO\t2021-06-02 04:10:21 +0000\tmaster-replica-0\t17\t\n",
      "INFO\t2021-06-02 04:10:21 +0000\tmaster-replica-0\t17\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/17/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:10:21 +0000\tmaster-replica-0\t17\t40000/40000 - 139s - loss: 0.9522 - rmse: 0.9691 - mse: 0.9522 - val_loss: 5.1945 - val_rmse: 2.2785 - val_mse: 5.1945\n",
      "ERROR\t2021-06-02 04:11:11 +0000\tmaster-replica-0\t20\t2021-06-02 04:11:11.096828: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 04:11:11 +0000\tmaster-replica-0\t20\t2021-06-02 04:11:11.097130: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 04:11:11 +0000\tmaster-replica-0\t20\t2021-06-02 04:11:11.097699: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\tInstructions for updating:\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\tInstructions for updating:\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\t2021-06-02 04:11:13.070041: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\t2021-06-02 04:11:13.070120: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\t2021-06-02 04:11:13.070176: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-6193855290958106465): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\t2021-06-02 04:11:13.070479: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\t2021-06-02 04:11:13.081797: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\t2021-06-02 04:11:13.082245: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ad1ff08820 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\t2021-06-02 04:11:13.082293: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\tInstructions for updating:\n",
      "ERROR\t2021-06-02 04:11:13 +0000\tmaster-replica-0\t20\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 04:11:41 +0000\tmaster-replica-0\t19\t2021-06-02 04:11:41.218657: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:11:45 +0000\tmaster-replica-0\t19\tEpoch 2/10\n",
      "INFO\t2021-06-02 04:11:45 +0000\tmaster-replica-0\t19\t\n",
      "INFO\t2021-06-02 04:11:45 +0000\tmaster-replica-0\t19\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/19/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:11:45 +0000\tmaster-replica-0\t19\t40000/40000 - 97s - loss: 0.9595 - rmse: 0.9725 - mse: 0.9595 - val_loss: 5.4663 - val_rmse: 2.3376 - val_mse: 5.4663\n",
      "ERROR\t2021-06-02 04:11:53 +0000\tmaster-replica-0\t18\t2021-06-02 04:11:53.455363: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:11:57 +0000\tmaster-replica-0\t18\tEpoch 10/10\n",
      "INFO\t2021-06-02 04:11:57 +0000\tmaster-replica-0\t18\t\n",
      "INFO\t2021-06-02 04:11:57 +0000\tmaster-replica-0\t18\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/18/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:11:57 +0000\tmaster-replica-0\t18\t40000/40000 - 115s - loss: 1.1647 - rmse: 1.0670 - mse: 1.1647 - val_loss: 3.4518 - val_rmse: 1.8574 - val_mse: 3.4518\n",
      "ERROR\t2021-06-02 04:11:57 +0000\tmaster-replica-0\t18\t2021-06-02 04:11:57.917070: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 04:12:04 +0000\tmaster-replica-0\t12\t2021-06-02 04:12:04.218940: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:12:07 +0000\tmaster-replica-0\t12\tEpoch 10/10\n",
      "INFO\t2021-06-02 04:12:07 +0000\tmaster-replica-0\t12\t\n",
      "INFO\t2021-06-02 04:12:07 +0000\tmaster-replica-0\t12\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/12/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:12:07 +0000\tmaster-replica-0\t12\t200000/200000 - 412s - loss: 1.1563 - rmse: 1.0374 - mse: 1.1563 - val_loss: 5.9546 - val_rmse: 2.4397 - val_mse: 5.9546\n",
      "ERROR\t2021-06-02 04:12:07 +0000\tmaster-replica-0\t12\t2021-06-02 04:12:07.947411: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 04:12:18 +0000\tmaster-replica-0\t18\t2021-06-02 04:12:18.460679: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 04:12:18 +0000\tmaster-replica-0\t18\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 04:12:18 +0000\tmaster-replica-0\t18\tInstructions for updating:\n",
      "ERROR\t2021-06-02 04:12:18 +0000\tmaster-replica-0\t18\tIf using Keras pass *_constraint arguments to layers.\n",
      "ERROR\t2021-06-02 04:12:21 +0000\tmaster-replica-0\t17\t2021-06-02 04:12:21.391145: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:12:25 +0000\tmaster-replica-0\t17\tEpoch 9/10\n",
      "INFO\t2021-06-02 04:12:25 +0000\tmaster-replica-0\t17\t\n",
      "INFO\t2021-06-02 04:12:25 +0000\tmaster-replica-0\t17\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/17/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:12:25 +0000\tmaster-replica-0\t17\t40000/40000 - 124s - loss: 1.2255 - rmse: 1.0876 - mse: 1.2255 - val_loss: 2.1177 - val_rmse: 1.4544 - val_mse: 2.1177\n",
      "ERROR\t2021-06-02 04:12:26 +0000\tmaster-replica-0\t12\t2021-06-02 04:12:26.016335: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "INFO\t2021-06-02 04:12:26 +0000\tmaster-replica-0\t18\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/18/20210602041217\n",
      "ERROR\t2021-06-02 04:12:26 +0000\tmaster-replica-0\t12\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 04:12:26 +0000\tmaster-replica-0\t12\tInstructions for updating:\n",
      "ERROR\t2021-06-02 04:12:26 +0000\tmaster-replica-0\t12\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 04:12:32 +0000\tmaster-replica-0\t12\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/12/20210602041224\n",
      "ERROR\t2021-06-02 04:13:02 +0000\tmaster-replica-0\t20\t2021-06-02 04:13:02.024114: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tModel: \"model\"\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t==================================================================================================\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tdeep_inputs (DenseFeatures)     (None, 55)           53000       gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tdeep_h1 (Dense)                 (None, 32)           1792        deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t==================================================================================================\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tTotal params: 55,649\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tTrainable params: 55,649\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tNone\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tTrain for 40816 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tEpoch 1/10\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/20/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:13:06 +0000\tmaster-replica-0\t20\t40816/40816 - 113s - loss: 1.3942 - rmse: 1.1592 - mse: 1.3942 - val_loss: 3.8462 - val_rmse: 1.9601 - val_mse: 3.8462\n",
      "ERROR\t2021-06-02 04:13:16 +0000\tmaster-replica-0\t19\t2021-06-02 04:13:16.051512: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:13:19 +0000\tmaster-replica-0\t19\tEpoch 3/10\n",
      "INFO\t2021-06-02 04:13:19 +0000\tmaster-replica-0\t19\t\n",
      "INFO\t2021-06-02 04:13:19 +0000\tmaster-replica-0\t19\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/19/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:13:19 +0000\tmaster-replica-0\t19\t40000/40000 - 95s - loss: 0.9467 - rmse: 0.9663 - mse: 0.9467 - val_loss: 6.6717 - val_rmse: 2.5822 - val_mse: 6.6717\n",
      "ERROR\t2021-06-02 04:14:26 +0000\tmaster-replica-0\t17\t2021-06-02 04:14:26.783328: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:14:30 +0000\tmaster-replica-0\t17\tEpoch 10/10\n",
      "INFO\t2021-06-02 04:14:30 +0000\tmaster-replica-0\t17\t\n",
      "INFO\t2021-06-02 04:14:30 +0000\tmaster-replica-0\t17\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/17/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:14:30 +0000\tmaster-replica-0\t17\t40000/40000 - 125s - loss: 1.1626 - rmse: 1.0662 - mse: 1.1626 - val_loss: 2.9695 - val_rmse: 1.7228 - val_mse: 2.9695\n",
      "ERROR\t2021-06-02 04:14:30 +0000\tmaster-replica-0\t17\t2021-06-02 04:14:30.306965: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 04:14:46 +0000\tmaster-replica-0\t17\t2021-06-02 04:14:46.514345: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 04:14:47 +0000\tmaster-replica-0\t17\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 04:14:47 +0000\tmaster-replica-0\t17\tInstructions for updating:\n",
      "ERROR\t2021-06-02 04:14:47 +0000\tmaster-replica-0\t17\tIf using Keras pass *_constraint arguments to layers.\n",
      "ERROR\t2021-06-02 04:14:49 +0000\tmaster-replica-0\t19\t2021-06-02 04:14:49.546719: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:14:53 +0000\tmaster-replica-0\t19\tEpoch 4/10\n",
      "INFO\t2021-06-02 04:14:53 +0000\tmaster-replica-0\t19\t\n",
      "INFO\t2021-06-02 04:14:53 +0000\tmaster-replica-0\t19\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/19/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:14:53 +0000\tmaster-replica-0\t19\t40000/40000 - 93s - loss: 0.9693 - rmse: 0.9778 - mse: 0.9693 - val_loss: 7.2233 - val_rmse: 2.6867 - val_mse: 7.2233\n",
      "INFO\t2021-06-02 04:14:53 +0000\tmaster-replica-0\t17\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/17/20210602041445\n",
      "ERROR\t2021-06-02 04:14:53 +0000\tmaster-replica-0\t20\t2021-06-02 04:14:53.775737: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:14:57 +0000\tmaster-replica-0\t20\tEpoch 2/10\n",
      "INFO\t2021-06-02 04:14:57 +0000\tmaster-replica-0\t20\t\n",
      "INFO\t2021-06-02 04:14:57 +0000\tmaster-replica-0\t20\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/20/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:14:57 +0000\tmaster-replica-0\t20\t40816/40816 - 111s - loss: 0.9584 - rmse: 0.9717 - mse: 0.9584 - val_loss: 5.0928 - val_rmse: 2.2563 - val_mse: 5.0928\n",
      "INFO\t2021-06-02 04:15:16 +0000\tservice\t18\tJob completed successfully.\n",
      "INFO\t2021-06-02 04:15:54 +0000\tservice\t12\tJob completed successfully.\n",
      "ERROR\t2021-06-02 04:16:23 +0000\tmaster-replica-0\t19\t2021-06-02 04:16:23.872016: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:16:27 +0000\tmaster-replica-0\t19\tEpoch 5/10\n",
      "INFO\t2021-06-02 04:16:27 +0000\tmaster-replica-0\t19\t\n",
      "INFO\t2021-06-02 04:16:27 +0000\tmaster-replica-0\t19\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/19/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:16:27 +0000\tmaster-replica-0\t19\t40000/40000 - 94s - loss: 1.3766 - rmse: 1.1555 - mse: 1.3766 - val_loss: 2.3754 - val_rmse: 1.5406 - val_mse: 2.3754\n",
      "ERROR\t2021-06-02 04:16:44 +0000\tmaster-replica-0\t20\t2021-06-02 04:16:44.340528: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:16:48 +0000\tmaster-replica-0\t20\tEpoch 3/10\n",
      "INFO\t2021-06-02 04:16:48 +0000\tmaster-replica-0\t20\t\n",
      "INFO\t2021-06-02 04:16:48 +0000\tmaster-replica-0\t20\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/20/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:16:48 +0000\tmaster-replica-0\t20\t40816/40816 - 111s - loss: 0.9469 - rmse: 0.9663 - mse: 0.9469 - val_loss: 6.9032 - val_rmse: 2.6268 - val_mse: 6.9032\n",
      "INFO\t2021-06-02 04:17:43 +0000\tservice\t17\tJob completed successfully.\n",
      "ERROR\t2021-06-02 04:17:57 +0000\tmaster-replica-0\t19\t2021-06-02 04:17:57.946447: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:18:01 +0000\tmaster-replica-0\t19\tEpoch 6/10\n",
      "INFO\t2021-06-02 04:18:01 +0000\tmaster-replica-0\t19\t\n",
      "INFO\t2021-06-02 04:18:01 +0000\tmaster-replica-0\t19\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/19/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:18:01 +0000\tmaster-replica-0\t19\t40000/40000 - 94s - loss: 0.9986 - rmse: 0.9919 - mse: 0.9986 - val_loss: 3.7363 - val_rmse: 1.9322 - val_mse: 3.7363\n",
      "ERROR\t2021-06-02 04:18:35 +0000\tmaster-replica-0\t20\t2021-06-02 04:18:35.678153: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:18:39 +0000\tmaster-replica-0\t20\tEpoch 4/10\n",
      "INFO\t2021-06-02 04:18:39 +0000\tmaster-replica-0\t20\t\n",
      "INFO\t2021-06-02 04:18:39 +0000\tmaster-replica-0\t20\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/20/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:18:39 +0000\tmaster-replica-0\t20\t40816/40816 - 111s - loss: 0.9691 - rmse: 0.9776 - mse: 0.9691 - val_loss: 8.3672 - val_rmse: 2.8918 - val_mse: 8.3672\n",
      "ERROR\t2021-06-02 04:19:31 +0000\tmaster-replica-0\t19\t2021-06-02 04:19:31.641703: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:19:35 +0000\tmaster-replica-0\t19\tEpoch 7/10\n",
      "INFO\t2021-06-02 04:19:35 +0000\tmaster-replica-0\t19\t\n",
      "INFO\t2021-06-02 04:19:35 +0000\tmaster-replica-0\t19\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/19/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:19:35 +0000\tmaster-replica-0\t19\t40000/40000 - 94s - loss: 0.9421 - rmse: 0.9637 - mse: 0.9421 - val_loss: 4.4657 - val_rmse: 2.1125 - val_mse: 4.4657\n",
      "ERROR\t2021-06-02 04:20:28 +0000\tmaster-replica-0\t20\t2021-06-02 04:20:28.256306: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:20:32 +0000\tmaster-replica-0\t20\tEpoch 5/10\n",
      "INFO\t2021-06-02 04:20:32 +0000\tmaster-replica-0\t20\t\n",
      "INFO\t2021-06-02 04:20:32 +0000\tmaster-replica-0\t20\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/20/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:20:32 +0000\tmaster-replica-0\t20\t40816/40816 - 113s - loss: 1.3777 - rmse: 1.1555 - mse: 1.3777 - val_loss: 2.5013 - val_rmse: 1.5811 - val_mse: 2.5013\n",
      "ERROR\t2021-06-02 04:21:06 +0000\tmaster-replica-0\t19\t2021-06-02 04:21:06.733941: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:21:10 +0000\tmaster-replica-0\t19\tEpoch 8/10\n",
      "INFO\t2021-06-02 04:21:10 +0000\tmaster-replica-0\t19\t\n",
      "INFO\t2021-06-02 04:21:10 +0000\tmaster-replica-0\t19\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/19/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:21:10 +0000\tmaster-replica-0\t19\t40000/40000 - 95s - loss: 0.9527 - rmse: 0.9695 - mse: 0.9527 - val_loss: 5.1308 - val_rmse: 2.2645 - val_mse: 5.1308\n",
      "ERROR\t2021-06-02 04:22:23 +0000\tmaster-replica-0\t20\t2021-06-02 04:22:23.964255: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:22:27 +0000\tmaster-replica-0\t20\tEpoch 6/10\n",
      "INFO\t2021-06-02 04:22:27 +0000\tmaster-replica-0\t20\t\n",
      "INFO\t2021-06-02 04:22:27 +0000\tmaster-replica-0\t20\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/20/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:22:27 +0000\tmaster-replica-0\t20\t40816/40816 - 116s - loss: 1.0006 - rmse: 0.9928 - mse: 1.0006 - val_loss: 3.2515 - val_rmse: 1.8029 - val_mse: 3.2515\n",
      "ERROR\t2021-06-02 04:22:44 +0000\tmaster-replica-0\t19\t2021-06-02 04:22:44.880733: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:22:48 +0000\tmaster-replica-0\t19\tEpoch 9/10\n",
      "INFO\t2021-06-02 04:22:48 +0000\tmaster-replica-0\t19\t\n",
      "INFO\t2021-06-02 04:22:48 +0000\tmaster-replica-0\t19\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/19/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:22:48 +0000\tmaster-replica-0\t19\t40000/40000 - 98s - loss: 1.2273 - rmse: 1.0885 - mse: 1.2273 - val_loss: 2.1161 - val_rmse: 1.4539 - val_mse: 2.1161\n",
      "ERROR\t2021-06-02 04:24:20 +0000\tmaster-replica-0\t20\t2021-06-02 04:24:20.271625: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 04:24:21 +0000\tmaster-replica-0\t19\t2021-06-02 04:24:21.083078: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:24:24 +0000\tmaster-replica-0\t20\tEpoch 7/10\n",
      "INFO\t2021-06-02 04:24:24 +0000\tmaster-replica-0\t20\t\n",
      "INFO\t2021-06-02 04:24:24 +0000\tmaster-replica-0\t20\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/20/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:24:24 +0000\tmaster-replica-0\t20\t40816/40816 - 116s - loss: 0.9410 - rmse: 0.9630 - mse: 0.9410 - val_loss: 3.7731 - val_rmse: 1.9419 - val_mse: 3.7731\n",
      "INFO\t2021-06-02 04:24:25 +0000\tmaster-replica-0\t19\tEpoch 10/10\n",
      "INFO\t2021-06-02 04:24:25 +0000\tmaster-replica-0\t19\t\n",
      "INFO\t2021-06-02 04:24:25 +0000\tmaster-replica-0\t19\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/19/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:24:25 +0000\tmaster-replica-0\t19\t40000/40000 - 97s - loss: 1.1601 - rmse: 1.0652 - mse: 1.1601 - val_loss: 2.6407 - val_rmse: 1.6247 - val_mse: 2.6407\n",
      "ERROR\t2021-06-02 04:24:25 +0000\tmaster-replica-0\t19\t2021-06-02 04:24:25.214326: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 04:24:39 +0000\tmaster-replica-0\t19\t2021-06-02 04:24:39.464858: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 04:24:40 +0000\tmaster-replica-0\t19\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 04:24:40 +0000\tmaster-replica-0\t19\tInstructions for updating:\n",
      "ERROR\t2021-06-02 04:24:40 +0000\tmaster-replica-0\t19\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 04:24:46 +0000\tmaster-replica-0\t19\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/19/20210602042438\n",
      "ERROR\t2021-06-02 04:26:11 +0000\tmaster-replica-0\t20\t2021-06-02 04:26:11.814334: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:26:15 +0000\tmaster-replica-0\t20\tEpoch 8/10\n",
      "INFO\t2021-06-02 04:26:15 +0000\tmaster-replica-0\t20\t\n",
      "INFO\t2021-06-02 04:26:15 +0000\tmaster-replica-0\t20\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/20/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:26:15 +0000\tmaster-replica-0\t20\t40816/40816 - 111s - loss: 0.9533 - rmse: 0.9696 - mse: 0.9533 - val_loss: 4.8850 - val_rmse: 2.2095 - val_mse: 4.8850\n",
      "INFO\t2021-06-02 04:27:43 +0000\tservice\t19\tJob completed successfully.\n",
      "ERROR\t2021-06-02 04:28:01 +0000\tmaster-replica-0\t20\t2021-06-02 04:28:01.288840: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:28:05 +0000\tmaster-replica-0\t20\tEpoch 9/10\n",
      "INFO\t2021-06-02 04:28:05 +0000\tmaster-replica-0\t20\t\n",
      "INFO\t2021-06-02 04:28:05 +0000\tmaster-replica-0\t20\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/20/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:28:05 +0000\tmaster-replica-0\t20\t40816/40816 - 110s - loss: 1.2254 - rmse: 1.0875 - mse: 1.2254 - val_loss: 2.1104 - val_rmse: 1.4519 - val_mse: 2.1104\n",
      "ERROR\t2021-06-02 04:29:54 +0000\tmaster-replica-0\t20\t2021-06-02 04:29:54.170487: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 04:29:58 +0000\tmaster-replica-0\t20\tEpoch 10/10\n",
      "INFO\t2021-06-02 04:29:58 +0000\tmaster-replica-0\t20\t\n",
      "INFO\t2021-06-02 04:29:58 +0000\tmaster-replica-0\t20\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/20/checkpoints/babyweight\n",
      "INFO\t2021-06-02 04:29:58 +0000\tmaster-replica-0\t20\t40816/40816 - 113s - loss: 1.1610 - rmse: 1.0654 - mse: 1.1610 - val_loss: 2.7102 - val_rmse: 1.6459 - val_mse: 2.7102\n",
      "ERROR\t2021-06-02 04:29:58 +0000\tmaster-replica-0\t20\t2021-06-02 04:29:58.140939: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 04:30:13 +0000\tmaster-replica-0\t20\t2021-06-02 04:30:13.396963: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 04:30:13 +0000\tmaster-replica-0\t20\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 04:30:13 +0000\tmaster-replica-0\t20\tInstructions for updating:\n",
      "ERROR\t2021-06-02 04:30:13 +0000\tmaster-replica-0\t20\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 04:30:20 +0000\tmaster-replica-0\t20\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/hyperparam/20/20210602043012\n",
      "INFO\t2021-06-02 04:33:33 +0000\tservice\t20\tJob completed successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs stream-logs babyweight_210602_014807"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat training\n",
    "\n",
    "This time with tuned parameters for `batch_size` and `nembeds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model_tuned us-central1 babyweight_210602_014810\n",
      "jobId: babyweight_210602_014810\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [babyweight_210602_014810] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe babyweight_210602_014810\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs babyweight_210602_014810\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/babyweight/trained_model_tuned\n",
    "JOBNAME=babyweight_$(date -u +%y%m%d_%H%M%S)\n",
    "echo ${OUTDIR} ${REGION} ${JOBNAME}\n",
    "gsutil -m rm -rf ${OUTDIR}\n",
    "\n",
    "IMAGE=gcr.io/${PROJECT}/babyweight_training_container\n",
    "\n",
    "gcloud ai-platform jobs submit training ${JOBNAME} \\\n",
    "    --staging-bucket=gs://${BUCKET} \\\n",
    "    --region=${REGION} \\\n",
    "    --master-image-uri=${IMAGE} \\\n",
    "    --master-machine-type=n1-standard-4 \\\n",
    "    --scale-tier=CUSTOM \\\n",
    "    -- \\\n",
    "    --train_data_path=gs://${BUCKET}/babyweight/data/train*.csv \\\n",
    "    --eval_data_path=gs://${BUCKET}/babyweight/data/eval*.csv \\\n",
    "    --output_dir=${OUTDIR} \\\n",
    "    --num_epochs=10 \\\n",
    "    --train_examples=20000 \\\n",
    "    --eval_steps=100 \\\n",
    "    --batch_size=32 \\\n",
    "    --nembeds=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO\t2021-06-02 01:48:12 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2021-06-02 01:48:13 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2021-06-02 01:48:13 +0000\tservice\t\tWaiting for job to be provisioned.\n",
      "INFO\t2021-06-02 01:48:13 +0000\tservice\t\tJob babyweight_210602_014810 is queued.\n",
      "INFO\t2021-06-02 01:48:14 +0000\tservice\t\tWaiting for training program to start.\n",
      "INFO\t2021-06-02 01:48:40 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:48:40 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:48:40 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:48:40 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:48:40 +0000\tmaster-replica-0\t\t\n",
      "ERROR\t2021-06-02 01:52:40 +0000\tmaster-replica-0\t\t2021-06-02 01:52:40.704514: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:40 +0000\tmaster-replica-0\t\t2021-06-02 01:52:40.704803: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:40 +0000\tmaster-replica-0\t\t2021-06-02 01:52:40.704835: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4267: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: BucketizedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\t2021-06-02 01:52:42.537364: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\t2021-06-02 01:52:42.537415: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\t2021-06-02 01:52:42.537460: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (cmle-training-7292094349687996477): /proc/driver/nvidia/version does not exist\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\t2021-06-02 01:52:42.537809: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\t2021-06-02 01:52:42.549523: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\t2021-06-02 01:52:42.550033: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c731f595c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\t2021-06-02 01:52:42.550100: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4322: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "ERROR\t2021-06-02 01:52:42 +0000\tmaster-replica-0\t\tThe old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "ERROR\t2021-06-02 01:54:44 +0000\tmaster-replica-0\t\t2021-06-02 01:54:44.902586: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tHere is our Wide-and-Deep architecture so far:\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tModel: \"model\"\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tLayer (type)                    Output Shape         Param #     Connected to                     \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t==================================================================================================\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tgestation_weeks (InputLayer)    [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tis_male (InputLayer)            [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tmother_age (InputLayer)         [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tplurality (InputLayer)          [(None,)]            0                                            \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tdeep_inputs (DenseFeatures)     (None, 10)           8000        gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tdeep_h1 (Dense)                 (None, 32)           352         deep_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\twide_inputs (DenseFeatures)     (None, 71)           0           gestation_weeks[0][0]            \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t                                                                 is_male[0][0]                    \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t                                                                 mother_age[0][0]                 \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t                                                                 plurality[0][0]                  \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tdeep_h2 (Dense)                 (None, 8)            264         deep_h1[0][0]                    \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\twide_h (Dense)                  (None, 8)            576         wide_inputs[0][0]                \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tboth (Concatenate)              (None, 16)           0           deep_h2[0][0]                    \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t                                                                 wide_h[0][0]                     \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\toutput (Dense)                  (None, 1)            17          both[0][0]                       \n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t==================================================================================================\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tTotal params: 9,209\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tTrainable params: 9,209\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tNon-trainable params: 0\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t__________________________________________________________________________________________________\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tNone\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tTrain for 62500 steps, validate for 100 steps\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tEpoch 1/10\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\tEpoch 00001: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model_tuned/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:54:49 +0000\tmaster-replica-0\t\t62500/62500 - 126s - loss: 1.3829 - rmse: 1.1525 - mse: 1.3828 - val_loss: 4.3016 - val_rmse: 2.0734 - val_mse: 4.3016\n",
      "ERROR\t2021-06-02 01:56:50 +0000\tmaster-replica-0\t\t2021-06-02 01:56:50.266206: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:56:53 +0000\tmaster-replica-0\t\tEpoch 2/10\n",
      "INFO\t2021-06-02 01:56:53 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:56:53 +0000\tmaster-replica-0\t\tEpoch 00002: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model_tuned/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:56:53 +0000\tmaster-replica-0\t\t62500/62500 - 124s - loss: 0.9593 - rmse: 0.9688 - mse: 0.9593 - val_loss: 6.0578 - val_rmse: 2.4605 - val_mse: 6.0578\n",
      "ERROR\t2021-06-02 01:58:53 +0000\tmaster-replica-0\t\t2021-06-02 01:58:53.141318: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 01:58:58 +0000\tmaster-replica-0\t\tEpoch 3/10\n",
      "INFO\t2021-06-02 01:58:58 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 01:58:58 +0000\tmaster-replica-0\t\tEpoch 00003: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model_tuned/checkpoints/babyweight\n",
      "INFO\t2021-06-02 01:58:58 +0000\tmaster-replica-0\t\t62500/62500 - 124s - loss: 0.9457 - rmse: 0.9624 - mse: 0.9457 - val_loss: 6.8900 - val_rmse: 2.6242 - val_mse: 6.8900\n",
      "ERROR\t2021-06-02 02:00:56 +0000\tmaster-replica-0\t\t2021-06-02 02:00:56.910382: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:01:00 +0000\tmaster-replica-0\t\tEpoch 4/10\n",
      "INFO\t2021-06-02 02:01:00 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 02:01:00 +0000\tmaster-replica-0\t\tEpoch 00004: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model_tuned/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:01:00 +0000\tmaster-replica-0\t\t62500/62500 - 122s - loss: 0.9699 - rmse: 0.9747 - mse: 0.9699 - val_loss: 7.9418 - val_rmse: 2.8166 - val_mse: 7.9418\n",
      "ERROR\t2021-06-02 02:02:59 +0000\tmaster-replica-0\t\t2021-06-02 02:02:59.034350: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:03:02 +0000\tmaster-replica-0\t\tEpoch 5/10\n",
      "INFO\t2021-06-02 02:03:02 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 02:03:02 +0000\tmaster-replica-0\t\tEpoch 00005: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model_tuned/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:03:02 +0000\tmaster-replica-0\t\t62500/62500 - 122s - loss: 1.3779 - rmse: 1.1518 - mse: 1.3779 - val_loss: 3.1355 - val_rmse: 1.7700 - val_mse: 3.1355\n",
      "ERROR\t2021-06-02 02:04:59 +0000\tmaster-replica-0\t\t2021-06-02 02:04:59.767496: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:05:03 +0000\tmaster-replica-0\t\tEpoch 6/10\n",
      "INFO\t2021-06-02 02:05:03 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 02:05:03 +0000\tmaster-replica-0\t\tEpoch 00006: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model_tuned/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:05:03 +0000\tmaster-replica-0\t\t62500/62500 - 121s - loss: 0.9975 - rmse: 0.9876 - mse: 0.9975 - val_loss: 4.9133 - val_rmse: 2.2154 - val_mse: 4.9133\n",
      "ERROR\t2021-06-02 02:07:00 +0000\tmaster-replica-0\t\t2021-06-02 02:07:00.145788: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:07:03 +0000\tmaster-replica-0\t\tEpoch 7/10\n",
      "INFO\t2021-06-02 02:07:03 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 02:07:03 +0000\tmaster-replica-0\t\tEpoch 00007: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model_tuned/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:07:03 +0000\tmaster-replica-0\t\t62500/62500 - 120s - loss: 0.9417 - rmse: 0.9599 - mse: 0.9417 - val_loss: 5.5198 - val_rmse: 2.3484 - val_mse: 5.5198\n",
      "ERROR\t2021-06-02 02:09:02 +0000\tmaster-replica-0\t\t2021-06-02 02:09:02.544985: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:09:05 +0000\tmaster-replica-0\t\tEpoch 8/10\n",
      "INFO\t2021-06-02 02:09:05 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 02:09:05 +0000\tmaster-replica-0\t\tEpoch 00008: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model_tuned/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:09:05 +0000\tmaster-replica-0\t\t62500/62500 - 122s - loss: 0.9532 - rmse: 0.9661 - mse: 0.9532 - val_loss: 6.3828 - val_rmse: 2.5258 - val_mse: 6.3828\n",
      "ERROR\t2021-06-02 02:11:03 +0000\tmaster-replica-0\t\t2021-06-02 02:11:03.501857: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:11:06 +0000\tmaster-replica-0\t\tEpoch 9/10\n",
      "INFO\t2021-06-02 02:11:06 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 02:11:06 +0000\tmaster-replica-0\t\tEpoch 00009: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model_tuned/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:11:06 +0000\tmaster-replica-0\t\t62500/62500 - 121s - loss: 1.2293 - rmse: 1.0854 - mse: 1.2293 - val_loss: 2.1428 - val_rmse: 1.4628 - val_mse: 2.1428\n",
      "ERROR\t2021-06-02 02:13:03 +0000\tmaster-replica-0\t\t2021-06-02 02:13:03.710665: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "INFO\t2021-06-02 02:13:07 +0000\tmaster-replica-0\t\tEpoch 10/10\n",
      "INFO\t2021-06-02 02:13:07 +0000\tmaster-replica-0\t\t\n",
      "INFO\t2021-06-02 02:13:07 +0000\tmaster-replica-0\t\tEpoch 00010: saving model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model_tuned/checkpoints/babyweight\n",
      "INFO\t2021-06-02 02:13:07 +0000\tmaster-replica-0\t\t62500/62500 - 120s - loss: 1.1615 - rmse: 1.0619 - mse: 1.1615 - val_loss: 3.5838 - val_rmse: 1.8928 - val_mse: 3.5838\n",
      "ERROR\t2021-06-02 02:13:07 +0000\tmaster-replica-0\t\t2021-06-02 02:13:07.107636: W tensorflow/core/kernels/data/cache_dataset_ops.cc:822] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "ERROR\t2021-06-02 02:13:20 +0000\tmaster-replica-0\t\t2021-06-02 02:13:20.421536: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "ERROR\t2021-06-02 02:13:21 +0000\tmaster-replica-0\t\tWARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "ERROR\t2021-06-02 02:13:21 +0000\tmaster-replica-0\t\tInstructions for updating:\n",
      "ERROR\t2021-06-02 02:13:21 +0000\tmaster-replica-0\t\tIf using Keras pass *_constraint arguments to layers.\n",
      "INFO\t2021-06-02 02:13:26 +0000\tmaster-replica-0\t\tExported trained model to gs://qwiklabs-gcp-00-0db9b1bc58c6/babyweight/trained_model_tuned/20210602021319\n",
      "INFO\t2021-06-02 02:16:04 +0000\tservice\t\tJob completed successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs stream-logs babyweight_210602_014810"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Summary: \n",
    "In this lab, we set up the environment, created the trainer module's task.py to hold hyperparameter argparsing code, created the trainer module's model.py to hold Keras model code, ran the trainer module package locally, submitted a training job to Cloud AI Platform, and submitted a hyperparameter tuning job to Cloud AI Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2019 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
