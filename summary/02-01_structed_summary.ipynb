{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9950e684",
   "metadata": {},
   "source": [
    "# training-data-analyst/courses/machine_learning/deepdive2/structured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992805a",
   "metadata": {},
   "source": [
    "1a_explore_data_babyweight\n",
    "1b_prepare_data_babyweight\n",
    "4b_keras_dnn_babyweight\n",
    "4b_keras_dnn_babyweight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c8f7d9",
   "metadata": {},
   "source": [
    "### Creating BigQuery dataset titled: babyweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e53f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"babyweight\"\n",
    "\n",
    "bq --location=US mk --dataset \\\n",
    "        --description \"description\" \\\n",
    "        $PROJECT:${DATASET_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b7b8d9",
   "metadata": {},
   "source": [
    "### Create GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6078216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil mb -l ${REGION} gs://${BUCKET}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd255f6",
   "metadata": {},
   "source": [
    "### Retrieve GCS bucket files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23745c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsutil ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada3db2",
   "metadata": {},
   "source": [
    "### Query to get all column names within table schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "    column_name\n",
    "FROM\n",
    "    publicdata.samples.INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE\n",
    "    table_name = \"natality\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1104cb",
   "metadata": {},
   "source": [
    "### Create dataset table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543867fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE {NEW_TABLE_NAME}\n",
    "AS SELECT {COLUMNS} FROM ... WHERE ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387b80d",
   "metadata": {},
   "source": [
    "### Create hash column using year and month in Bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT FARM_FINGERPRINT(CONCAT(\n",
    "        CAST(year AS STRING),\n",
    "        CAST(month AS STRING)) AS hashmonth\n",
    "FROM ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bccf165",
   "metadata": {},
   "source": [
    "### Augment dataset to simulate missing data\n",
    "만약 예측 시점에 특정될 수도 있고 특정되지 않을 수도 있는 데이터는 두 개 case 모두를 이용하여 data augment 할 수 있다.\n",
    "\n",
    "*예: 초음파 검사를 하는 경우 알 수 있는 쌍둥이 여부, 성별을 unknown 데이터로 변경*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c85b20c",
   "metadata": {},
   "source": [
    "### Split augmented dataset into train and eval sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3603ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "CREATE OR REPLACE TABLE\n",
    "    ...data_train AS\n",
    "SELECT\n",
    "    ...\n",
    "FROM\n",
    "    ...\n",
    "WHERE\n",
    "    MOD(hashmonth, 4) < 3  # hash column 값을 나눈 나머지로 split. 3/4\n",
    "    \n",
    "\n",
    "CREATE OR REPLACE TABLE\n",
    "    ...data_evel AS\n",
    "SELECT\n",
    "    ...\n",
    "FROM\n",
    "    ...\n",
    "WHERE\n",
    "    MOD(hashmonth, 4) = 3  # hash column 값을 나눈 나머지로 split. 1/4\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6a381",
   "metadata": {},
   "source": [
    "### convert bigquery result to dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa9d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call BigQuery and examine in dataframe\n",
    "df = bigquery.Client().query(query).to_dataframe()\n",
    "df.head()\n",
    "\n",
    "# sort dataframe with coloumn name\n",
    "df.sort_values(column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc51507",
   "metadata": {},
   "source": [
    "### data plotting을 통해서 유효한 data를 확인할 필요가 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe39c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dataframe\n",
    "df.plot(x=column_name, y=\"num_babies\", kind=\"bar\", figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85d9b97",
   "metadata": {},
   "source": [
    "### Export from BigQuery to CSVs in GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afdee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# bigquery에 생성한 dataset name\n",
    "dataset_name = 'babyweight'\n",
    "\n",
    "# Create dataset reference object\n",
    "dataset_ref = client.dataset(\n",
    "    dataset_id=dataset_name, project=client.project)\n",
    "\n",
    "table_ref = dataset_ref.table(table_name)\n",
    "extract_job = client.extract_table(\n",
    "    table_ref,\n",
    "    # google storage url\n",
    "    # ex) os.path.join(\"gs://\", BUCKET, dataset_name, \"data\", \"{}*.csv\".format(step))\n",
    "    destination_uri,\n",
    "    # Location must match that of the source table.\n",
    "    location=\"US\",\n",
    ")  # API request\n",
    "\n",
    "extract_job.result() # Waits for job to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd1eed8",
   "metadata": {},
   "source": [
    "### Verify CSV creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/data/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8a6e3",
   "metadata": {},
   "source": [
    "#### load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8595f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_and_labels(row_data):\n",
    "    \"\"\"Splits features and labels from feature dictionary.\n",
    "\n",
    "    Args:\n",
    "        row_data: Dictionary of CSV column names and tensor values.\n",
    "    Returns:\n",
    "        Dictionary of feature tensors and label tensor.\n",
    "    \"\"\"\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "\n",
    "    return row_data, label  # features, label\n",
    "\n",
    "\n",
    "from tf.data.experimental import make_csv_dataset\n",
    "\n",
    "# Make a CSV dataset. \n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/experimental/make_csv_dataset\n",
    "dataset = make_csv_dataset(pattern,  # path of csv files\n",
    "                           batch_size,  # the number of records to combine in a single batch\n",
    "                           CSV_COLUMNS,  # the keys of the features dict of each dataset element\n",
    "                           DEFAULTS)  # default values for the CSV fields\n",
    "\n",
    "# Map dataset to features and label\n",
    "dataset = dataset.map(features_and_labels)  # features, label\n",
    "\n",
    "# Shuffle and repeat for training\n",
    "# shuffle: buffer_size = the number of elements from this dataset\n",
    "# repeat: if count -1 or None then the dataset be repeated indefinitely\n",
    "# https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "if mode == 'train':\n",
    "    dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "\n",
    "# Take advantage of multi-threading; 1=AUTOTUNE\n",
    "# 백그라운드 스레드와 내부 버퍼를 사용하여 요청된 시간 전에 입력 데이터셋에서 요소를 가져옵니다\n",
    "# buffer_size: the maximum number of elements that will be buffered\n",
    "# buffer_size=1 is tf.data.AUTOTUNE. It means the buffer size is dynamically tuned\n",
    "# https://www.tensorflow.org/guide/data_performance\n",
    "dataset = dataset.prefetch(buffer_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75077010",
   "metadata": {},
   "source": [
    "### encoding one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9344dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding one-hot vector\n",
    "# https://www.tensorflow.org/tutorials/structured_data/feature_columns?hl=ko\n",
    "\n",
    "# 모델에 문자열을 바로 주입할 수 없습니다. 대신 문자열을 먼저 수치형으로 매핑해야 합니다. \n",
    "# 범주형 열(categorical column)을 사용하여 문자열을 원-핫 벡터로 표현할 수 있습니다\n",
    "# 가능한 문자열이 몇 개로 제한될 때\n",
    "def get_categorical(name, voc_list):\n",
    "    cat = tf.feature_column.categorical_column_with_vocabulary_list(name, voc_list)\n",
    "    return tf.feature_column.indicator_column(cat)\n",
    "\n",
    "# 가능한 문자열이 몇 개가 있는 것이 아니라 범주마다 수천 개 이상의 값이 있는 경우\n",
    "# dimension = dimension of the embedding\n",
    "def get_embedding_column(name, voc_list):\n",
    "    cat = tf.feature_column.categorical_column_with_vocabulary_list(name, voc_list)\n",
    "    return tf.feature_column.embedding_column(cat, dimension=len(voc_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20eabd4",
   "metadata": {},
   "source": [
    "### tensorflow.feature_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d8b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/tutorials/structured_data/feature_columns\n",
    "\n",
    "feature_columns = []\n",
    "\n",
    "# 수치형 열: 데이터프레임 열의 값을 변형시키지 않고 그대로 전달  \n",
    "for header in ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope', 'ca']:\n",
    "  feature_columns.append(feature_column.numeric_column(header))\n",
    "\n",
    "# 버킷형 열: 수치 값의 구간을 나누어 이를 기반으로 범주형 one-hot encoding으로 변환\n",
    "age_buckets = feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "feature_columns.append(age_buckets)\n",
    "\n",
    "# 범주형 열: 문자열을 먼저 수치형으로 매핑\n",
    "# 범주형 열(categorical column)을 사용하여 문자열을 원-핫 벡터로 표현\n",
    "thal = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'thal', ['fixed', 'normal', 'reversible'])\n",
    "thal_one_hot = feature_column.indicator_column(thal)\n",
    "feature_columns.append(thal_one_hot)\n",
    "\n",
    "# 임베딩 열\n",
    "# 고차원 원-핫 벡터로 데이터를 표현하는 대신 임베딩 열을 사용하여 저차원으로 데이터를 표현합니다. \n",
    "# 이 벡터는 0 또는 1이 아니라 각 원소에 어떤 숫자도 넣을 수 있는 밀집 벡터(dense vector)입니다\n",
    "thal_embedding = feature_column.embedding_column(thal, dimension=8)\n",
    "feature_columns.append(thal_embedding)\n",
    "\n",
    "# 교차 특성 열: 여러 특성을 연결하여 하나의 특성으로 만드는 것\n",
    "# numeric column은 사용 불가\n",
    "# All keys must be either string, or categorical column except HashedCategoricalColumn\n",
    "# 모델이 특성의 조합에 대한 가중치를 학습\n",
    "crossed_feature = feature_column.crossed_column([age_buckets, thal], hash_bucket_size=1000)\n",
    "crossed_feature = feature_column.indicator_column(crossed_feature)\n",
    "feature_columns.append(crossed_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c2d84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### rmse 구하기\n",
    "rmse = lambda y_true, y_pred: tf.sqrt(tf.reduce_mean((y_true - y_pred)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1e550e",
   "metadata": {},
   "source": [
    "### Steps for building model\n",
    "1. Create input layers\n",
    "2. Create feature columns\n",
    "3. Build model and compile it all together\n",
    "  - 최종 dense layer의 activation은 linear, 그 외 dense layer는 relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7efb1c",
   "metadata": {},
   "source": [
    "### feature_columns to input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that produces a dense Tensor\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/DenseFeatures\n",
    "tf.keras.layers.DenseFeatures(\n",
    "    feature_columns, # numeric_column, embedding_column, bucketized_column, indicator_column. \n",
    "    # If you have categorical features, you can wrap them with an embedding_column or indicator_column.\n",
    "    name\n",
    ")(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb22616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit\n",
    "model.fit(\n",
    "    trainds,  # tuple(features, labels)\n",
    "    validation_data=evalds,  # tuple(features, labels)\n",
    "    epochs=NUM_EVALS,  # on epcoh end, check validation. so epochs equals to evals.\n",
    "    batch_size,  # If unspecified, batch_size will default to 32\n",
    "    steps_per_epoch=steps_per_epoch,  # Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch\n",
    "    callbacks=[tensorboard_callback])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m69",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m69"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
